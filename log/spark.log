2020-04-07 21:19:18,232 WARN org.apache.spark.util.Utils: Your hostname, MacBook-Pro-CW.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)
2020-04-07 21:19:18,233 WARN org.apache.spark.util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
2020-04-07 21:19:18,274 INFO org.apache.spark.SparkContext: Running Spark version 2.4.0.cloudera2
2020-04-07 21:19:18,495 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-04-07 21:19:18,622 INFO org.apache.spark.SparkContext: Submitted application: Kafka2SparkStreaming2Kudu-kerberos
2020-04-07 21:19:18,664 INFO org.apache.spark.SecurityManager: Changing view acls to: godiswc
2020-04-07 21:19:18,664 INFO org.apache.spark.SecurityManager: Changing modify acls to: godiswc
2020-04-07 21:19:18,665 INFO org.apache.spark.SecurityManager: Changing view acls groups to: 
2020-04-07 21:19:18,665 INFO org.apache.spark.SecurityManager: Changing modify acls groups to: 
2020-04-07 21:19:18,666 INFO org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(godiswc); groups with view permissions: Set(); users  with modify permissions: Set(godiswc); groups with modify permissions: Set()
2020-04-07 21:19:18,801 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-07 21:19:18,877 INFO org.apache.spark.util.Utils: Successfully started service 'sparkDriver' on port 58688.
2020-04-07 21:19:18,892 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
2020-04-07 21:19:18,903 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
2020-04-07 21:19:18,905 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-04-07 21:19:18,905 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2020-04-07 21:19:18,917 INFO org.apache.spark.storage.DiskBlockManager: Created local directory at /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/blockmgr-bb8ae219-201b-4660-bdb0-159c64810f0d
2020-04-07 21:19:18,958 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 4.1 GB
2020-04-07 21:19:18,970 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
2020-04-07 21:19:19,049 INFO org.spark_project.jetty.util.log: Logging initialized @1620ms
2020-04-07 21:19:19,112 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2019-04-30T04:42:08+08:00, git hash: e1bc35120a6617ee3df052294e433f3a25ce7097
2020-04-07 21:19:19,129 INFO org.spark_project.jetty.server.Server: Started @1701ms
2020-04-07 21:19:19,132 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-07 21:19:19,149 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@72bd06ca{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-07 21:19:19,149 INFO org.apache.spark.util.Utils: Successfully started service 'SparkUI' on port 4040.
2020-04-07 21:19:19,177 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@41fe9859{/jobs,null,AVAILABLE,@Spark}
2020-04-07 21:19:19,179 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@79e18e38{/jobs/json,null,AVAILABLE,@Spark}
2020-04-07 21:19:19,180 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@29a60c27{/jobs/job,null,AVAILABLE,@Spark}
2020-04-07 21:19:19,184 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1ca25c47{/jobs/job/json,null,AVAILABLE,@Spark}
2020-04-07 21:19:19,187 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fcacc0{/stages,null,AVAILABLE,@Spark}
2020-04-07 21:19:19,188 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@533b266e{/stages/json,null,AVAILABLE,@Spark}
2020-04-07 21:19:19,189 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6d1d4d7{/stages/stage,null,AVAILABLE,@Spark}
2020-04-07 21:19:19,190 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62679465{/stages/stage/json,null,AVAILABLE,@Spark}
2020-04-07 21:19:19,190 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a988392{/stages/pool,null,AVAILABLE,@Spark}
2020-04-07 21:19:19,191 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1d71006f{/stages/pool/json,null,AVAILABLE,@Spark}
2020-04-07 21:19:19,192 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5b6813df{/storage,null,AVAILABLE,@Spark}
2020-04-07 21:19:19,192 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5f2606b{/storage/json,null,AVAILABLE,@Spark}
2020-04-07 21:19:19,193 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2b58f754{/storage/rdd,null,AVAILABLE,@Spark}
2020-04-07 21:19:19,193 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3ebff828{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-04-07 21:19:19,193 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2552f2cb{/environment,null,AVAILABLE,@Spark}
2020-04-07 21:19:19,194 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@33352f32{/environment/json,null,AVAILABLE,@Spark}
2020-04-07 21:19:19,194 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5f3b9c57{/executors,null,AVAILABLE,@Spark}
2020-04-07 21:19:19,195 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1e044120{/executors/json,null,AVAILABLE,@Spark}
2020-04-07 21:19:19,195 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2cf23c81{/executors/threadDump,null,AVAILABLE,@Spark}
2020-04-07 21:19:19,196 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3624da92{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-04-07 21:19:19,201 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@35fe2125{/static,null,AVAILABLE,@Spark}
2020-04-07 21:19:19,202 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@27a5328c{/,null,AVAILABLE,@Spark}
2020-04-07 21:19:19,203 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1e5f4170{/api,null,AVAILABLE,@Spark}
2020-04-07 21:19:19,203 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1568159{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-04-07 21:19:19,204 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4fcee388{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-04-07 21:19:19,205 INFO org.apache.spark.ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
2020-04-07 21:19:19,288 INFO org.apache.spark.executor.Executor: Starting executor ID driver on host localhost
2020-04-07 21:19:19,344 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-07 21:19:19,346 INFO org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 58689.
2020-04-07 21:19:19,347 INFO org.apache.spark.network.netty.NettyBlockTransferService: Server created on 192.168.0.102:58689
2020-04-07 21:19:19,349 INFO org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-04-07 21:19:19,381 INFO org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 58689, None)
2020-04-07 21:19:19,385 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:58689 with 4.1 GB RAM, BlockManagerId(driver, 192.168.0.102, 58689, None)
2020-04-07 21:19:19,388 INFO org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 58689, None)
2020-04-07 21:19:19,389 INFO org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 58689, None)
2020-04-07 21:19:19,548 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@46baf579{/metrics/json,null,AVAILABLE,@Spark}
2020-04-07 21:19:19,843 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding enable.auto.commit to false for executor
2020-04-07 21:19:19,843 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding auto.offset.reset to none for executor
2020-04-07 21:19:19,844 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding executor group.id to spark-executor-testgroup
2020-04-07 21:19:19,844 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
2020-04-07 21:19:21,578 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Slide time = 10000 ms
2020-04-07 21:19:21,578 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Storage level = Serialized 1x Replicated
2020-04-07 21:19:21,579 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Checkpoint interval = null
2020-04-07 21:19:21,579 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Remember interval = 10000 ms
2020-04-07 21:19:21,579 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@6f790556
2020-04-07 21:19:21,579 INFO org.apache.spark.streaming.dstream.ForEachDStream: Slide time = 10000 ms
2020-04-07 21:19:21,579 INFO org.apache.spark.streaming.dstream.ForEachDStream: Storage level = Serialized 1x Replicated
2020-04-07 21:19:21,580 INFO org.apache.spark.streaming.dstream.ForEachDStream: Checkpoint interval = null
2020-04-07 21:19:21,580 INFO org.apache.spark.streaming.dstream.ForEachDStream: Remember interval = 10000 ms
2020-04-07 21:19:21,580 INFO org.apache.spark.streaming.dstream.ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@3a1e71b2
2020-04-07 21:19:21,622 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = testgroup
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-07 21:19:21,671 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-07 21:19:21,672 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-07 21:19:21,672 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586265561670
2020-04-07 21:19:21,673 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-1, groupId=testgroup] Subscribed to topic(s): PABUSI2
2020-04-07 21:19:22,321 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-1, groupId=testgroup] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-07 21:19:22,322 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Discovered group coordinator cw-instances-4.vpc.cloudera.com:9092 (id: 2147483594 rack: null)
2020-04-07 21:19:22,325 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Revoking previously assigned partitions []
2020-04-07 21:19:22,325 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] (Re-)joining group
2020-04-07 21:19:22,844 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] (Re-)joining group
2020-04-07 21:19:26,271 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Successfully joined group with generation 11
2020-04-07 21:19:26,275 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Setting newly assigned partitions: PABUSI2-2, PABUSI2-0, PABUSI2-1
2020-04-07 21:19:26,458 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Setting offset for partition PABUSI2-2 to the committed offset FetchPosition{offset=88, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-2.vpc.cloudera.com:9092 (id: 52 rack: null), epoch=0}}
2020-04-07 21:19:26,460 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Setting offset for partition PABUSI2-0 to the committed offset FetchPosition{offset=87, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-4.vpc.cloudera.com:9092 (id: 53 rack: null), epoch=0}}
2020-04-07 21:19:26,460 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Setting offset for partition PABUSI2-1 to the committed offset FetchPosition{offset=89, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-3.vpc.cloudera.com:9092 (id: 54 rack: null), epoch=0}}
2020-04-07 21:19:26,468 INFO org.apache.spark.streaming.util.RecurringTimer: Started timer for JobGenerator at time 1586265570000
2020-04-07 21:19:26,469 INFO org.apache.spark.streaming.scheduler.JobGenerator: Started JobGenerator at 1586265570000 ms
2020-04-07 21:19:26,470 INFO org.apache.spark.streaming.scheduler.JobScheduler: Started JobScheduler
2020-04-07 21:19:26,477 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4eed2acf{/streaming,null,AVAILABLE,@Spark}
2020-04-07 21:19:26,480 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@36fc05ff{/streaming/json,null,AVAILABLE,@Spark}
2020-04-07 21:19:26,482 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@153cd6bb{/streaming/batch,null,AVAILABLE,@Spark}
2020-04-07 21:19:26,483 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@61d84e08{/streaming/batch/json,null,AVAILABLE,@Spark}
2020-04-07 21:19:26,484 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@52d6cd34{/static/streaming,null,AVAILABLE,@Spark}
2020-04-07 21:19:26,485 INFO org.apache.spark.streaming.StreamingContext: StreamingContext started
2020-04-07 21:19:30,048 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-2
2020-04-07 21:19:30,049 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-0
2020-04-07 21:19:30,049 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-1
2020-04-07 21:19:30,230 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-0 to offset 87.
2020-04-07 21:19:30,231 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-2 to offset 88.
2020-04-07 21:19:30,231 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-1 to offset 89.
2020-04-07 21:19:30,262 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586265570000 ms
2020-04-07 21:19:30,265 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586265570000 ms.0 from job set of time 1586265570000 ms
2020-04-07 21:19:30,333 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586265570000 ms.0 from job set of time 1586265570000 ms
2020-04-07 21:19:30,336 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.332 s for time 1586265570000 ms (execution: 0.068 s)
2020-04-07 21:19:30,344 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-07 21:19:30,348 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 
2020-04-07 21:19:40,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-2
2020-04-07 21:19:40,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-0
2020-04-07 21:19:40,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-1
2020-04-07 21:19:40,180 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-2 to offset 95.
2020-04-07 21:19:40,180 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-0 to offset 93.
2020-04-07 21:19:40,181 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-1 to offset 96.
2020-04-07 21:19:40,182 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586265580000 ms
2020-04-07 21:19:40,183 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586265580000 ms.0 from job set of time 1586265580000 ms
2020-04-07 21:19:40,187 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586265580000 ms.0 from job set of time 1586265580000 ms
2020-04-07 21:19:40,188 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.187 s for time 1586265580000 ms (execution: 0.004 s)
2020-04-07 21:19:40,188 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 0 from persistence list
2020-04-07 21:19:40,192 INFO org.apache.spark.storage.BlockManager: Removing RDD 0
2020-04-07 21:19:40,193 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-07 21:19:40,193 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 
2020-04-07 21:19:50,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-0
2020-04-07 21:19:50,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-1
2020-04-07 21:19:50,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-2
2020-04-07 21:19:50,177 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-2 to offset 95.
2020-04-07 21:19:50,177 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-0 to offset 93.
2020-04-07 21:19:50,178 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-1 to offset 96.
2020-04-07 21:19:50,179 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586265590000 ms
2020-04-07 21:19:50,179 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586265590000 ms.0 from job set of time 1586265590000 ms
2020-04-07 21:19:50,184 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586265590000 ms.0 from job set of time 1586265590000 ms
2020-04-07 21:19:50,184 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.184 s for time 1586265590000 ms (execution: 0.005 s)
2020-04-07 21:19:50,184 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 2 from persistence list
2020-04-07 21:19:50,185 INFO org.apache.spark.storage.BlockManager: Removing RDD 2
2020-04-07 21:19:50,185 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-07 21:19:50,185 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586265570000 ms
2020-04-07 21:20:00,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-0
2020-04-07 21:20:00,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-1
2020-04-07 21:20:00,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-2
2020-04-07 21:20:00,182 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-2 to offset 95.
2020-04-07 21:20:00,183 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-0 to offset 93.
2020-04-07 21:20:00,183 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-1 to offset 96.
2020-04-07 21:20:00,186 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586265600000 ms
2020-04-07 21:20:00,187 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586265600000 ms.0 from job set of time 1586265600000 ms
2020-04-07 21:20:00,195 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586265600000 ms.0 from job set of time 1586265600000 ms
2020-04-07 21:20:00,197 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 4 from persistence list
2020-04-07 21:20:00,197 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.195 s for time 1586265600000 ms (execution: 0.008 s)
2020-04-07 21:20:00,199 INFO org.apache.spark.storage.BlockManager: Removing RDD 4
2020-04-07 21:20:00,199 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-07 21:20:00,200 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586265580000 ms
2020-04-07 21:20:10,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-0
2020-04-07 21:20:10,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-1
2020-04-07 21:20:10,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-2
2020-04-07 21:20:10,175 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-0 to offset 93.
2020-04-07 21:20:10,175 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-2 to offset 95.
2020-04-07 21:20:10,176 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-1 to offset 96.
2020-04-07 21:20:10,177 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586265610000 ms
2020-04-07 21:20:10,178 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586265610000 ms.0 from job set of time 1586265610000 ms
2020-04-07 21:20:10,182 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586265610000 ms.0 from job set of time 1586265610000 ms
2020-04-07 21:20:10,182 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.182 s for time 1586265610000 ms (execution: 0.004 s)
2020-04-07 21:20:10,182 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 6 from persistence list
2020-04-07 21:20:10,183 INFO org.apache.spark.storage.BlockManager: Removing RDD 6
2020-04-07 21:20:10,183 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-07 21:20:10,183 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586265590000 ms
2020-04-07 21:20:20,001 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-0
2020-04-07 21:20:20,001 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-1
2020-04-07 21:20:20,001 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-2
2020-04-07 21:20:20,176 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-2 to offset 95.
2020-04-07 21:20:20,177 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-0 to offset 93.
2020-04-07 21:20:20,177 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-1 to offset 96.
2020-04-07 21:20:20,178 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586265620000 ms
2020-04-07 21:20:20,178 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586265620000 ms.0 from job set of time 1586265620000 ms
2020-04-07 21:20:20,182 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586265620000 ms.0 from job set of time 1586265620000 ms
2020-04-07 21:20:20,183 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.182 s for time 1586265620000 ms (execution: 0.004 s)
2020-04-07 21:20:20,183 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 8 from persistence list
2020-04-07 21:20:20,183 INFO org.apache.spark.storage.BlockManager: Removing RDD 8
2020-04-07 21:20:20,183 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-07 21:20:20,183 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586265600000 ms
2020-04-07 21:20:30,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-0
2020-04-07 21:20:30,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-1
2020-04-07 21:20:30,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-2
2020-04-07 21:20:30,179 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-2 to offset 95.
2020-04-07 21:20:30,179 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-0 to offset 93.
2020-04-07 21:20:30,180 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-1 to offset 96.
2020-04-07 21:20:30,181 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586265630000 ms
2020-04-07 21:20:30,181 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586265630000 ms.0 from job set of time 1586265630000 ms
2020-04-07 21:20:30,186 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586265630000 ms.0 from job set of time 1586265630000 ms
2020-04-07 21:20:30,186 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.186 s for time 1586265630000 ms (execution: 0.005 s)
2020-04-07 21:20:30,186 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 10 from persistence list
2020-04-07 21:20:30,187 INFO org.apache.spark.storage.BlockManager: Removing RDD 10
2020-04-07 21:20:30,187 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-07 21:20:30,187 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586265610000 ms
2020-04-07 21:20:35,694 INFO org.apache.spark.streaming.StreamingContext: Invoking stop(stopGracefully=false) from shutdown hook
2020-04-07 21:20:35,697 INFO org.apache.spark.streaming.scheduler.ReceiverTracker: ReceiverTracker stopped
2020-04-07 21:20:35,700 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopping JobGenerator immediately
2020-04-07 21:20:35,701 INFO org.apache.spark.streaming.util.RecurringTimer: Stopped timer for JobGenerator after time 1586265630000
2020-04-07 21:20:35,892 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Member consumer-1-72e30bb4-8f2b-43e0-a3f1-f9d36f248646 sending LeaveGroup request to coordinator cw-instances-4.vpc.cloudera.com:9092 (id: 2147483594 rack: null)
2020-04-07 21:20:36,073 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopped JobGenerator
2020-04-07 21:20:36,076 INFO org.apache.spark.streaming.scheduler.JobScheduler: Stopped JobScheduler
2020-04-07 21:20:36,080 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@4eed2acf{/streaming,null,UNAVAILABLE,@Spark}
2020-04-07 21:20:36,080 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@153cd6bb{/streaming/batch,null,UNAVAILABLE,@Spark}
2020-04-07 21:20:36,081 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@52d6cd34{/static/streaming,null,UNAVAILABLE,@Spark}
2020-04-07 21:20:36,082 INFO org.apache.spark.streaming.StreamingContext: StreamingContext stopped successfully
2020-04-07 21:20:36,082 INFO org.apache.spark.SparkContext: Invoking stop() from shutdown hook
2020-04-07 21:20:36,087 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@72bd06ca{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-07 21:20:36,088 INFO org.apache.spark.ui.SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
2020-04-07 21:20:36,094 INFO org.apache.spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2020-04-07 21:20:36,109 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2020-04-07 21:20:36,109 INFO org.apache.spark.storage.BlockManager: BlockManager stopped
2020-04-07 21:20:36,110 INFO org.apache.spark.storage.BlockManagerMaster: BlockManagerMaster stopped
2020-04-07 21:20:36,114 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2020-04-07 21:20:36,123 INFO org.apache.spark.SparkContext: Successfully stopped SparkContext
2020-04-07 21:20:36,123 INFO org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2020-04-07 21:20:36,124 INFO org.apache.spark.util.ShutdownHookManager: Deleting directory /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/spark-3f3582ed-6ef6-4602-a677-569edcbe7ddf
2020-04-07 21:23:50,462 WARN org.apache.spark.util.Utils: Your hostname, MacBook-Pro-CW.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)
2020-04-07 21:23:50,463 WARN org.apache.spark.util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
2020-04-07 21:23:50,503 INFO org.apache.spark.SparkContext: Running Spark version 2.4.0.cloudera2
2020-04-07 21:23:50,697 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-04-07 21:23:50,806 INFO org.apache.spark.SparkContext: Submitted application: Kafka2SparkStreaming2Kudu-kerberos
2020-04-07 21:23:50,845 INFO org.apache.spark.SecurityManager: Changing view acls to: godiswc
2020-04-07 21:23:50,845 INFO org.apache.spark.SecurityManager: Changing modify acls to: godiswc
2020-04-07 21:23:50,846 INFO org.apache.spark.SecurityManager: Changing view acls groups to: 
2020-04-07 21:23:50,846 INFO org.apache.spark.SecurityManager: Changing modify acls groups to: 
2020-04-07 21:23:50,846 INFO org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(godiswc); groups with view permissions: Set(); users  with modify permissions: Set(godiswc); groups with modify permissions: Set()
2020-04-07 21:23:50,966 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-07 21:23:51,043 INFO org.apache.spark.util.Utils: Successfully started service 'sparkDriver' on port 59805.
2020-04-07 21:23:51,059 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
2020-04-07 21:23:51,071 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
2020-04-07 21:23:51,074 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-04-07 21:23:51,074 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2020-04-07 21:23:51,087 INFO org.apache.spark.storage.DiskBlockManager: Created local directory at /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/blockmgr-0ec4ceb5-17ff-4f4f-b7c6-229733f70e9d
2020-04-07 21:23:51,119 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 4.1 GB
2020-04-07 21:23:51,128 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
2020-04-07 21:23:51,186 INFO org.spark_project.jetty.util.log: Logging initialized @1509ms
2020-04-07 21:23:51,244 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2019-04-30T04:42:08+08:00, git hash: e1bc35120a6617ee3df052294e433f3a25ce7097
2020-04-07 21:23:51,259 INFO org.spark_project.jetty.server.Server: Started @1584ms
2020-04-07 21:23:51,261 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-07 21:23:51,278 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@72bd06ca{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-07 21:23:51,278 INFO org.apache.spark.util.Utils: Successfully started service 'SparkUI' on port 4040.
2020-04-07 21:23:51,304 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@41fe9859{/jobs,null,AVAILABLE,@Spark}
2020-04-07 21:23:51,305 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@79e18e38{/jobs/json,null,AVAILABLE,@Spark}
2020-04-07 21:23:51,306 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@29a60c27{/jobs/job,null,AVAILABLE,@Spark}
2020-04-07 21:23:51,309 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1ca25c47{/jobs/job/json,null,AVAILABLE,@Spark}
2020-04-07 21:23:51,313 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fcacc0{/stages,null,AVAILABLE,@Spark}
2020-04-07 21:23:51,314 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@533b266e{/stages/json,null,AVAILABLE,@Spark}
2020-04-07 21:23:51,314 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6d1d4d7{/stages/stage,null,AVAILABLE,@Spark}
2020-04-07 21:23:51,316 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62679465{/stages/stage/json,null,AVAILABLE,@Spark}
2020-04-07 21:23:51,316 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a988392{/stages/pool,null,AVAILABLE,@Spark}
2020-04-07 21:23:51,317 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1d71006f{/stages/pool/json,null,AVAILABLE,@Spark}
2020-04-07 21:23:51,318 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5b6813df{/storage,null,AVAILABLE,@Spark}
2020-04-07 21:23:51,318 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5f2606b{/storage/json,null,AVAILABLE,@Spark}
2020-04-07 21:23:51,319 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2b58f754{/storage/rdd,null,AVAILABLE,@Spark}
2020-04-07 21:23:51,320 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3ebff828{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-04-07 21:23:51,320 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2552f2cb{/environment,null,AVAILABLE,@Spark}
2020-04-07 21:23:51,321 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@33352f32{/environment/json,null,AVAILABLE,@Spark}
2020-04-07 21:23:51,322 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5f3b9c57{/executors,null,AVAILABLE,@Spark}
2020-04-07 21:23:51,323 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1e044120{/executors/json,null,AVAILABLE,@Spark}
2020-04-07 21:23:51,323 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2cf23c81{/executors/threadDump,null,AVAILABLE,@Spark}
2020-04-07 21:23:51,324 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3624da92{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-04-07 21:23:51,332 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@35fe2125{/static,null,AVAILABLE,@Spark}
2020-04-07 21:23:51,332 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@27a5328c{/,null,AVAILABLE,@Spark}
2020-04-07 21:23:51,333 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1e5f4170{/api,null,AVAILABLE,@Spark}
2020-04-07 21:23:51,334 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1568159{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-04-07 21:23:51,334 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4fcee388{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-04-07 21:23:51,336 INFO org.apache.spark.ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
2020-04-07 21:23:51,442 INFO org.apache.spark.executor.Executor: Starting executor ID driver on host localhost
2020-04-07 21:23:51,530 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-07 21:23:51,532 INFO org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59809.
2020-04-07 21:23:51,532 INFO org.apache.spark.network.netty.NettyBlockTransferService: Server created on 192.168.0.102:59809
2020-04-07 21:23:51,533 INFO org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-04-07 21:23:51,554 INFO org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 59809, None)
2020-04-07 21:23:51,556 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:59809 with 4.1 GB RAM, BlockManagerId(driver, 192.168.0.102, 59809, None)
2020-04-07 21:23:51,558 INFO org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 59809, None)
2020-04-07 21:23:51,559 INFO org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 59809, None)
2020-04-07 21:23:51,749 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@46baf579{/metrics/json,null,AVAILABLE,@Spark}
2020-04-07 21:23:52,150 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding enable.auto.commit to false for executor
2020-04-07 21:23:52,151 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding auto.offset.reset to none for executor
2020-04-07 21:23:52,152 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding executor group.id to spark-executor-testgroup
2020-04-07 21:23:52,153 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
2020-04-07 21:23:52,705 WARN org.apache.kudu.util.NetUtil: Slow DNS lookup! Resolved IP of `cw-instances-2.vpc.cloudera.com' to cw-instances-2.vpc.cloudera.com/10.65.1.47 in 153738877ns
2020-04-07 21:23:52,943 WARN org.apache.kudu.util.NetUtil: Slow DNS lookup! Resolved IP of `cw-instances-3.vpc.cloudera.com' to cw-instances-3.vpc.cloudera.com/10.65.9.108 in 153979832ns
2020-04-07 21:23:53,099 WARN org.apache.kudu.util.NetUtil: Slow DNS lookup! Resolved IP of `cw-instances-4.vpc.cloudera.com' to cw-instances-4.vpc.cloudera.com/10.65.12.223 in 154697681ns
2020-04-07 21:23:54,238 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Slide time = 10000 ms
2020-04-07 21:23:54,238 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Storage level = Serialized 1x Replicated
2020-04-07 21:23:54,238 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Checkpoint interval = null
2020-04-07 21:23:54,239 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Remember interval = 10000 ms
2020-04-07 21:23:54,239 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@48277bb0
2020-04-07 21:23:54,239 INFO org.apache.spark.streaming.dstream.ForEachDStream: Slide time = 10000 ms
2020-04-07 21:23:54,239 INFO org.apache.spark.streaming.dstream.ForEachDStream: Storage level = Serialized 1x Replicated
2020-04-07 21:23:54,240 INFO org.apache.spark.streaming.dstream.ForEachDStream: Checkpoint interval = null
2020-04-07 21:23:54,240 INFO org.apache.spark.streaming.dstream.ForEachDStream: Remember interval = 10000 ms
2020-04-07 21:23:54,240 INFO org.apache.spark.streaming.dstream.ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@28a69d66
2020-04-07 21:23:54,292 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = testgroup
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-07 21:23:54,343 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-07 21:23:54,343 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-07 21:23:54,343 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586265834342
2020-04-07 21:23:54,345 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-1, groupId=testgroup] Subscribed to topic(s): PABUSI2
2020-04-07 21:23:55,051 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-1, groupId=testgroup] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-07 21:23:55,052 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Discovered group coordinator cw-instances-4.vpc.cloudera.com:9092 (id: 2147483594 rack: null)
2020-04-07 21:23:55,055 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Revoking previously assigned partitions []
2020-04-07 21:23:55,055 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] (Re-)joining group
2020-04-07 21:23:55,578 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] (Re-)joining group
2020-04-07 21:23:58,931 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Successfully joined group with generation 13
2020-04-07 21:23:58,934 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Setting newly assigned partitions: PABUSI2-2, PABUSI2-0, PABUSI2-1
2020-04-07 21:23:59,115 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Setting offset for partition PABUSI2-2 to the committed offset FetchPosition{offset=95, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-2.vpc.cloudera.com:9092 (id: 52 rack: null), epoch=0}}
2020-04-07 21:23:59,116 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Setting offset for partition PABUSI2-0 to the committed offset FetchPosition{offset=93, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-4.vpc.cloudera.com:9092 (id: 53 rack: null), epoch=0}}
2020-04-07 21:23:59,116 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Setting offset for partition PABUSI2-1 to the committed offset FetchPosition{offset=96, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-3.vpc.cloudera.com:9092 (id: 54 rack: null), epoch=0}}
2020-04-07 21:23:59,123 INFO org.apache.spark.streaming.util.RecurringTimer: Started timer for JobGenerator at time 1586265840000
2020-04-07 21:23:59,124 INFO org.apache.spark.streaming.scheduler.JobGenerator: Started JobGenerator at 1586265840000 ms
2020-04-07 21:23:59,124 INFO org.apache.spark.streaming.scheduler.JobScheduler: Started JobScheduler
2020-04-07 21:23:59,128 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@642505c7{/streaming,null,AVAILABLE,@Spark}
2020-04-07 21:23:59,129 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4339e0de{/streaming/json,null,AVAILABLE,@Spark}
2020-04-07 21:23:59,130 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@21ac5eb4{/streaming/batch,null,AVAILABLE,@Spark}
2020-04-07 21:23:59,130 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@52d6cd34{/streaming/batch/json,null,AVAILABLE,@Spark}
2020-04-07 21:23:59,130 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6428591a{/static/streaming,null,AVAILABLE,@Spark}
2020-04-07 21:23:59,131 INFO org.apache.spark.streaming.StreamingContext: StreamingContext started
2020-04-07 21:24:00,043 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-0
2020-04-07 21:24:00,043 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-1
2020-04-07 21:24:00,043 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-2
2020-04-07 21:24:00,432 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-0 to offset 93.
2020-04-07 21:24:00,432 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-1 to offset 96.
2020-04-07 21:24:00,433 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-2 to offset 95.
2020-04-07 21:24:00,456 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586265840000 ms
2020-04-07 21:24:00,458 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586265840000 ms.0 from job set of time 1586265840000 ms
2020-04-07 21:24:00,504 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586265840000 ms.0 from job set of time 1586265840000 ms
2020-04-07 21:24:00,505 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.503 s for time 1586265840000 ms (execution: 0.046 s)
2020-04-07 21:24:00,509 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-07 21:24:00,513 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 
2020-04-07 21:24:10,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-0
2020-04-07 21:24:10,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-1
2020-04-07 21:24:10,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-2
2020-04-07 21:24:10,180 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-2 to offset 95.
2020-04-07 21:24:10,180 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-1 to offset 96.
2020-04-07 21:24:10,186 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-0 to offset 93.
2020-04-07 21:24:10,187 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586265850000 ms
2020-04-07 21:24:10,187 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586265850000 ms.0 from job set of time 1586265850000 ms
2020-04-07 21:24:10,193 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586265850000 ms.0 from job set of time 1586265850000 ms
2020-04-07 21:24:10,193 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.193 s for time 1586265850000 ms (execution: 0.006 s)
2020-04-07 21:24:10,194 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 0 from persistence list
2020-04-07 21:24:10,198 INFO org.apache.spark.storage.BlockManager: Removing RDD 0
2020-04-07 21:24:10,198 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-07 21:24:10,198 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 
2020-04-07 21:24:20,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-0
2020-04-07 21:24:20,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-1
2020-04-07 21:24:20,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-2
2020-04-07 21:24:20,177 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-0 to offset 93.
2020-04-07 21:24:20,177 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-1 to offset 96.
2020-04-07 21:24:20,178 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-2 to offset 95.
2020-04-07 21:24:20,179 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586265860000 ms
2020-04-07 21:24:20,180 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586265860000 ms.0 from job set of time 1586265860000 ms
2020-04-07 21:24:20,190 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586265860000 ms.0 from job set of time 1586265860000 ms
2020-04-07 21:24:20,190 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.190 s for time 1586265860000 ms (execution: 0.010 s)
2020-04-07 21:24:20,190 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 2 from persistence list
2020-04-07 21:24:20,190 INFO org.apache.spark.storage.BlockManager: Removing RDD 2
2020-04-07 21:24:20,191 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-07 21:24:20,191 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586265840000 ms
2020-04-07 21:24:30,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-0
2020-04-07 21:24:30,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-1
2020-04-07 21:24:30,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-2
2020-04-07 21:24:30,178 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-0 to offset 93.
2020-04-07 21:24:30,178 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-2 to offset 95.
2020-04-07 21:24:30,178 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-1 to offset 96.
2020-04-07 21:24:30,179 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586265870000 ms
2020-04-07 21:24:30,180 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586265870000 ms.0 from job set of time 1586265870000 ms
2020-04-07 21:24:30,186 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586265870000 ms.0 from job set of time 1586265870000 ms
2020-04-07 21:24:30,186 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 4 from persistence list
2020-04-07 21:24:30,186 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.186 s for time 1586265870000 ms (execution: 0.006 s)
2020-04-07 21:24:30,187 INFO org.apache.spark.storage.BlockManager: Removing RDD 4
2020-04-07 21:24:30,187 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-07 21:24:30,187 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586265850000 ms
2020-04-07 21:24:40,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-0
2020-04-07 21:24:40,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-1
2020-04-07 21:24:40,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-2
2020-04-07 21:24:40,179 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-2 to offset 95.
2020-04-07 21:24:40,180 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-0 to offset 93.
2020-04-07 21:24:40,180 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-1 to offset 96.
2020-04-07 21:24:40,181 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586265880000 ms
2020-04-07 21:24:40,181 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586265880000 ms.0 from job set of time 1586265880000 ms
2020-04-07 21:24:40,188 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586265880000 ms.0 from job set of time 1586265880000 ms
2020-04-07 21:24:40,188 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.188 s for time 1586265880000 ms (execution: 0.007 s)
2020-04-07 21:24:40,188 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 6 from persistence list
2020-04-07 21:24:40,189 INFO org.apache.spark.storage.BlockManager: Removing RDD 6
2020-04-07 21:24:40,189 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-07 21:24:40,189 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586265860000 ms
2020-04-07 21:24:50,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-0
2020-04-07 21:24:50,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-1
2020-04-07 21:24:50,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-2
2020-04-07 21:24:50,182 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-0 to offset 93.
2020-04-07 21:24:50,182 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-1 to offset 96.
2020-04-07 21:24:50,183 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-2 to offset 95.
2020-04-07 21:24:50,184 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586265890000 ms
2020-04-07 21:24:50,184 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586265890000 ms.0 from job set of time 1586265890000 ms
2020-04-07 21:24:50,190 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586265890000 ms.0 from job set of time 1586265890000 ms
2020-04-07 21:24:50,191 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.190 s for time 1586265890000 ms (execution: 0.006 s)
2020-04-07 21:24:50,191 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 8 from persistence list
2020-04-07 21:24:50,191 INFO org.apache.spark.storage.BlockManager: Removing RDD 8
2020-04-07 21:24:50,191 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-07 21:24:50,192 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586265870000 ms
2020-04-07 21:25:00,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-0
2020-04-07 21:25:00,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-1
2020-04-07 21:25:00,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-2
2020-04-07 21:25:00,179 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-0 to offset 93.
2020-04-07 21:25:00,179 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-1 to offset 96.
2020-04-07 21:25:00,179 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-2 to offset 95.
2020-04-07 21:25:00,180 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586265900000 ms
2020-04-07 21:25:00,181 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586265900000 ms.0 from job set of time 1586265900000 ms
2020-04-07 21:25:00,186 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586265900000 ms.0 from job set of time 1586265900000 ms
2020-04-07 21:25:00,186 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.186 s for time 1586265900000 ms (execution: 0.005 s)
2020-04-07 21:25:00,186 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 10 from persistence list
2020-04-07 21:25:00,187 INFO org.apache.spark.storage.BlockManager: Removing RDD 10
2020-04-07 21:25:00,187 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-07 21:25:00,187 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586265880000 ms
2020-04-07 21:25:10,001 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-0
2020-04-07 21:25:10,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-1
2020-04-07 21:25:10,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-2
2020-04-07 21:25:10,175 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-0 to offset 93.
2020-04-07 21:25:10,175 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-1 to offset 96.
2020-04-07 21:25:10,175 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-2 to offset 95.
2020-04-07 21:25:10,176 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586265910000 ms
2020-04-07 21:25:10,177 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586265910000 ms.0 from job set of time 1586265910000 ms
2020-04-07 21:25:10,182 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586265910000 ms.0 from job set of time 1586265910000 ms
2020-04-07 21:25:10,182 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.182 s for time 1586265910000 ms (execution: 0.005 s)
2020-04-07 21:25:10,183 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 12 from persistence list
2020-04-07 21:25:10,186 INFO org.apache.spark.storage.BlockManager: Removing RDD 12
2020-04-07 21:25:10,186 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-07 21:25:10,187 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586265890000 ms
2020-04-07 21:25:20,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-0
2020-04-07 21:25:20,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-1
2020-04-07 21:25:20,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-2
2020-04-07 21:25:20,178 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-0 to offset 93.
2020-04-07 21:25:20,179 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-1 to offset 96.
2020-04-07 21:25:20,179 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-2 to offset 95.
2020-04-07 21:25:20,180 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586265920000 ms
2020-04-07 21:25:20,181 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586265920000 ms.0 from job set of time 1586265920000 ms
2020-04-07 21:25:20,186 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586265920000 ms.0 from job set of time 1586265920000 ms
2020-04-07 21:25:20,186 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.186 s for time 1586265920000 ms (execution: 0.005 s)
2020-04-07 21:25:20,186 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 14 from persistence list
2020-04-07 21:25:20,187 INFO org.apache.spark.storage.BlockManager: Removing RDD 14
2020-04-07 21:25:20,187 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-07 21:25:20,187 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586265900000 ms
2020-04-07 21:25:30,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-0
2020-04-07 21:25:30,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-1
2020-04-07 21:25:30,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-2
2020-04-07 21:25:30,181 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-0 to offset 93.
2020-04-07 21:25:30,181 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-1 to offset 96.
2020-04-07 21:25:30,181 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-2 to offset 95.
2020-04-07 21:25:30,182 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586265930000 ms
2020-04-07 21:25:30,183 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586265930000 ms.0 from job set of time 1586265930000 ms
2020-04-07 21:25:30,188 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586265930000 ms.0 from job set of time 1586265930000 ms
2020-04-07 21:25:30,188 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.188 s for time 1586265930000 ms (execution: 0.005 s)
2020-04-07 21:25:30,188 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 16 from persistence list
2020-04-07 21:25:30,190 INFO org.apache.spark.storage.BlockManager: Removing RDD 16
2020-04-07 21:25:30,190 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-07 21:25:30,190 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586265910000 ms
2020-04-07 21:25:40,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-0
2020-04-07 21:25:40,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-1
2020-04-07 21:25:40,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-2
2020-04-07 21:25:40,178 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-0 to offset 93.
2020-04-07 21:25:40,178 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-1 to offset 96.
2020-04-07 21:25:40,178 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-2 to offset 95.
2020-04-07 21:25:40,179 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586265940000 ms
2020-04-07 21:25:40,180 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586265940000 ms.0 from job set of time 1586265940000 ms
2020-04-07 21:25:40,186 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586265940000 ms.0 from job set of time 1586265940000 ms
2020-04-07 21:25:40,187 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.186 s for time 1586265940000 ms (execution: 0.006 s)
2020-04-07 21:25:40,187 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 18 from persistence list
2020-04-07 21:25:40,188 INFO org.apache.spark.storage.BlockManager: Removing RDD 18
2020-04-07 21:25:40,188 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-07 21:25:40,188 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586265920000 ms
2020-04-07 21:25:50,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-0
2020-04-07 21:25:50,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-1
2020-04-07 21:25:50,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-2
2020-04-07 21:25:50,182 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-0 to offset 93.
2020-04-07 21:25:50,183 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-1 to offset 96.
2020-04-07 21:25:50,183 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-2 to offset 95.
2020-04-07 21:25:50,184 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586265950000 ms
2020-04-07 21:25:50,184 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586265950000 ms.0 from job set of time 1586265950000 ms
2020-04-07 21:25:50,189 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586265950000 ms.0 from job set of time 1586265950000 ms
2020-04-07 21:25:50,189 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 20 from persistence list
2020-04-07 21:25:50,189 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.189 s for time 1586265950000 ms (execution: 0.005 s)
2020-04-07 21:25:50,189 INFO org.apache.spark.storage.BlockManager: Removing RDD 20
2020-04-07 21:25:50,190 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-07 21:25:50,190 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586265930000 ms
2020-04-07 21:26:00,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-0
2020-04-07 21:26:00,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-1
2020-04-07 21:26:00,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-2
2020-04-07 21:26:00,175 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-0 to offset 93.
2020-04-07 21:26:00,182 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-2 to offset 95.
2020-04-07 21:26:00,182 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-1 to offset 96.
2020-04-07 21:26:00,183 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586265960000 ms
2020-04-07 21:26:00,183 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586265960000 ms.0 from job set of time 1586265960000 ms
2020-04-07 21:26:00,189 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586265960000 ms.0 from job set of time 1586265960000 ms
2020-04-07 21:26:00,189 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.189 s for time 1586265960000 ms (execution: 0.006 s)
2020-04-07 21:26:00,189 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 22 from persistence list
2020-04-07 21:26:00,190 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-07 21:26:00,190 INFO org.apache.spark.storage.BlockManager: Removing RDD 22
2020-04-07 21:26:00,190 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586265940000 ms
2020-04-07 21:26:10,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-0
2020-04-07 21:26:10,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-1
2020-04-07 21:26:10,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-2
2020-04-07 21:26:10,177 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-0 to offset 93.
2020-04-07 21:26:10,177 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-1 to offset 96.
2020-04-07 21:26:10,178 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-2 to offset 95.
2020-04-07 21:26:10,179 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586265970000 ms
2020-04-07 21:26:10,179 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586265970000 ms.0 from job set of time 1586265970000 ms
2020-04-07 21:26:10,184 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586265970000 ms.0 from job set of time 1586265970000 ms
2020-04-07 21:26:10,184 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.184 s for time 1586265970000 ms (execution: 0.005 s)
2020-04-07 21:26:10,184 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 24 from persistence list
2020-04-07 21:26:10,185 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-07 21:26:10,185 INFO org.apache.spark.storage.BlockManager: Removing RDD 24
2020-04-07 21:26:10,185 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586265950000 ms
2020-04-07 21:26:20,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-0
2020-04-07 21:26:20,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-1
2020-04-07 21:26:20,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-2
2020-04-07 21:26:20,182 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-0 to offset 93.
2020-04-07 21:26:20,182 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-1 to offset 96.
2020-04-07 21:26:20,182 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-2 to offset 95.
2020-04-07 21:26:20,183 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586265980000 ms
2020-04-07 21:26:20,184 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586265980000 ms.0 from job set of time 1586265980000 ms
2020-04-07 21:26:20,188 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586265980000 ms.0 from job set of time 1586265980000 ms
2020-04-07 21:26:20,189 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.188 s for time 1586265980000 ms (execution: 0.004 s)
2020-04-07 21:26:20,189 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 26 from persistence list
2020-04-07 21:26:20,189 INFO org.apache.spark.storage.BlockManager: Removing RDD 26
2020-04-07 21:26:20,189 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-07 21:26:20,189 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586265960000 ms
2020-04-07 21:26:30,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-0
2020-04-07 21:26:30,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-1
2020-04-07 21:26:30,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-2
2020-04-07 21:26:30,175 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-0 to offset 93.
2020-04-07 21:26:30,176 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-2 to offset 95.
2020-04-07 21:26:30,176 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-1 to offset 96.
2020-04-07 21:26:30,177 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586265990000 ms
2020-04-07 21:26:30,177 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586265990000 ms.0 from job set of time 1586265990000 ms
2020-04-07 21:26:30,181 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586265990000 ms.0 from job set of time 1586265990000 ms
2020-04-07 21:26:30,182 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 28 from persistence list
2020-04-07 21:26:30,182 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.181 s for time 1586265990000 ms (execution: 0.004 s)
2020-04-07 21:26:30,182 INFO org.apache.spark.storage.BlockManager: Removing RDD 28
2020-04-07 21:26:30,182 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-07 21:26:30,182 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586265970000 ms
2020-04-07 21:26:40,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-0
2020-04-07 21:26:40,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-1
2020-04-07 21:26:40,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-2
2020-04-07 21:26:40,178 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-0 to offset 93.
2020-04-07 21:26:40,184 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-2 to offset 95.
2020-04-07 21:26:40,184 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-1 to offset 96.
2020-04-07 21:26:40,185 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586266000000 ms
2020-04-07 21:26:40,188 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586266000000 ms.0 from job set of time 1586266000000 ms
2020-04-07 21:26:40,194 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586266000000 ms.0 from job set of time 1586266000000 ms
2020-04-07 21:26:40,195 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 30 from persistence list
2020-04-07 21:26:40,195 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.194 s for time 1586266000000 ms (execution: 0.006 s)
2020-04-07 21:26:40,195 INFO org.apache.spark.storage.BlockManager: Removing RDD 30
2020-04-07 21:26:40,195 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-07 21:26:40,196 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586265980000 ms
2020-04-07 21:26:50,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-0
2020-04-07 21:26:50,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-1
2020-04-07 21:26:50,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-2
2020-04-07 21:26:50,173 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-0 to offset 93.
2020-04-07 21:26:50,174 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-2 to offset 95.
2020-04-07 21:26:50,174 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-1 to offset 96.
2020-04-07 21:26:50,175 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586266010000 ms
2020-04-07 21:26:50,176 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586266010000 ms.0 from job set of time 1586266010000 ms
2020-04-07 21:26:50,180 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586266010000 ms.0 from job set of time 1586266010000 ms
2020-04-07 21:26:50,180 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 32 from persistence list
2020-04-07 21:26:50,180 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.180 s for time 1586266010000 ms (execution: 0.004 s)
2020-04-07 21:26:50,180 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-07 21:26:50,180 INFO org.apache.spark.storage.BlockManager: Removing RDD 32
2020-04-07 21:26:50,181 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586265990000 ms
2020-04-07 21:27:00,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-0
2020-04-07 21:27:00,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-1
2020-04-07 21:27:00,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-2
2020-04-07 21:27:00,176 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-0 to offset 93.
2020-04-07 21:27:00,176 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-1 to offset 96.
2020-04-07 21:27:00,177 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-2 to offset 95.
2020-04-07 21:27:00,177 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586266020000 ms
2020-04-07 21:27:00,178 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586266020000 ms.0 from job set of time 1586266020000 ms
2020-04-07 21:27:00,182 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586266020000 ms.0 from job set of time 1586266020000 ms
2020-04-07 21:27:00,182 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.182 s for time 1586266020000 ms (execution: 0.004 s)
2020-04-07 21:27:00,182 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 34 from persistence list
2020-04-07 21:27:00,182 INFO org.apache.spark.storage.BlockManager: Removing RDD 34
2020-04-07 21:27:00,182 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-07 21:27:00,183 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586266000000 ms
2020-04-07 21:27:10,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-0
2020-04-07 21:27:10,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-1
2020-04-07 21:27:10,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-2
2020-04-07 21:27:10,178 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-0 to offset 93.
2020-04-07 21:27:10,178 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-1 to offset 96.
2020-04-07 21:27:10,178 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-2 to offset 95.
2020-04-07 21:27:10,179 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586266030000 ms
2020-04-07 21:27:10,180 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586266030000 ms.0 from job set of time 1586266030000 ms
2020-04-07 21:27:10,185 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586266030000 ms.0 from job set of time 1586266030000 ms
2020-04-07 21:27:10,186 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.185 s for time 1586266030000 ms (execution: 0.005 s)
2020-04-07 21:27:10,186 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 36 from persistence list
2020-04-07 21:27:10,189 INFO org.apache.spark.storage.BlockManager: Removing RDD 36
2020-04-07 21:27:10,190 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-07 21:27:10,190 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586266010000 ms
2020-04-07 21:27:20,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-0
2020-04-07 21:27:20,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-1
2020-04-07 21:27:20,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-2
2020-04-07 21:27:20,179 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-0 to offset 93.
2020-04-07 21:27:20,179 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-1 to offset 96.
2020-04-07 21:27:20,179 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-2 to offset 95.
2020-04-07 21:27:20,180 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586266040000 ms
2020-04-07 21:27:20,181 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586266040000 ms.0 from job set of time 1586266040000 ms
2020-04-07 21:27:20,185 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586266040000 ms.0 from job set of time 1586266040000 ms
2020-04-07 21:27:20,185 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.185 s for time 1586266040000 ms (execution: 0.005 s)
2020-04-07 21:27:20,185 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 38 from persistence list
2020-04-07 21:27:20,186 INFO org.apache.spark.storage.BlockManager: Removing RDD 38
2020-04-07 21:27:20,186 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-07 21:27:20,186 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586266020000 ms
2020-04-07 21:27:30,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-0
2020-04-07 21:27:30,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-1
2020-04-07 21:27:30,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-2
2020-04-07 21:27:30,186 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-0 to offset 93.
2020-04-07 21:27:30,186 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-1 to offset 96.
2020-04-07 21:27:30,186 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-2 to offset 95.
2020-04-07 21:27:30,187 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586266050000 ms
2020-04-07 21:27:30,187 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586266050000 ms.0 from job set of time 1586266050000 ms
2020-04-07 21:27:30,191 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586266050000 ms.0 from job set of time 1586266050000 ms
2020-04-07 21:27:30,191 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.191 s for time 1586266050000 ms (execution: 0.004 s)
2020-04-07 21:27:30,191 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 40 from persistence list
2020-04-07 21:27:30,192 INFO org.apache.spark.storage.BlockManager: Removing RDD 40
2020-04-07 21:27:30,192 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-07 21:27:30,192 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586266030000 ms
2020-04-07 21:27:40,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-0
2020-04-07 21:27:40,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-1
2020-04-07 21:27:40,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-2
2020-04-07 21:27:40,179 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-1 to offset 96.
2020-04-07 21:27:40,180 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-2 to offset 95.
2020-04-07 21:27:40,180 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-0 to offset 93.
2020-04-07 21:27:40,181 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586266060000 ms
2020-04-07 21:27:40,181 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586266060000 ms.0 from job set of time 1586266060000 ms
2020-04-07 21:27:40,191 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586266060000 ms.0 from job set of time 1586266060000 ms
2020-04-07 21:27:40,192 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 42 from persistence list
2020-04-07 21:27:40,192 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.191 s for time 1586266060000 ms (execution: 0.010 s)
2020-04-07 21:27:40,193 INFO org.apache.spark.storage.BlockManager: Removing RDD 42
2020-04-07 21:27:40,193 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-07 21:27:40,194 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586266040000 ms
2020-04-07 21:27:50,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-0
2020-04-07 21:27:50,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-1
2020-04-07 21:27:50,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-2
2020-04-07 21:27:50,179 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-0 to offset 93.
2020-04-07 21:27:50,179 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-1 to offset 96.
2020-04-07 21:27:50,179 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-2 to offset 95.
2020-04-07 21:27:50,180 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586266070000 ms
2020-04-07 21:27:50,180 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586266070000 ms.0 from job set of time 1586266070000 ms
2020-04-07 21:27:50,184 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586266070000 ms.0 from job set of time 1586266070000 ms
2020-04-07 21:27:50,184 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 44 from persistence list
2020-04-07 21:27:50,184 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.184 s for time 1586266070000 ms (execution: 0.004 s)
2020-04-07 21:27:50,185 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-07 21:27:50,185 INFO org.apache.spark.storage.BlockManager: Removing RDD 44
2020-04-07 21:27:50,185 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586266050000 ms
2020-04-07 21:28:00,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-0
2020-04-07 21:28:00,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-1
2020-04-07 21:28:00,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-2
2020-04-07 21:28:00,179 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-1 to offset 96.
2020-04-07 21:28:00,179 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-2 to offset 95.
2020-04-07 21:28:00,179 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-0 to offset 93.
2020-04-07 21:28:00,180 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586266080000 ms
2020-04-07 21:28:00,180 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586266080000 ms.0 from job set of time 1586266080000 ms
2020-04-07 21:28:00,185 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586266080000 ms.0 from job set of time 1586266080000 ms
2020-04-07 21:28:00,185 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 46 from persistence list
2020-04-07 21:28:00,185 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.185 s for time 1586266080000 ms (execution: 0.005 s)
2020-04-07 21:28:00,186 INFO org.apache.spark.storage.BlockManager: Removing RDD 46
2020-04-07 21:28:00,186 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-07 21:28:00,186 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586266060000 ms
2020-04-07 21:28:10,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-0
2020-04-07 21:28:10,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-1
2020-04-07 21:28:10,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-2
2020-04-07 21:28:10,176 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-1 to offset 96.
2020-04-07 21:28:10,176 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-2 to offset 95.
2020-04-07 21:28:10,176 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-0 to offset 93.
2020-04-07 21:28:10,177 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586266090000 ms
2020-04-07 21:28:10,178 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586266090000 ms.0 from job set of time 1586266090000 ms
2020-04-07 21:28:10,181 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586266090000 ms.0 from job set of time 1586266090000 ms
2020-04-07 21:28:10,181 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 48 from persistence list
2020-04-07 21:28:10,181 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.181 s for time 1586266090000 ms (execution: 0.004 s)
2020-04-07 21:28:10,182 INFO org.apache.spark.storage.BlockManager: Removing RDD 48
2020-04-07 21:28:10,182 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-07 21:28:10,182 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586266070000 ms
2020-04-07 21:28:20,001 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-0
2020-04-07 21:28:20,001 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-1
2020-04-07 21:28:20,001 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-2
2020-04-07 21:28:20,174 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-1 to offset 96.
2020-04-07 21:28:20,174 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-0 to offset 93.
2020-04-07 21:28:20,175 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-2 to offset 95.
2020-04-07 21:28:20,175 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586266100000 ms
2020-04-07 21:28:20,176 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586266100000 ms.0 from job set of time 1586266100000 ms
2020-04-07 21:28:20,179 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586266100000 ms.0 from job set of time 1586266100000 ms
2020-04-07 21:28:20,179 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 50 from persistence list
2020-04-07 21:28:20,179 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.179 s for time 1586266100000 ms (execution: 0.003 s)
2020-04-07 21:28:20,180 INFO org.apache.spark.storage.BlockManager: Removing RDD 50
2020-04-07 21:28:20,180 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-07 21:28:20,180 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586266080000 ms
2020-04-07 21:28:30,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-0
2020-04-07 21:28:30,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-1
2020-04-07 21:28:30,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-2
2020-04-07 21:28:30,181 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-2 to offset 95.
2020-04-07 21:28:30,181 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-0 to offset 93.
2020-04-07 21:28:30,181 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-1 to offset 96.
2020-04-07 21:28:30,182 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586266110000 ms
2020-04-07 21:28:30,182 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586266110000 ms.0 from job set of time 1586266110000 ms
2020-04-07 21:28:30,187 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586266110000 ms.0 from job set of time 1586266110000 ms
2020-04-07 21:28:30,187 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.187 s for time 1586266110000 ms (execution: 0.005 s)
2020-04-07 21:28:30,187 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 52 from persistence list
2020-04-07 21:28:30,188 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-07 21:28:30,188 INFO org.apache.spark.storage.BlockManager: Removing RDD 52
2020-04-07 21:28:30,188 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586266090000 ms
2020-04-07 21:28:40,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-0
2020-04-07 21:28:40,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-1
2020-04-07 21:28:40,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-2
2020-04-07 21:28:40,178 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-1 to offset 96.
2020-04-07 21:28:40,178 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-0 to offset 93.
2020-04-07 21:28:40,179 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-2 to offset 95.
2020-04-07 21:28:40,180 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586266120000 ms
2020-04-07 21:28:40,180 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586266120000 ms.0 from job set of time 1586266120000 ms
2020-04-07 21:28:40,184 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586266120000 ms.0 from job set of time 1586266120000 ms
2020-04-07 21:28:40,184 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 54 from persistence list
2020-04-07 21:28:40,184 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.184 s for time 1586266120000 ms (execution: 0.004 s)
2020-04-07 21:28:40,184 INFO org.apache.spark.storage.BlockManager: Removing RDD 54
2020-04-07 21:28:40,184 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-07 21:28:40,185 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586266100000 ms
2020-04-07 21:28:50,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-0
2020-04-07 21:28:50,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-1
2020-04-07 21:28:50,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-2
2020-04-07 21:28:50,176 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-1 to offset 96.
2020-04-07 21:28:50,179 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-2 to offset 95.
2020-04-07 21:28:50,180 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-0 to offset 93.
2020-04-07 21:28:50,181 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586266130000 ms
2020-04-07 21:28:50,181 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586266130000 ms.0 from job set of time 1586266130000 ms
2020-04-07 21:28:50,185 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586266130000 ms.0 from job set of time 1586266130000 ms
2020-04-07 21:28:50,185 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 56 from persistence list
2020-04-07 21:28:50,185 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.185 s for time 1586266130000 ms (execution: 0.004 s)
2020-04-07 21:28:50,187 INFO org.apache.spark.storage.BlockManager: Removing RDD 56
2020-04-07 21:28:50,187 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-07 21:28:50,188 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586266110000 ms
2020-04-07 21:28:54,815 INFO org.apache.spark.streaming.StreamingContext: Invoking stop(stopGracefully=false) from shutdown hook
2020-04-07 21:28:54,818 INFO org.apache.spark.streaming.scheduler.ReceiverTracker: ReceiverTracker stopped
2020-04-07 21:28:54,820 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopping JobGenerator immediately
2020-04-07 21:28:54,821 INFO org.apache.spark.streaming.util.RecurringTimer: Stopped timer for JobGenerator after time 1586266130000
2020-04-07 21:28:55,005 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Member consumer-1-c62f1313-2f0f-42f9-b771-270e5aeabf44 sending LeaveGroup request to coordinator cw-instances-4.vpc.cloudera.com:9092 (id: 2147483594 rack: null)
2020-04-07 21:28:55,185 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopped JobGenerator
2020-04-07 21:28:55,187 INFO org.apache.spark.streaming.scheduler.JobScheduler: Stopped JobScheduler
2020-04-07 21:28:55,191 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@642505c7{/streaming,null,UNAVAILABLE,@Spark}
2020-04-07 21:28:55,192 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@21ac5eb4{/streaming/batch,null,UNAVAILABLE,@Spark}
2020-04-07 21:28:55,193 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@6428591a{/static/streaming,null,UNAVAILABLE,@Spark}
2020-04-07 21:28:55,193 INFO org.apache.spark.streaming.StreamingContext: StreamingContext stopped successfully
2020-04-07 21:28:55,193 INFO org.apache.spark.SparkContext: Invoking stop() from shutdown hook
2020-04-07 21:28:55,201 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@72bd06ca{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-07 21:28:55,202 INFO org.apache.spark.ui.SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
2020-04-07 21:28:55,208 INFO org.apache.spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2020-04-07 21:28:55,224 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2020-04-07 21:28:55,225 INFO org.apache.spark.storage.BlockManager: BlockManager stopped
2020-04-07 21:28:55,225 INFO org.apache.spark.storage.BlockManagerMaster: BlockManagerMaster stopped
2020-04-07 21:28:55,228 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2020-04-07 21:28:55,237 INFO org.apache.spark.SparkContext: Successfully stopped SparkContext
2020-04-07 21:28:55,237 INFO org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2020-04-07 21:28:55,238 INFO org.apache.spark.util.ShutdownHookManager: Deleting directory /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/spark-c1e400d5-7ded-4a91-bcae-d72d1a0a234e
2020-04-07 21:29:03,673 WARN org.apache.spark.util.Utils: Your hostname, MacBook-Pro-CW.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)
2020-04-07 21:29:03,674 WARN org.apache.spark.util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
2020-04-07 21:29:03,711 INFO org.apache.spark.SparkContext: Running Spark version 2.4.0.cloudera2
2020-04-07 21:29:03,896 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-04-07 21:29:04,006 INFO org.apache.spark.SparkContext: Submitted application: Kafka2SparkStreaming2Kudu-kerberos
2020-04-07 21:29:04,044 INFO org.apache.spark.SecurityManager: Changing view acls to: godiswc
2020-04-07 21:29:04,044 INFO org.apache.spark.SecurityManager: Changing modify acls to: godiswc
2020-04-07 21:29:04,045 INFO org.apache.spark.SecurityManager: Changing view acls groups to: 
2020-04-07 21:29:04,045 INFO org.apache.spark.SecurityManager: Changing modify acls groups to: 
2020-04-07 21:29:04,045 INFO org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(godiswc); groups with view permissions: Set(); users  with modify permissions: Set(godiswc); groups with modify permissions: Set()
2020-04-07 21:29:04,169 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-07 21:29:04,256 INFO org.apache.spark.util.Utils: Successfully started service 'sparkDriver' on port 61007.
2020-04-07 21:29:04,274 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
2020-04-07 21:29:04,286 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
2020-04-07 21:29:04,288 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-04-07 21:29:04,289 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2020-04-07 21:29:04,301 INFO org.apache.spark.storage.DiskBlockManager: Created local directory at /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/blockmgr-7719ab59-c3c1-4855-bd84-fa841ca9d64c
2020-04-07 21:29:04,337 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 4.1 GB
2020-04-07 21:29:04,346 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
2020-04-07 21:29:04,391 INFO org.spark_project.jetty.util.log: Logging initialized @1310ms
2020-04-07 21:29:04,424 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2019-04-30T04:42:08+08:00, git hash: e1bc35120a6617ee3df052294e433f3a25ce7097
2020-04-07 21:29:04,434 INFO org.spark_project.jetty.server.Server: Started @1353ms
2020-04-07 21:29:04,435 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-07 21:29:04,443 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@72bd06ca{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-07 21:29:04,444 INFO org.apache.spark.util.Utils: Successfully started service 'SparkUI' on port 4040.
2020-04-07 21:29:04,458 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@41fe9859{/jobs,null,AVAILABLE,@Spark}
2020-04-07 21:29:04,459 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@79e18e38{/jobs/json,null,AVAILABLE,@Spark}
2020-04-07 21:29:04,459 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@29a60c27{/jobs/job,null,AVAILABLE,@Spark}
2020-04-07 21:29:04,462 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1ca25c47{/jobs/job/json,null,AVAILABLE,@Spark}
2020-04-07 21:29:04,464 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fcacc0{/stages,null,AVAILABLE,@Spark}
2020-04-07 21:29:04,465 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@533b266e{/stages/json,null,AVAILABLE,@Spark}
2020-04-07 21:29:04,466 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6d1d4d7{/stages/stage,null,AVAILABLE,@Spark}
2020-04-07 21:29:04,467 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62679465{/stages/stage/json,null,AVAILABLE,@Spark}
2020-04-07 21:29:04,468 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a988392{/stages/pool,null,AVAILABLE,@Spark}
2020-04-07 21:29:04,468 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1d71006f{/stages/pool/json,null,AVAILABLE,@Spark}
2020-04-07 21:29:04,469 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5b6813df{/storage,null,AVAILABLE,@Spark}
2020-04-07 21:29:04,469 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5f2606b{/storage/json,null,AVAILABLE,@Spark}
2020-04-07 21:29:04,470 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2b58f754{/storage/rdd,null,AVAILABLE,@Spark}
2020-04-07 21:29:04,470 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3ebff828{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-04-07 21:29:04,471 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2552f2cb{/environment,null,AVAILABLE,@Spark}
2020-04-07 21:29:04,471 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@33352f32{/environment/json,null,AVAILABLE,@Spark}
2020-04-07 21:29:04,472 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5f3b9c57{/executors,null,AVAILABLE,@Spark}
2020-04-07 21:29:04,472 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1e044120{/executors/json,null,AVAILABLE,@Spark}
2020-04-07 21:29:04,473 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2cf23c81{/executors/threadDump,null,AVAILABLE,@Spark}
2020-04-07 21:29:04,473 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3624da92{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-04-07 21:29:04,479 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@35fe2125{/static,null,AVAILABLE,@Spark}
2020-04-07 21:29:04,480 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@27a5328c{/,null,AVAILABLE,@Spark}
2020-04-07 21:29:04,481 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1e5f4170{/api,null,AVAILABLE,@Spark}
2020-04-07 21:29:04,482 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1568159{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-04-07 21:29:04,483 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4fcee388{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-04-07 21:29:04,486 INFO org.apache.spark.ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
2020-04-07 21:29:04,624 INFO org.apache.spark.executor.Executor: Starting executor ID driver on host localhost
2020-04-07 21:29:04,686 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-07 21:29:04,687 INFO org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 61008.
2020-04-07 21:29:04,687 INFO org.apache.spark.network.netty.NettyBlockTransferService: Server created on 192.168.0.102:61008
2020-04-07 21:29:04,689 INFO org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-04-07 21:29:04,718 INFO org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 61008, None)
2020-04-07 21:29:04,724 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:61008 with 4.1 GB RAM, BlockManagerId(driver, 192.168.0.102, 61008, None)
2020-04-07 21:29:04,726 INFO org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 61008, None)
2020-04-07 21:29:04,727 INFO org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 61008, None)
2020-04-07 21:29:04,910 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@46baf579{/metrics/json,null,AVAILABLE,@Spark}
2020-04-07 21:29:05,250 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding enable.auto.commit to false for executor
2020-04-07 21:29:05,251 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding auto.offset.reset to none for executor
2020-04-07 21:29:05,251 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding executor group.id to spark-executor-testgroup
2020-04-07 21:29:05,252 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
2020-04-07 21:29:07,012 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Slide time = 10000 ms
2020-04-07 21:29:07,012 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Storage level = Serialized 1x Replicated
2020-04-07 21:29:07,012 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Checkpoint interval = null
2020-04-07 21:29:07,013 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Remember interval = 10000 ms
2020-04-07 21:29:07,013 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@4a71ae79
2020-04-07 21:29:07,013 INFO org.apache.spark.streaming.dstream.ForEachDStream: Slide time = 10000 ms
2020-04-07 21:29:07,013 INFO org.apache.spark.streaming.dstream.ForEachDStream: Storage level = Serialized 1x Replicated
2020-04-07 21:29:07,014 INFO org.apache.spark.streaming.dstream.ForEachDStream: Checkpoint interval = null
2020-04-07 21:29:07,014 INFO org.apache.spark.streaming.dstream.ForEachDStream: Remember interval = 10000 ms
2020-04-07 21:29:07,014 INFO org.apache.spark.streaming.dstream.ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@ef60bfb
2020-04-07 21:29:07,059 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = testgroup
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-07 21:29:07,112 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-07 21:29:07,112 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-07 21:29:07,112 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586266147111
2020-04-07 21:29:07,114 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-1, groupId=testgroup] Subscribed to topic(s): PABUSI2
2020-04-07 21:29:07,780 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-1, groupId=testgroup] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-07 21:29:07,781 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Discovered group coordinator cw-instances-4.vpc.cloudera.com:9092 (id: 2147483594 rack: null)
2020-04-07 21:29:07,785 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Revoking previously assigned partitions []
2020-04-07 21:29:07,785 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] (Re-)joining group
2020-04-07 21:29:08,301 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] (Re-)joining group
2020-04-07 21:29:11,656 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Successfully joined group with generation 15
2020-04-07 21:29:11,660 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Setting newly assigned partitions: PABUSI2-2, PABUSI2-0, PABUSI2-1
2020-04-07 21:29:11,845 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Setting offset for partition PABUSI2-2 to the committed offset FetchPosition{offset=95, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-2.vpc.cloudera.com:9092 (id: 52 rack: null), epoch=0}}
2020-04-07 21:29:11,847 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Setting offset for partition PABUSI2-0 to the committed offset FetchPosition{offset=93, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-4.vpc.cloudera.com:9092 (id: 53 rack: null), epoch=0}}
2020-04-07 21:29:11,847 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Setting offset for partition PABUSI2-1 to the committed offset FetchPosition{offset=96, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-3.vpc.cloudera.com:9092 (id: 54 rack: null), epoch=0}}
2020-04-07 21:29:11,855 INFO org.apache.spark.streaming.util.RecurringTimer: Started timer for JobGenerator at time 1586266150000
2020-04-07 21:29:11,855 INFO org.apache.spark.streaming.scheduler.JobGenerator: Started JobGenerator at 1586266150000 ms
2020-04-07 21:29:11,856 INFO org.apache.spark.streaming.scheduler.JobScheduler: Started JobScheduler
2020-04-07 21:29:11,861 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@642505c7{/streaming,null,AVAILABLE,@Spark}
2020-04-07 21:29:11,862 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4339e0de{/streaming/json,null,AVAILABLE,@Spark}
2020-04-07 21:29:11,863 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@21ac5eb4{/streaming/batch,null,AVAILABLE,@Spark}
2020-04-07 21:29:11,863 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@52d6cd34{/streaming/batch/json,null,AVAILABLE,@Spark}
2020-04-07 21:29:11,865 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6428591a{/static/streaming,null,AVAILABLE,@Spark}
2020-04-07 21:29:11,865 INFO org.apache.spark.streaming.StreamingContext: StreamingContext started
2020-04-07 21:29:11,901 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-0
2020-04-07 21:29:11,902 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-1
2020-04-07 21:29:11,902 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-2
2020-04-07 21:29:12,954 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-0 to offset 93.
2020-04-07 21:29:12,955 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-2 to offset 95.
2020-04-07 21:29:12,956 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-1 to offset 96.
2020-04-07 21:29:12,978 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586266150000 ms
2020-04-07 21:29:12,980 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586266150000 ms.0 from job set of time 1586266150000 ms
2020-04-07 21:29:13,022 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586266150000 ms.0 from job set of time 1586266150000 ms
2020-04-07 21:29:13,023 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 3.021 s for time 1586266150000 ms (execution: 0.042 s)
2020-04-07 21:29:13,027 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-07 21:29:13,029 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 
2020-04-07 21:29:20,010 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-0
2020-04-07 21:29:20,011 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-1
2020-04-07 21:29:20,011 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-2
2020-04-07 21:29:20,238 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-2 to offset 95.
2020-04-07 21:29:20,238 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-1 to offset 96.
2020-04-07 21:29:20,239 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-0 to offset 93.
2020-04-07 21:29:20,240 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586266160000 ms
2020-04-07 21:29:20,241 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586266160000 ms.0 from job set of time 1586266160000 ms
2020-04-07 21:29:20,246 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586266160000 ms.0 from job set of time 1586266160000 ms
2020-04-07 21:29:20,247 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.246 s for time 1586266160000 ms (execution: 0.006 s)
2020-04-07 21:29:20,247 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 0 from persistence list
2020-04-07 21:29:20,251 INFO org.apache.spark.storage.BlockManager: Removing RDD 0
2020-04-07 21:29:20,251 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-07 21:29:20,251 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 
2020-04-07 21:29:30,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-0
2020-04-07 21:29:30,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-1
2020-04-07 21:29:30,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-2
2020-04-07 21:29:30,180 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-2 to offset 102.
2020-04-07 21:29:30,181 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-0 to offset 99.
2020-04-07 21:29:30,181 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-1 to offset 103.
2020-04-07 21:29:30,181 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586266170000 ms
2020-04-07 21:29:30,182 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586266170000 ms.0 from job set of time 1586266170000 ms
2020-04-07 21:29:30,189 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586266170000 ms.0 from job set of time 1586266170000 ms
2020-04-07 21:29:30,190 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 2 from persistence list
2020-04-07 21:29:30,190 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.189 s for time 1586266170000 ms (execution: 0.007 s)
2020-04-07 21:29:30,191 INFO org.apache.spark.storage.BlockManager: Removing RDD 2
2020-04-07 21:29:30,191 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-07 21:29:30,191 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586266150000 ms
2020-04-07 21:29:40,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-2
2020-04-07 21:29:40,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-0
2020-04-07 21:29:40,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-1
2020-04-07 21:29:40,178 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-2 to offset 102.
2020-04-07 21:29:40,179 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-0 to offset 99.
2020-04-07 21:29:40,179 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-1 to offset 103.
2020-04-07 21:29:40,180 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586266180000 ms
2020-04-07 21:29:40,181 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586266180000 ms.0 from job set of time 1586266180000 ms
2020-04-07 21:29:40,188 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586266180000 ms.0 from job set of time 1586266180000 ms
2020-04-07 21:29:40,188 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.188 s for time 1586266180000 ms (execution: 0.007 s)
2020-04-07 21:29:40,188 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 4 from persistence list
2020-04-07 21:29:40,189 INFO org.apache.spark.storage.BlockManager: Removing RDD 4
2020-04-07 21:29:40,189 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-07 21:29:40,189 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586266160000 ms
2020-04-07 21:29:44,185 INFO org.apache.spark.streaming.StreamingContext: Invoking stop(stopGracefully=false) from shutdown hook
2020-04-07 21:29:44,187 INFO org.apache.spark.streaming.scheduler.ReceiverTracker: ReceiverTracker stopped
2020-04-07 21:29:44,188 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopping JobGenerator immediately
2020-04-07 21:29:44,190 INFO org.apache.spark.streaming.util.RecurringTimer: Stopped timer for JobGenerator after time 1586266180000
2020-04-07 21:29:44,365 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Member consumer-1-455dbe0e-e87a-415e-aeea-ee2d34652d83 sending LeaveGroup request to coordinator cw-instances-4.vpc.cloudera.com:9092 (id: 2147483594 rack: null)
2020-04-07 21:29:44,544 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopped JobGenerator
2020-04-07 21:29:44,547 INFO org.apache.spark.streaming.scheduler.JobScheduler: Stopped JobScheduler
2020-04-07 21:29:44,552 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@642505c7{/streaming,null,UNAVAILABLE,@Spark}
2020-04-07 21:29:44,552 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@21ac5eb4{/streaming/batch,null,UNAVAILABLE,@Spark}
2020-04-07 21:29:44,553 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@6428591a{/static/streaming,null,UNAVAILABLE,@Spark}
2020-04-07 21:29:44,554 INFO org.apache.spark.streaming.StreamingContext: StreamingContext stopped successfully
2020-04-07 21:29:44,554 INFO org.apache.spark.SparkContext: Invoking stop() from shutdown hook
2020-04-07 21:29:44,559 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@72bd06ca{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-07 21:29:44,562 INFO org.apache.spark.ui.SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
2020-04-07 21:29:44,569 INFO org.apache.spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2020-04-07 21:29:44,584 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2020-04-07 21:29:44,585 INFO org.apache.spark.storage.BlockManager: BlockManager stopped
2020-04-07 21:29:44,586 INFO org.apache.spark.storage.BlockManagerMaster: BlockManagerMaster stopped
2020-04-07 21:29:44,589 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2020-04-07 21:29:44,599 INFO org.apache.spark.SparkContext: Successfully stopped SparkContext
2020-04-07 21:29:44,600 INFO org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2020-04-07 21:29:44,600 INFO org.apache.spark.util.ShutdownHookManager: Deleting directory /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/spark-6aa0df88-57b1-429e-b8d3-1ef825929a5d
2020-04-07 21:30:35,313 WARN org.apache.spark.util.Utils: Your hostname, MacBook-Pro-CW.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)
2020-04-07 21:30:35,315 WARN org.apache.spark.util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
2020-04-07 21:30:35,368 INFO org.apache.spark.SparkContext: Running Spark version 2.4.0.cloudera2
2020-04-07 21:30:35,608 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-04-07 21:30:35,733 INFO org.apache.spark.SparkContext: Submitted application: Kafka2SparkStreaming2Kudu-kerberos
2020-04-07 21:30:35,785 INFO org.apache.spark.SecurityManager: Changing view acls to: godiswc
2020-04-07 21:30:35,785 INFO org.apache.spark.SecurityManager: Changing modify acls to: godiswc
2020-04-07 21:30:35,786 INFO org.apache.spark.SecurityManager: Changing view acls groups to: 
2020-04-07 21:30:35,786 INFO org.apache.spark.SecurityManager: Changing modify acls groups to: 
2020-04-07 21:30:35,787 INFO org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(godiswc); groups with view permissions: Set(); users  with modify permissions: Set(godiswc); groups with modify permissions: Set()
2020-04-07 21:30:35,942 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-07 21:30:36,037 INFO org.apache.spark.util.Utils: Successfully started service 'sparkDriver' on port 61342.
2020-04-07 21:30:36,061 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
2020-04-07 21:30:36,076 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
2020-04-07 21:30:36,078 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-04-07 21:30:36,079 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2020-04-07 21:30:36,096 INFO org.apache.spark.storage.DiskBlockManager: Created local directory at /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/blockmgr-a760407c-d3f5-415a-a097-dea6a33ef490
2020-04-07 21:30:36,138 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 4.1 GB
2020-04-07 21:30:36,148 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
2020-04-07 21:30:36,225 INFO org.spark_project.jetty.util.log: Logging initialized @1694ms
2020-04-07 21:30:36,287 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2019-04-30T04:42:08+08:00, git hash: e1bc35120a6617ee3df052294e433f3a25ce7097
2020-04-07 21:30:36,302 INFO org.spark_project.jetty.server.Server: Started @1772ms
2020-04-07 21:30:36,304 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-07 21:30:36,316 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@72bd06ca{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-07 21:30:36,316 INFO org.apache.spark.util.Utils: Successfully started service 'SparkUI' on port 4040.
2020-04-07 21:30:36,335 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@41fe9859{/jobs,null,AVAILABLE,@Spark}
2020-04-07 21:30:36,336 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@79e18e38{/jobs/json,null,AVAILABLE,@Spark}
2020-04-07 21:30:36,336 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@29a60c27{/jobs/job,null,AVAILABLE,@Spark}
2020-04-07 21:30:36,339 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1ca25c47{/jobs/job/json,null,AVAILABLE,@Spark}
2020-04-07 21:30:36,341 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fcacc0{/stages,null,AVAILABLE,@Spark}
2020-04-07 21:30:36,342 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@533b266e{/stages/json,null,AVAILABLE,@Spark}
2020-04-07 21:30:36,342 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6d1d4d7{/stages/stage,null,AVAILABLE,@Spark}
2020-04-07 21:30:36,343 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62679465{/stages/stage/json,null,AVAILABLE,@Spark}
2020-04-07 21:30:36,344 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a988392{/stages/pool,null,AVAILABLE,@Spark}
2020-04-07 21:30:36,344 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1d71006f{/stages/pool/json,null,AVAILABLE,@Spark}
2020-04-07 21:30:36,344 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5b6813df{/storage,null,AVAILABLE,@Spark}
2020-04-07 21:30:36,345 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5f2606b{/storage/json,null,AVAILABLE,@Spark}
2020-04-07 21:30:36,345 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2b58f754{/storage/rdd,null,AVAILABLE,@Spark}
2020-04-07 21:30:36,346 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3ebff828{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-04-07 21:30:36,346 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2552f2cb{/environment,null,AVAILABLE,@Spark}
2020-04-07 21:30:36,347 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@33352f32{/environment/json,null,AVAILABLE,@Spark}
2020-04-07 21:30:36,348 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5f3b9c57{/executors,null,AVAILABLE,@Spark}
2020-04-07 21:30:36,348 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1e044120{/executors/json,null,AVAILABLE,@Spark}
2020-04-07 21:30:36,349 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2cf23c81{/executors/threadDump,null,AVAILABLE,@Spark}
2020-04-07 21:30:36,350 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3624da92{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-04-07 21:30:36,357 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@35fe2125{/static,null,AVAILABLE,@Spark}
2020-04-07 21:30:36,358 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@27a5328c{/,null,AVAILABLE,@Spark}
2020-04-07 21:30:36,359 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1e5f4170{/api,null,AVAILABLE,@Spark}
2020-04-07 21:30:36,360 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1568159{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-04-07 21:30:36,361 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4fcee388{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-04-07 21:30:36,364 INFO org.apache.spark.ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
2020-04-07 21:30:36,501 INFO org.apache.spark.executor.Executor: Starting executor ID driver on host localhost
2020-04-07 21:30:36,568 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-07 21:30:36,571 INFO org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 61347.
2020-04-07 21:30:36,572 INFO org.apache.spark.network.netty.NettyBlockTransferService: Server created on 192.168.0.102:61347
2020-04-07 21:30:36,574 INFO org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-04-07 21:30:36,609 INFO org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 61347, None)
2020-04-07 21:30:36,612 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:61347 with 4.1 GB RAM, BlockManagerId(driver, 192.168.0.102, 61347, None)
2020-04-07 21:30:36,614 INFO org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 61347, None)
2020-04-07 21:30:36,615 INFO org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 61347, None)
2020-04-07 21:30:36,800 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@46baf579{/metrics/json,null,AVAILABLE,@Spark}
2020-04-07 21:30:37,128 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding enable.auto.commit to false for executor
2020-04-07 21:30:37,129 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding auto.offset.reset to none for executor
2020-04-07 21:30:37,129 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding executor group.id to spark-executor-testgroup
2020-04-07 21:30:37,130 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
2020-04-07 21:30:37,661 WARN org.apache.kudu.util.NetUtil: Slow DNS lookup! Resolved IP of `cw-instances-2.vpc.cloudera.com' to cw-instances-2.vpc.cloudera.com/10.65.1.47 in 154927983ns
2020-04-07 21:30:37,887 WARN org.apache.kudu.util.NetUtil: Slow DNS lookup! Resolved IP of `cw-instances-3.vpc.cloudera.com' to cw-instances-3.vpc.cloudera.com/10.65.9.108 in 154737560ns
2020-04-07 21:30:38,043 WARN org.apache.kudu.util.NetUtil: Slow DNS lookup! Resolved IP of `cw-instances-4.vpc.cloudera.com' to cw-instances-4.vpc.cloudera.com/10.65.12.223 in 153094838ns
2020-04-07 21:30:39,036 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Slide time = 10000 ms
2020-04-07 21:30:39,036 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Storage level = Serialized 1x Replicated
2020-04-07 21:30:39,037 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Checkpoint interval = null
2020-04-07 21:30:39,037 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Remember interval = 10000 ms
2020-04-07 21:30:39,037 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@6f790556
2020-04-07 21:30:39,038 INFO org.apache.spark.streaming.dstream.ForEachDStream: Slide time = 10000 ms
2020-04-07 21:30:39,038 INFO org.apache.spark.streaming.dstream.ForEachDStream: Storage level = Serialized 1x Replicated
2020-04-07 21:30:39,038 INFO org.apache.spark.streaming.dstream.ForEachDStream: Checkpoint interval = null
2020-04-07 21:30:39,038 INFO org.apache.spark.streaming.dstream.ForEachDStream: Remember interval = 10000 ms
2020-04-07 21:30:39,038 INFO org.apache.spark.streaming.dstream.ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@3a1e71b2
2020-04-07 21:30:39,093 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = testgroup
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-07 21:30:39,146 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-07 21:30:39,146 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-07 21:30:39,146 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586266239145
2020-04-07 21:30:39,148 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-1, groupId=testgroup] Subscribed to topic(s): PABUSI2
2020-04-07 21:30:39,803 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-1, groupId=testgroup] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-07 21:30:39,804 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Discovered group coordinator cw-instances-4.vpc.cloudera.com:9092 (id: 2147483594 rack: null)
2020-04-07 21:30:39,807 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Revoking previously assigned partitions []
2020-04-07 21:30:39,807 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] (Re-)joining group
2020-04-07 21:30:40,331 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] (Re-)joining group
2020-04-07 21:30:43,691 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Successfully joined group with generation 17
2020-04-07 21:30:43,696 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Setting newly assigned partitions: PABUSI2-2, PABUSI2-0, PABUSI2-1
2020-04-07 21:30:43,882 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Setting offset for partition PABUSI2-2 to the committed offset FetchPosition{offset=102, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-2.vpc.cloudera.com:9092 (id: 52 rack: null), epoch=0}}
2020-04-07 21:30:43,883 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Setting offset for partition PABUSI2-0 to the committed offset FetchPosition{offset=99, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-4.vpc.cloudera.com:9092 (id: 53 rack: null), epoch=0}}
2020-04-07 21:30:43,883 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Setting offset for partition PABUSI2-1 to the committed offset FetchPosition{offset=103, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-3.vpc.cloudera.com:9092 (id: 54 rack: null), epoch=0}}
2020-04-07 21:30:43,893 INFO org.apache.spark.streaming.util.RecurringTimer: Started timer for JobGenerator at time 1586266240000
2020-04-07 21:30:43,893 INFO org.apache.spark.streaming.scheduler.JobGenerator: Started JobGenerator at 1586266240000 ms
2020-04-07 21:30:43,894 INFO org.apache.spark.streaming.scheduler.JobScheduler: Started JobScheduler
2020-04-07 21:30:43,900 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@52d6cd34{/streaming,null,AVAILABLE,@Spark}
2020-04-07 21:30:43,900 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@715d6168{/streaming/json,null,AVAILABLE,@Spark}
2020-04-07 21:30:43,904 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1abfe081{/streaming/batch,null,AVAILABLE,@Spark}
2020-04-07 21:30:43,905 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2a685eba{/streaming/batch/json,null,AVAILABLE,@Spark}
2020-04-07 21:30:43,906 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@75a118e6{/static/streaming,null,AVAILABLE,@Spark}
2020-04-07 21:30:43,906 INFO org.apache.spark.streaming.StreamingContext: StreamingContext started
2020-04-07 21:30:43,952 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-2
2020-04-07 21:30:43,953 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-0
2020-04-07 21:30:43,953 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-1
2020-04-07 21:30:45,015 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-1 to offset 103.
2020-04-07 21:30:45,016 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-2 to offset 102.
2020-04-07 21:30:45,016 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-0 to offset 99.
2020-04-07 21:30:45,046 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586266240000 ms
2020-04-07 21:30:45,051 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586266240000 ms.0 from job set of time 1586266240000 ms
2020-04-07 21:30:45,120 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:119
2020-04-07 21:30:45,138 INFO org.apache.spark.scheduler.DAGScheduler: Got job 0 (collect at Kafka2SparkStreaming2Kudu.scala:119) with 3 output partitions
2020-04-07 21:30:45,139 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 0 (collect at Kafka2SparkStreaming2Kudu.scala:119)
2020-04-07 21:30:45,139 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-07 21:30:45,140 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-07 21:30:45,147 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at Kafka2SparkStreaming2Kudu.scala:119), which has no missing parents
2020-04-07 21:30:45,205 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-07 21:30:45,494 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2046.0 B, free 4.1 GB)
2020-04-07 21:30:45,497 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:61347 (size: 2046.0 B, free: 4.1 GB)
2020-04-07 21:30:45,498 INFO org.apache.spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2020-04-07 21:30:45,522 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at Kafka2SparkStreaming2Kudu.scala:119) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-07 21:30:45,523 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 0.0 with 3 tasks
2020-04-07 21:30:45,554 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-07 21:30:45,557 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-07 21:30:45,559 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-07 21:30:45,569 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
2020-04-07 21:30:45,570 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 0.0 (TID 1)
2020-04-07 21:30:45,570 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 0.0 (TID 2)
2020-04-07 21:30:45,593 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 102 is the same as ending offset skipping PABUSI2 2
2020-04-07 21:30:45,593 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 99 is the same as ending offset skipping PABUSI2 0
2020-04-07 21:30:45,593 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 103 is the same as ending offset skipping PABUSI2 1
2020-04-07 21:30:45,606 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 704 bytes result sent to driver
2020-04-07 21:30:45,606 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 0.0 (TID 1). 704 bytes result sent to driver
2020-04-07 21:30:45,606 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 0.0 (TID 2). 704 bytes result sent to driver
2020-04-07 21:30:45,618 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 58 ms on localhost (executor driver) (1/3)
2020-04-07 21:30:45,620 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 63 ms on localhost (executor driver) (2/3)
2020-04-07 21:30:45,620 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 74 ms on localhost (executor driver) (3/3)
2020-04-07 21:30:45,620 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-04-07 21:30:45,625 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 0 (collect at Kafka2SparkStreaming2Kudu.scala:119) finished in 0.456 s
2020-04-07 21:30:45,630 INFO org.apache.spark.scheduler.DAGScheduler: Job 0 finished: collect at Kafka2SparkStreaming2Kudu.scala:119, took 0.509132 s
2020-04-07 21:30:45,668 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586266240000 ms.0 from job set of time 1586266240000 ms
2020-04-07 21:30:45,669 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 5.667 s for time 1586266240000 ms (execution: 0.617 s)
2020-04-07 21:30:45,674 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-07 21:30:45,679 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 
2020-04-07 21:30:45,704 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 24
2020-04-07 21:30:45,704 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 8
2020-04-07 21:30:45,704 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 3
2020-04-07 21:30:45,704 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 1
2020-04-07 21:30:45,704 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 10
2020-04-07 21:30:45,704 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 4
2020-04-07 21:30:45,704 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 12
2020-04-07 21:30:45,704 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 21
2020-04-07 21:30:45,704 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 23
2020-04-07 21:30:45,705 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 6
2020-04-07 21:30:45,705 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 15
2020-04-07 21:30:45,705 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 16
2020-04-07 21:30:45,705 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 11
2020-04-07 21:30:45,705 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 14
2020-04-07 21:30:45,705 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 22
2020-04-07 21:30:45,705 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 9
2020-04-07 21:30:45,705 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 20
2020-04-07 21:30:45,705 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 18
2020-04-07 21:30:45,705 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 5
2020-04-07 21:30:45,705 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 17
2020-04-07 21:30:45,705 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 19
2020-04-07 21:30:45,705 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 25
2020-04-07 21:30:45,705 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 13
2020-04-07 21:30:45,705 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 7
2020-04-07 21:30:45,706 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 2
2020-04-07 21:30:45,722 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.102:61347 in memory (size: 2046.0 B, free: 4.1 GB)
2020-04-07 21:30:50,008 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-2
2020-04-07 21:30:50,008 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-0
2020-04-07 21:30:50,008 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-1
2020-04-07 21:30:50,180 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-0 to offset 99.
2020-04-07 21:30:50,183 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-1 to offset 103.
2020-04-07 21:30:50,184 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-2 to offset 102.
2020-04-07 21:30:50,185 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586266250000 ms
2020-04-07 21:30:50,185 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586266250000 ms.0 from job set of time 1586266250000 ms
2020-04-07 21:30:50,190 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:119
2020-04-07 21:30:50,191 INFO org.apache.spark.scheduler.DAGScheduler: Got job 1 (collect at Kafka2SparkStreaming2Kudu.scala:119) with 3 output partitions
2020-04-07 21:30:50,191 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 1 (collect at Kafka2SparkStreaming2Kudu.scala:119)
2020-04-07 21:30:50,191 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-07 21:30:50,191 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-07 21:30:50,192 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at map at Kafka2SparkStreaming2Kudu.scala:119), which has no missing parents
2020-04-07 21:30:50,194 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-07 21:30:50,195 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2046.0 B, free 4.1 GB)
2020-04-07 21:30:50,196 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.102:61347 (size: 2046.0 B, free: 4.1 GB)
2020-04-07 21:30:50,196 INFO org.apache.spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2020-04-07 21:30:50,196 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at map at Kafka2SparkStreaming2Kudu.scala:119) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-07 21:30:50,197 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 1.0 with 3 tasks
2020-04-07 21:30:50,198 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-07 21:30:50,198 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 4, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-07 21:30:50,198 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 1.0 (TID 5, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-07 21:30:50,198 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 1.0 (TID 4)
2020-04-07 21:30:50,198 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 1.0 (TID 5)
2020-04-07 21:30:50,198 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 1.0 (TID 3)
2020-04-07 21:30:50,201 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 99 is the same as ending offset skipping PABUSI2 0
2020-04-07 21:30:50,201 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 102 is the same as ending offset skipping PABUSI2 2
2020-04-07 21:30:50,201 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 103 is the same as ending offset skipping PABUSI2 1
2020-04-07 21:30:50,202 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 1.0 (TID 4). 704 bytes result sent to driver
2020-04-07 21:30:50,202 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 1.0 (TID 3). 704 bytes result sent to driver
2020-04-07 21:30:50,202 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 1.0 (TID 5). 704 bytes result sent to driver
2020-04-07 21:30:50,202 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 4) in 4 ms on localhost (executor driver) (1/3)
2020-04-07 21:30:50,203 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 3) in 6 ms on localhost (executor driver) (2/3)
2020-04-07 21:30:50,203 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 1.0 (TID 5) in 5 ms on localhost (executor driver) (3/3)
2020-04-07 21:30:50,203 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
2020-04-07 21:30:50,204 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 1 (collect at Kafka2SparkStreaming2Kudu.scala:119) finished in 0.010 s
2020-04-07 21:30:50,204 INFO org.apache.spark.scheduler.DAGScheduler: Job 1 finished: collect at Kafka2SparkStreaming2Kudu.scala:119, took 0.013451 s
2020-04-07 21:30:50,209 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586266250000 ms.0 from job set of time 1586266250000 ms
2020-04-07 21:30:50,209 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.209 s for time 1586266250000 ms (execution: 0.024 s)
2020-04-07 21:30:50,210 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 0 from persistence list
2020-04-07 21:30:50,212 INFO org.apache.spark.storage.BlockManager: Removing RDD 0
2020-04-07 21:30:50,212 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-07 21:30:50,212 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 
2020-04-07 21:31:00,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-2
2020-04-07 21:31:00,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-0
2020-04-07 21:31:00,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-1
2020-04-07 21:31:00,178 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-1 to offset 110.
2020-04-07 21:31:00,179 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-0 to offset 106.
2020-04-07 21:31:00,181 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-2 to offset 108.
2020-04-07 21:31:00,181 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586266260000 ms
2020-04-07 21:31:00,182 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586266260000 ms.0 from job set of time 1586266260000 ms
2020-04-07 21:31:00,186 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:119
2020-04-07 21:31:00,187 INFO org.apache.spark.scheduler.DAGScheduler: Got job 2 (collect at Kafka2SparkStreaming2Kudu.scala:119) with 3 output partitions
2020-04-07 21:31:00,187 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 2 (collect at Kafka2SparkStreaming2Kudu.scala:119)
2020-04-07 21:31:00,187 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-07 21:31:00,187 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-07 21:31:00,188 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[7] at map at Kafka2SparkStreaming2Kudu.scala:119), which has no missing parents
2020-04-07 21:31:00,190 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-07 21:31:00,191 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 2041.0 B, free 4.1 GB)
2020-04-07 21:31:00,191 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.102:61347 (size: 2041.0 B, free: 4.1 GB)
2020-04-07 21:31:00,192 INFO org.apache.spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2020-04-07 21:31:00,193 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at map at Kafka2SparkStreaming2Kudu.scala:119) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-07 21:31:00,193 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 2.0 with 3 tasks
2020-04-07 21:31:00,193 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-07 21:31:00,194 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 2.0 (TID 7, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-07 21:31:00,194 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 2.0 (TID 8, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-07 21:31:00,194 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 2.0 (TID 6)
2020-04-07 21:31:00,194 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 2.0 (TID 8)
2020-04-07 21:31:00,194 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 2.0 (TID 7)
2020-04-07 21:31:00,196 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Computing topic PABUSI2, partition 1 offsets 103 -> 110
2020-04-07 21:31:00,197 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Computing topic PABUSI2, partition 2 offsets 102 -> 108
2020-04-07 21:31:00,197 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Computing topic PABUSI2, partition 0 offsets 99 -> 106
2020-04-07 21:31:00,199 INFO org.apache.spark.streaming.kafka010.KafkaDataConsumer: Initializing cache 16 64 0.75
2020-04-07 21:31:00,201 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-testgroup
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-07 21:31:00,206 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-07 21:31:00,206 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-07 21:31:00,206 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586266260206
2020-04-07 21:31:00,206 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup] Subscribed to partition(s): PABUSI2-2
2020-04-07 21:31:00,208 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-testgroup
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-07 21:31:00,209 INFO org.apache.spark.streaming.kafka010.InternalKafkaConsumer: Initial fetch for spark-executor-testgroup PABUSI2-2 102
2020-04-07 21:31:00,209 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup] Seeking to offset 102 for partition PABUSI2-2
2020-04-07 21:31:00,213 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-07 21:31:00,214 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-07 21:31:00,214 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586266260213
2020-04-07 21:31:00,214 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-3, groupId=spark-executor-testgroup] Subscribed to partition(s): PABUSI2-1
2020-04-07 21:31:00,214 INFO org.apache.spark.streaming.kafka010.InternalKafkaConsumer: Initial fetch for spark-executor-testgroup PABUSI2-1 103
2020-04-07 21:31:00,215 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-3, groupId=spark-executor-testgroup] Seeking to offset 103 for partition PABUSI2-1
2020-04-07 21:31:00,215 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-testgroup
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-07 21:31:00,219 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-07 21:31:00,219 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-07 21:31:00,219 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586266260219
2020-04-07 21:31:00,219 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-4, groupId=spark-executor-testgroup] Subscribed to partition(s): PABUSI2-0
2020-04-07 21:31:00,219 INFO org.apache.spark.streaming.kafka010.InternalKafkaConsumer: Initial fetch for spark-executor-testgroup PABUSI2-0 99
2020-04-07 21:31:00,220 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-4, groupId=spark-executor-testgroup] Seeking to offset 99 for partition PABUSI2-0
2020-04-07 21:31:00,735 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-3, groupId=spark-executor-testgroup] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-07 21:31:00,735 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-07 21:31:00,860 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-4, groupId=spark-executor-testgroup] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-07 21:31:01,657 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 2.0 (TID 7). 26021 bytes result sent to driver
2020-04-07 21:31:01,657 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 2.0 (TID 6). 32848 bytes result sent to driver
2020-04-07 21:31:01,660 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 2.0 (TID 7) in 1467 ms on localhost (executor driver) (1/3)
2020-04-07 21:31:01,661 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 6) in 1468 ms on localhost (executor driver) (2/3)
2020-04-07 21:31:01,783 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 2.0 (TID 8). 36211 bytes result sent to driver
2020-04-07 21:31:01,789 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 2.0 (TID 8) in 1595 ms on localhost (executor driver) (3/3)
2020-04-07 21:31:01,789 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
2020-04-07 21:31:01,790 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 2 (collect at Kafka2SparkStreaming2Kudu.scala:119) finished in 1.601 s
2020-04-07 21:31:01,790 INFO org.apache.spark.scheduler.DAGScheduler: Job 2 finished: collect at Kafka2SparkStreaming2Kudu.scala:119, took 1.603600 s
2020-04-07 21:31:01,804 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586266260000 ms.0 from job set of time 1586266260000 ms
2020-04-07 21:31:01,804 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 1.804 s for time 1586266260000 ms (execution: 1.622 s)
2020-04-07 21:31:01,804 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 3 from persistence list
2020-04-07 21:31:01,806 INFO org.apache.spark.storage.BlockManager: Removing RDD 3
2020-04-07 21:31:01,807 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-07 21:31:01,809 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586266240000 ms
2020-04-07 21:31:04,245 INFO org.apache.spark.streaming.StreamingContext: Invoking stop(stopGracefully=false) from shutdown hook
2020-04-07 21:31:04,248 INFO org.apache.spark.streaming.scheduler.ReceiverTracker: ReceiverTracker stopped
2020-04-07 21:31:04,249 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopping JobGenerator immediately
2020-04-07 21:31:04,250 INFO org.apache.spark.streaming.util.RecurringTimer: Stopped timer for JobGenerator after time 1586266260000
2020-04-07 21:31:04,433 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Member consumer-1-df405dc1-78de-4a7d-ac68-3f58ae1a405a sending LeaveGroup request to coordinator cw-instances-4.vpc.cloudera.com:9092 (id: 2147483594 rack: null)
2020-04-07 21:31:04,611 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopped JobGenerator
2020-04-07 21:31:04,614 INFO org.apache.spark.streaming.scheduler.JobScheduler: Stopped JobScheduler
2020-04-07 21:31:04,619 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@52d6cd34{/streaming,null,UNAVAILABLE,@Spark}
2020-04-07 21:31:04,620 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@1abfe081{/streaming/batch,null,UNAVAILABLE,@Spark}
2020-04-07 21:31:04,621 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@75a118e6{/static/streaming,null,UNAVAILABLE,@Spark}
2020-04-07 21:31:04,621 INFO org.apache.spark.streaming.StreamingContext: StreamingContext stopped successfully
2020-04-07 21:31:04,622 INFO org.apache.spark.SparkContext: Invoking stop() from shutdown hook
2020-04-07 21:31:04,627 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@72bd06ca{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-07 21:31:04,628 INFO org.apache.spark.ui.SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
2020-04-07 21:31:04,635 INFO org.apache.spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2020-04-07 21:31:04,648 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2020-04-07 21:31:04,648 INFO org.apache.spark.storage.BlockManager: BlockManager stopped
2020-04-07 21:31:04,649 INFO org.apache.spark.storage.BlockManagerMaster: BlockManagerMaster stopped
2020-04-07 21:31:04,651 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2020-04-07 21:31:04,660 INFO org.apache.spark.SparkContext: Successfully stopped SparkContext
2020-04-07 21:31:04,661 INFO org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2020-04-07 21:31:04,661 INFO org.apache.spark.util.ShutdownHookManager: Deleting directory /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/spark-9cdcefd5-6f5a-4cb6-b35c-38bda8d4d209
2020-04-07 21:34:33,190 WARN org.apache.spark.util.Utils: Your hostname, MacBook-Pro-CW.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)
2020-04-07 21:34:33,191 WARN org.apache.spark.util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
2020-04-07 21:34:33,232 INFO org.apache.spark.SparkContext: Running Spark version 2.4.0.cloudera2
2020-04-07 21:34:33,439 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-04-07 21:34:33,573 INFO org.apache.spark.SparkContext: Submitted application: Kafka2SparkStreaming2Kudu-kerberos
2020-04-07 21:34:33,616 INFO org.apache.spark.SecurityManager: Changing view acls to: godiswc
2020-04-07 21:34:33,616 INFO org.apache.spark.SecurityManager: Changing modify acls to: godiswc
2020-04-07 21:34:33,617 INFO org.apache.spark.SecurityManager: Changing view acls groups to: 
2020-04-07 21:34:33,617 INFO org.apache.spark.SecurityManager: Changing modify acls groups to: 
2020-04-07 21:34:33,617 INFO org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(godiswc); groups with view permissions: Set(); users  with modify permissions: Set(godiswc); groups with modify permissions: Set()
2020-04-07 21:34:33,762 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-07 21:34:33,849 INFO org.apache.spark.util.Utils: Successfully started service 'sparkDriver' on port 62213.
2020-04-07 21:34:33,866 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
2020-04-07 21:34:33,877 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
2020-04-07 21:34:33,879 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-04-07 21:34:33,880 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2020-04-07 21:34:33,891 INFO org.apache.spark.storage.DiskBlockManager: Created local directory at /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/blockmgr-da7333c2-d3ec-4680-9e86-2245de69591a
2020-04-07 21:34:33,924 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 4.1 GB
2020-04-07 21:34:33,936 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
2020-04-07 21:34:34,006 INFO org.spark_project.jetty.util.log: Logging initialized @1525ms
2020-04-07 21:34:34,066 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2019-04-30T04:42:08+08:00, git hash: e1bc35120a6617ee3df052294e433f3a25ce7097
2020-04-07 21:34:34,082 INFO org.spark_project.jetty.server.Server: Started @1603ms
2020-04-07 21:34:34,085 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-07 21:34:34,100 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@72bd06ca{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-07 21:34:34,101 INFO org.apache.spark.util.Utils: Successfully started service 'SparkUI' on port 4040.
2020-04-07 21:34:34,121 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@41fe9859{/jobs,null,AVAILABLE,@Spark}
2020-04-07 21:34:34,122 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@79e18e38{/jobs/json,null,AVAILABLE,@Spark}
2020-04-07 21:34:34,122 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@29a60c27{/jobs/job,null,AVAILABLE,@Spark}
2020-04-07 21:34:34,125 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1ca25c47{/jobs/job/json,null,AVAILABLE,@Spark}
2020-04-07 21:34:34,127 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fcacc0{/stages,null,AVAILABLE,@Spark}
2020-04-07 21:34:34,128 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@533b266e{/stages/json,null,AVAILABLE,@Spark}
2020-04-07 21:34:34,128 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6d1d4d7{/stages/stage,null,AVAILABLE,@Spark}
2020-04-07 21:34:34,130 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62679465{/stages/stage/json,null,AVAILABLE,@Spark}
2020-04-07 21:34:34,130 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a988392{/stages/pool,null,AVAILABLE,@Spark}
2020-04-07 21:34:34,131 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1d71006f{/stages/pool/json,null,AVAILABLE,@Spark}
2020-04-07 21:34:34,131 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5b6813df{/storage,null,AVAILABLE,@Spark}
2020-04-07 21:34:34,131 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5f2606b{/storage/json,null,AVAILABLE,@Spark}
2020-04-07 21:34:34,132 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2b58f754{/storage/rdd,null,AVAILABLE,@Spark}
2020-04-07 21:34:34,132 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3ebff828{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-04-07 21:34:34,133 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2552f2cb{/environment,null,AVAILABLE,@Spark}
2020-04-07 21:34:34,133 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@33352f32{/environment/json,null,AVAILABLE,@Spark}
2020-04-07 21:34:34,134 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5f3b9c57{/executors,null,AVAILABLE,@Spark}
2020-04-07 21:34:34,134 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1e044120{/executors/json,null,AVAILABLE,@Spark}
2020-04-07 21:34:34,135 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2cf23c81{/executors/threadDump,null,AVAILABLE,@Spark}
2020-04-07 21:34:34,135 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3624da92{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-04-07 21:34:34,141 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@35fe2125{/static,null,AVAILABLE,@Spark}
2020-04-07 21:34:34,141 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@27a5328c{/,null,AVAILABLE,@Spark}
2020-04-07 21:34:34,142 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1e5f4170{/api,null,AVAILABLE,@Spark}
2020-04-07 21:34:34,143 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1568159{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-04-07 21:34:34,143 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4fcee388{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-04-07 21:34:34,145 INFO org.apache.spark.ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
2020-04-07 21:34:34,231 INFO org.apache.spark.executor.Executor: Starting executor ID driver on host localhost
2020-04-07 21:34:34,307 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-07 21:34:34,308 INFO org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 62215.
2020-04-07 21:34:34,309 INFO org.apache.spark.network.netty.NettyBlockTransferService: Server created on 192.168.0.102:62215
2020-04-07 21:34:34,311 INFO org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-04-07 21:34:34,342 INFO org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 62215, None)
2020-04-07 21:34:34,345 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:62215 with 4.1 GB RAM, BlockManagerId(driver, 192.168.0.102, 62215, None)
2020-04-07 21:34:34,346 INFO org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 62215, None)
2020-04-07 21:34:34,347 INFO org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 62215, None)
2020-04-07 21:34:34,517 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@46baf579{/metrics/json,null,AVAILABLE,@Spark}
2020-04-07 21:34:34,778 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding enable.auto.commit to false for executor
2020-04-07 21:34:34,778 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding auto.offset.reset to none for executor
2020-04-07 21:34:34,778 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding executor group.id to spark-executor-testgroup
2020-04-07 21:34:34,779 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
2020-04-07 21:34:36,521 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Slide time = 10000 ms
2020-04-07 21:34:36,521 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Storage level = Serialized 1x Replicated
2020-04-07 21:34:36,522 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Checkpoint interval = null
2020-04-07 21:34:36,523 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Remember interval = 10000 ms
2020-04-07 21:34:36,524 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@493ab045
2020-04-07 21:34:36,524 INFO org.apache.spark.streaming.dstream.ForEachDStream: Slide time = 10000 ms
2020-04-07 21:34:36,524 INFO org.apache.spark.streaming.dstream.ForEachDStream: Storage level = Serialized 1x Replicated
2020-04-07 21:34:36,524 INFO org.apache.spark.streaming.dstream.ForEachDStream: Checkpoint interval = null
2020-04-07 21:34:36,524 INFO org.apache.spark.streaming.dstream.ForEachDStream: Remember interval = 10000 ms
2020-04-07 21:34:36,524 INFO org.apache.spark.streaming.dstream.ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@17a097dd
2020-04-07 21:34:36,613 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = testgroup
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-07 21:34:36,689 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-07 21:34:36,689 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-07 21:34:36,689 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586266476688
2020-04-07 21:34:36,691 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-1, groupId=testgroup] Subscribed to topic(s): PABUSI2
2020-04-07 21:34:37,496 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-1, groupId=testgroup] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-07 21:34:37,498 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Discovered group coordinator cw-instances-4.vpc.cloudera.com:9092 (id: 2147483594 rack: null)
2020-04-07 21:34:37,501 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Revoking previously assigned partitions []
2020-04-07 21:34:37,501 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] (Re-)joining group
2020-04-07 21:34:38,019 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] (Re-)joining group
2020-04-07 21:34:41,419 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Successfully joined group with generation 19
2020-04-07 21:34:41,423 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Setting newly assigned partitions: PABUSI2-2, PABUSI2-0, PABUSI2-1
2020-04-07 21:34:41,606 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Setting offset for partition PABUSI2-2 to the committed offset FetchPosition{offset=108, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-2.vpc.cloudera.com:9092 (id: 52 rack: null), epoch=0}}
2020-04-07 21:34:41,607 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Setting offset for partition PABUSI2-0 to the committed offset FetchPosition{offset=106, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-4.vpc.cloudera.com:9092 (id: 53 rack: null), epoch=0}}
2020-04-07 21:34:41,607 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Setting offset for partition PABUSI2-1 to the committed offset FetchPosition{offset=110, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-3.vpc.cloudera.com:9092 (id: 54 rack: null), epoch=0}}
2020-04-07 21:34:41,613 INFO org.apache.spark.streaming.util.RecurringTimer: Started timer for JobGenerator at time 1586266480000
2020-04-07 21:34:41,614 INFO org.apache.spark.streaming.scheduler.JobGenerator: Started JobGenerator at 1586266480000 ms
2020-04-07 21:34:41,615 INFO org.apache.spark.streaming.scheduler.JobScheduler: Started JobScheduler
2020-04-07 21:34:41,618 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@52d6cd34{/streaming,null,AVAILABLE,@Spark}
2020-04-07 21:34:41,619 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@715d6168{/streaming/json,null,AVAILABLE,@Spark}
2020-04-07 21:34:41,620 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1abfe081{/streaming/batch,null,AVAILABLE,@Spark}
2020-04-07 21:34:41,621 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2a685eba{/streaming/batch/json,null,AVAILABLE,@Spark}
2020-04-07 21:34:41,622 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@75a118e6{/static/streaming,null,AVAILABLE,@Spark}
2020-04-07 21:34:41,622 INFO org.apache.spark.streaming.StreamingContext: StreamingContext started
2020-04-07 21:34:41,662 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-0
2020-04-07 21:34:41,663 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-2
2020-04-07 21:34:41,663 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-1
2020-04-07 21:34:42,526 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-1 to offset 116.
2020-04-07 21:34:42,527 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-0 to offset 113.
2020-04-07 21:34:42,979 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-2 to offset 115.
2020-04-07 21:34:43,005 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586266480000 ms
2020-04-07 21:34:43,007 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586266480000 ms.0 from job set of time 1586266480000 ms
2020-04-07 21:34:43,049 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:126
2020-04-07 21:34:43,062 INFO org.apache.spark.scheduler.DAGScheduler: Got job 0 (collect at Kafka2SparkStreaming2Kudu.scala:126) with 3 output partitions
2020-04-07 21:34:43,062 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 0 (collect at Kafka2SparkStreaming2Kudu.scala:126)
2020-04-07 21:34:43,062 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-07 21:34:43,063 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-07 21:34:43,067 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at Kafka2SparkStreaming2Kudu.scala:119), which has no missing parents
2020-04-07 21:34:43,113 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-07 21:34:43,368 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-07 21:34:43,370 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:62215 (size: 2.0 KB, free: 4.1 GB)
2020-04-07 21:34:43,371 INFO org.apache.spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2020-04-07 21:34:43,384 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at Kafka2SparkStreaming2Kudu.scala:119) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-07 21:34:43,384 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 0.0 with 3 tasks
2020-04-07 21:34:43,414 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-07 21:34:43,416 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-07 21:34:43,417 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-07 21:34:43,425 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 0.0 (TID 1)
2020-04-07 21:34:43,425 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 0.0 (TID 2)
2020-04-07 21:34:43,426 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
2020-04-07 21:34:43,452 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Computing topic PABUSI2, partition 1 offsets 110 -> 116
2020-04-07 21:34:43,452 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Computing topic PABUSI2, partition 0 offsets 106 -> 113
2020-04-07 21:34:43,452 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Computing topic PABUSI2, partition 2 offsets 108 -> 115
2020-04-07 21:34:43,455 INFO org.apache.spark.streaming.kafka010.KafkaDataConsumer: Initializing cache 16 64 0.75
2020-04-07 21:34:43,458 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-testgroup
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-07 21:34:43,463 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-07 21:34:43,463 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-07 21:34:43,463 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586266483463
2020-04-07 21:34:43,463 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup] Subscribed to partition(s): PABUSI2-1
2020-04-07 21:34:43,465 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-testgroup
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-07 21:34:43,466 INFO org.apache.spark.streaming.kafka010.InternalKafkaConsumer: Initial fetch for spark-executor-testgroup PABUSI2-1 110
2020-04-07 21:34:43,467 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup] Seeking to offset 110 for partition PABUSI2-1
2020-04-07 21:34:43,471 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-07 21:34:43,471 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-07 21:34:43,471 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586266483471
2020-04-07 21:34:43,472 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-3, groupId=spark-executor-testgroup] Subscribed to partition(s): PABUSI2-2
2020-04-07 21:34:43,472 INFO org.apache.spark.streaming.kafka010.InternalKafkaConsumer: Initial fetch for spark-executor-testgroup PABUSI2-2 108
2020-04-07 21:34:43,472 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-3, groupId=spark-executor-testgroup] Seeking to offset 108 for partition PABUSI2-2
2020-04-07 21:34:43,472 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-testgroup
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-07 21:34:43,477 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-07 21:34:43,477 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-07 21:34:43,477 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586266483477
2020-04-07 21:34:43,478 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-4, groupId=spark-executor-testgroup] Subscribed to partition(s): PABUSI2-0
2020-04-07 21:34:43,478 INFO org.apache.spark.streaming.kafka010.InternalKafkaConsumer: Initial fetch for spark-executor-testgroup PABUSI2-0 106
2020-04-07 21:34:43,478 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-4, groupId=spark-executor-testgroup] Seeking to offset 106 for partition PABUSI2-0
2020-04-07 21:34:43,994 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-07 21:34:43,994 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-3, groupId=spark-executor-testgroup] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-07 21:34:44,002 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-4, groupId=spark-executor-testgroup] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-07 21:34:44,968 ERROR org.apache.spark.executor.Executor: Exception in task 2.0 in stage 0.0 (TID 2)
java.lang.ClassCastException: scala.collection.immutable.HashMap$HashTrieMap cannot be cast to java.lang.String
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:124)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:119)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:363)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1174)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:296)
	at scala.collection.AbstractIterator.to(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:275)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1174)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-07 21:34:44,968 ERROR org.apache.spark.executor.Executor: Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.ClassCastException: scala.collection.immutable.HashMap$HashTrieMap cannot be cast to java.lang.String
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:124)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:119)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:363)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1174)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:296)
	at scala.collection.AbstractIterator.to(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:275)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1174)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-07 21:34:44,968 ERROR org.apache.spark.executor.Executor: Exception in task 1.0 in stage 0.0 (TID 1)
java.lang.ClassCastException: scala.collection.immutable.HashMap$HashTrieMap cannot be cast to java.lang.String
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:124)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:119)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:363)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1174)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:296)
	at scala.collection.AbstractIterator.to(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:275)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1174)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-07 21:34:44,985 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1, localhost, executor driver): java.lang.ClassCastException: scala.collection.immutable.HashMap$HashTrieMap cannot be cast to java.lang.String
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:124)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:119)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:363)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1174)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:296)
	at scala.collection.AbstractIterator.to(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:275)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1174)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2020-04-07 21:34:44,986 ERROR org.apache.spark.scheduler.TaskSetManager: Task 1 in stage 0.0 failed 1 times; aborting job
2020-04-07 21:34:44,987 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-04-07 21:34:44,988 INFO org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) on localhost, executor driver: java.lang.ClassCastException (scala.collection.immutable.HashMap$HashTrieMap cannot be cast to java.lang.String) [duplicate 1]
2020-04-07 21:34:44,988 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-04-07 21:34:44,988 INFO org.apache.spark.scheduler.TaskSetManager: Lost task 2.0 in stage 0.0 (TID 2) on localhost, executor driver: java.lang.ClassCastException (scala.collection.immutable.HashMap$HashTrieMap cannot be cast to java.lang.String) [duplicate 2]
2020-04-07 21:34:44,988 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-04-07 21:34:44,991 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Cancelling stage 0
2020-04-07 21:34:44,991 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Killing all running tasks in stage 0: Stage cancelled
2020-04-07 21:34:44,992 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 0 (collect at Kafka2SparkStreaming2Kudu.scala:126) failed in 1.911 s due to Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1, localhost, executor driver): java.lang.ClassCastException: scala.collection.immutable.HashMap$HashTrieMap cannot be cast to java.lang.String
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:124)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:119)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:363)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1174)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:296)
	at scala.collection.AbstractIterator.to(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:275)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1174)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
2020-04-07 21:34:44,995 INFO org.apache.spark.scheduler.DAGScheduler: Job 0 failed: collect at Kafka2SparkStreaming2Kudu.scala:126, took 1.946257 s
2020-04-07 21:34:44,997 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586266480000 ms.0 from job set of time 1586266480000 ms
2020-04-07 21:34:44,999 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1586266480000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1, localhost, executor driver): java.lang.ClassCastException: scala.collection.immutable.HashMap$HashTrieMap cannot be cast to java.lang.String
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:124)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:119)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:363)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1174)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:296)
	at scala.collection.AbstractIterator.to(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:275)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1174)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:245)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:944)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1.apply(Kafka2SparkStreaming2Kudu.scala:126)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1.apply(Kafka2SparkStreaming2Kudu.scala:117)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:191)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassCastException: scala.collection.immutable.HashMap$HashTrieMap cannot be cast to java.lang.String
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:124)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:119)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:363)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1174)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:296)
	at scala.collection.AbstractIterator.to(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:275)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1174)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	... 3 more
2020-04-07 21:34:45,008 INFO org.apache.spark.streaming.StreamingContext: Invoking stop(stopGracefully=false) from shutdown hook
2020-04-07 21:34:45,010 INFO org.apache.spark.streaming.scheduler.ReceiverTracker: ReceiverTracker stopped
2020-04-07 21:34:45,010 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopping JobGenerator immediately
2020-04-07 21:34:45,011 INFO org.apache.spark.streaming.util.RecurringTimer: Stopped timer for JobGenerator after time 1586266480000
2020-04-07 21:34:45,186 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Member consumer-1-74738aaf-013f-4214-9d91-212b28059a6a sending LeaveGroup request to coordinator cw-instances-4.vpc.cloudera.com:9092 (id: 2147483594 rack: null)
2020-04-07 21:34:45,367 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopped JobGenerator
2020-04-07 21:34:45,370 INFO org.apache.spark.streaming.scheduler.JobScheduler: Stopped JobScheduler
2020-04-07 21:34:45,374 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@52d6cd34{/streaming,null,UNAVAILABLE,@Spark}
2020-04-07 21:34:45,375 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@1abfe081{/streaming/batch,null,UNAVAILABLE,@Spark}
2020-04-07 21:34:45,376 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@75a118e6{/static/streaming,null,UNAVAILABLE,@Spark}
2020-04-07 21:34:45,377 INFO org.apache.spark.streaming.StreamingContext: StreamingContext stopped successfully
2020-04-07 21:34:45,377 INFO org.apache.spark.SparkContext: Invoking stop() from shutdown hook
2020-04-07 21:34:45,382 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@72bd06ca{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-07 21:34:45,383 INFO org.apache.spark.ui.SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
2020-04-07 21:34:45,390 INFO org.apache.spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2020-04-07 21:34:45,401 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2020-04-07 21:34:45,402 INFO org.apache.spark.storage.BlockManager: BlockManager stopped
2020-04-07 21:34:45,406 INFO org.apache.spark.storage.BlockManagerMaster: BlockManagerMaster stopped
2020-04-07 21:34:45,408 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2020-04-07 21:34:45,418 INFO org.apache.spark.SparkContext: Successfully stopped SparkContext
2020-04-07 21:34:45,418 INFO org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2020-04-07 21:34:45,419 INFO org.apache.spark.util.ShutdownHookManager: Deleting directory /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/spark-73ce887f-d7e1-4e5c-bca1-9ea5e4a3909d
2020-04-07 21:35:59,242 WARN org.apache.spark.util.Utils: Your hostname, MacBook-Pro-CW.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)
2020-04-07 21:35:59,242 WARN org.apache.spark.util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
2020-04-07 21:35:59,275 INFO org.apache.spark.SparkContext: Running Spark version 2.4.0.cloudera2
2020-04-07 21:35:59,467 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-04-07 21:35:59,577 INFO org.apache.spark.SparkContext: Submitted application: Kafka2SparkStreaming2Kudu-kerberos
2020-04-07 21:35:59,617 INFO org.apache.spark.SecurityManager: Changing view acls to: godiswc
2020-04-07 21:35:59,617 INFO org.apache.spark.SecurityManager: Changing modify acls to: godiswc
2020-04-07 21:35:59,617 INFO org.apache.spark.SecurityManager: Changing view acls groups to: 
2020-04-07 21:35:59,618 INFO org.apache.spark.SecurityManager: Changing modify acls groups to: 
2020-04-07 21:35:59,618 INFO org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(godiswc); groups with view permissions: Set(); users  with modify permissions: Set(godiswc); groups with modify permissions: Set()
2020-04-07 21:35:59,735 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-07 21:35:59,812 INFO org.apache.spark.util.Utils: Successfully started service 'sparkDriver' on port 62543.
2020-04-07 21:35:59,829 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
2020-04-07 21:35:59,841 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
2020-04-07 21:35:59,843 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-04-07 21:35:59,843 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2020-04-07 21:35:59,855 INFO org.apache.spark.storage.DiskBlockManager: Created local directory at /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/blockmgr-619b0cd7-2303-408f-8124-3fdd5a1a3b8b
2020-04-07 21:35:59,892 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 4.1 GB
2020-04-07 21:35:59,902 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
2020-04-07 21:35:59,960 INFO org.spark_project.jetty.util.log: Logging initialized @1287ms
2020-04-07 21:35:59,997 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2019-04-30T04:42:08+08:00, git hash: e1bc35120a6617ee3df052294e433f3a25ce7097
2020-04-07 21:36:00,007 INFO org.spark_project.jetty.server.Server: Started @1336ms
2020-04-07 21:36:00,009 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-07 21:36:00,018 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@5dbe30be{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-07 21:36:00,018 INFO org.apache.spark.util.Utils: Successfully started service 'SparkUI' on port 4040.
2020-04-07 21:36:00,040 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c67e137{/jobs,null,AVAILABLE,@Spark}
2020-04-07 21:36:00,041 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1849db1a{/jobs/json,null,AVAILABLE,@Spark}
2020-04-07 21:36:00,042 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@69c79f09{/jobs/job,null,AVAILABLE,@Spark}
2020-04-07 21:36:00,045 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@533b266e{/jobs/job/json,null,AVAILABLE,@Spark}
2020-04-07 21:36:00,049 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6d1d4d7{/stages,null,AVAILABLE,@Spark}
2020-04-07 21:36:00,050 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@89ff02e{/stages/json,null,AVAILABLE,@Spark}
2020-04-07 21:36:00,051 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6865c751{/stages/stage,null,AVAILABLE,@Spark}
2020-04-07 21:36:00,053 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1d71006f{/stages/stage/json,null,AVAILABLE,@Spark}
2020-04-07 21:36:00,054 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5b6813df{/stages/pool,null,AVAILABLE,@Spark}
2020-04-07 21:36:00,055 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5f2606b{/stages/pool/json,null,AVAILABLE,@Spark}
2020-04-07 21:36:00,055 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2b58f754{/storage,null,AVAILABLE,@Spark}
2020-04-07 21:36:00,056 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3ebff828{/storage/json,null,AVAILABLE,@Spark}
2020-04-07 21:36:00,057 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2552f2cb{/storage/rdd,null,AVAILABLE,@Spark}
2020-04-07 21:36:00,058 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@33352f32{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-04-07 21:36:00,059 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5f3b9c57{/environment,null,AVAILABLE,@Spark}
2020-04-07 21:36:00,061 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1e044120{/environment/json,null,AVAILABLE,@Spark}
2020-04-07 21:36:00,064 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2cf23c81{/executors,null,AVAILABLE,@Spark}
2020-04-07 21:36:00,065 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3624da92{/executors/json,null,AVAILABLE,@Spark}
2020-04-07 21:36:00,066 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@35fe2125{/executors/threadDump,null,AVAILABLE,@Spark}
2020-04-07 21:36:00,067 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@94f6bfb{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-04-07 21:36:00,074 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@34645867{/static,null,AVAILABLE,@Spark}
2020-04-07 21:36:00,075 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c345c5f{/,null,AVAILABLE,@Spark}
2020-04-07 21:36:00,076 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6b5966e1{/api,null,AVAILABLE,@Spark}
2020-04-07 21:36:00,077 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6f80fafe{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-04-07 21:36:00,077 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3af17be2{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-04-07 21:36:00,080 INFO org.apache.spark.ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
2020-04-07 21:36:00,203 INFO org.apache.spark.executor.Executor: Starting executor ID driver on host localhost
2020-04-07 21:36:00,274 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-07 21:36:00,275 INFO org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 62544.
2020-04-07 21:36:00,276 INFO org.apache.spark.network.netty.NettyBlockTransferService: Server created on 192.168.0.102:62544
2020-04-07 21:36:00,277 INFO org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-04-07 21:36:00,312 INFO org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 62544, None)
2020-04-07 21:36:00,316 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:62544 with 4.1 GB RAM, BlockManagerId(driver, 192.168.0.102, 62544, None)
2020-04-07 21:36:00,321 INFO org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 62544, None)
2020-04-07 21:36:00,321 INFO org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 62544, None)
2020-04-07 21:36:00,483 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4f7c0be3{/metrics/json,null,AVAILABLE,@Spark}
2020-04-07 21:36:00,804 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding enable.auto.commit to false for executor
2020-04-07 21:36:00,805 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding auto.offset.reset to none for executor
2020-04-07 21:36:00,805 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding executor group.id to spark-executor-testgroup
2020-04-07 21:36:00,806 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
2020-04-07 21:36:02,550 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Slide time = 10000 ms
2020-04-07 21:36:02,551 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Storage level = Serialized 1x Replicated
2020-04-07 21:36:02,551 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Checkpoint interval = null
2020-04-07 21:36:02,551 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Remember interval = 10000 ms
2020-04-07 21:36:02,552 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@176d85f5
2020-04-07 21:36:02,552 INFO org.apache.spark.streaming.dstream.ForEachDStream: Slide time = 10000 ms
2020-04-07 21:36:02,552 INFO org.apache.spark.streaming.dstream.ForEachDStream: Storage level = Serialized 1x Replicated
2020-04-07 21:36:02,552 INFO org.apache.spark.streaming.dstream.ForEachDStream: Checkpoint interval = null
2020-04-07 21:36:02,552 INFO org.apache.spark.streaming.dstream.ForEachDStream: Remember interval = 10000 ms
2020-04-07 21:36:02,552 INFO org.apache.spark.streaming.dstream.ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@73e2c43c
2020-04-07 21:36:02,594 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = testgroup
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-07 21:36:02,643 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-07 21:36:02,643 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-07 21:36:02,643 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586266562642
2020-04-07 21:36:02,645 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-1, groupId=testgroup] Subscribed to topic(s): PABUSI2
2020-04-07 21:36:03,302 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-1, groupId=testgroup] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-07 21:36:03,303 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Discovered group coordinator cw-instances-4.vpc.cloudera.com:9092 (id: 2147483594 rack: null)
2020-04-07 21:36:03,306 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Revoking previously assigned partitions []
2020-04-07 21:36:03,306 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] (Re-)joining group
2020-04-07 21:36:03,822 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] (Re-)joining group
2020-04-07 21:36:07,229 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Successfully joined group with generation 21
2020-04-07 21:36:07,233 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Setting newly assigned partitions: PABUSI2-2, PABUSI2-0, PABUSI2-1
2020-04-07 21:36:07,418 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Setting offset for partition PABUSI2-2 to the committed offset FetchPosition{offset=115, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-2.vpc.cloudera.com:9092 (id: 52 rack: null), epoch=0}}
2020-04-07 21:36:07,419 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Setting offset for partition PABUSI2-0 to the committed offset FetchPosition{offset=113, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-4.vpc.cloudera.com:9092 (id: 53 rack: null), epoch=0}}
2020-04-07 21:36:07,419 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Setting offset for partition PABUSI2-1 to the committed offset FetchPosition{offset=116, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-3.vpc.cloudera.com:9092 (id: 54 rack: null), epoch=0}}
2020-04-07 21:36:07,428 INFO org.apache.spark.streaming.util.RecurringTimer: Started timer for JobGenerator at time 1586266570000
2020-04-07 21:36:07,428 INFO org.apache.spark.streaming.scheduler.JobGenerator: Started JobGenerator at 1586266570000 ms
2020-04-07 21:36:07,429 INFO org.apache.spark.streaming.scheduler.JobScheduler: Started JobScheduler
2020-04-07 21:36:07,434 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@27b2faa6{/streaming,null,AVAILABLE,@Spark}
2020-04-07 21:36:07,435 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6428591a{/streaming/json,null,AVAILABLE,@Spark}
2020-04-07 21:36:07,436 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@c2e3264{/streaming/batch,null,AVAILABLE,@Spark}
2020-04-07 21:36:07,437 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@107f4980{/streaming/batch/json,null,AVAILABLE,@Spark}
2020-04-07 21:36:07,437 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6014a9ba{/static/streaming,null,AVAILABLE,@Spark}
2020-04-07 21:36:07,438 INFO org.apache.spark.streaming.StreamingContext: StreamingContext started
2020-04-07 21:36:10,044 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-0
2020-04-07 21:36:10,045 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-1
2020-04-07 21:36:10,045 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-2
2020-04-07 21:36:10,228 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-2 to offset 115.
2020-04-07 21:36:10,229 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-0 to offset 113.
2020-04-07 21:36:10,229 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-1 to offset 116.
2020-04-07 21:36:10,255 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586266570000 ms
2020-04-07 21:36:10,258 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586266570000 ms.0 from job set of time 1586266570000 ms
2020-04-07 21:36:10,309 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:127
2020-04-07 21:36:10,327 INFO org.apache.spark.scheduler.DAGScheduler: Got job 0 (collect at Kafka2SparkStreaming2Kudu.scala:127) with 3 output partitions
2020-04-07 21:36:10,327 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 0 (collect at Kafka2SparkStreaming2Kudu.scala:127)
2020-04-07 21:36:10,327 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-07 21:36:10,328 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-07 21:36:10,334 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at Kafka2SparkStreaming2Kudu.scala:119), which has no missing parents
2020-04-07 21:36:10,382 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-07 21:36:10,634 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-07 21:36:10,635 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:62544 (size: 2.0 KB, free: 4.1 GB)
2020-04-07 21:36:10,637 INFO org.apache.spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2020-04-07 21:36:10,649 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at Kafka2SparkStreaming2Kudu.scala:119) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-07 21:36:10,650 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 0.0 with 3 tasks
2020-04-07 21:36:10,682 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-07 21:36:10,684 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-07 21:36:10,685 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-07 21:36:10,695 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
2020-04-07 21:36:10,695 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 0.0 (TID 1)
2020-04-07 21:36:10,696 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 0.0 (TID 2)
2020-04-07 21:36:10,722 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 115 is the same as ending offset skipping PABUSI2 2
2020-04-07 21:36:10,722 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 116 is the same as ending offset skipping PABUSI2 1
2020-04-07 21:36:10,722 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 113 is the same as ending offset skipping PABUSI2 0
2020-04-07 21:36:10,733 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 708 bytes result sent to driver
2020-04-07 21:36:10,733 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 0.0 (TID 1). 708 bytes result sent to driver
2020-04-07 21:36:10,733 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 0.0 (TID 2). 708 bytes result sent to driver
2020-04-07 21:36:10,738 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 64 ms on localhost (executor driver) (1/3)
2020-04-07 21:36:10,739 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 55 ms on localhost (executor driver) (2/3)
2020-04-07 21:36:10,740 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 56 ms on localhost (executor driver) (3/3)
2020-04-07 21:36:10,740 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-04-07 21:36:10,744 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 0 (collect at Kafka2SparkStreaming2Kudu.scala:127) finished in 0.391 s
2020-04-07 21:36:10,748 INFO org.apache.spark.scheduler.DAGScheduler: Job 0 finished: collect at Kafka2SparkStreaming2Kudu.scala:127, took 0.438775 s
2020-04-07 21:36:10,776 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.102:62544 in memory (size: 2.0 KB, free: 4.1 GB)
2020-04-07 21:36:10,791 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586266570000 ms.0 from job set of time 1586266570000 ms
2020-04-07 21:36:10,792 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.791 s for time 1586266570000 ms (execution: 0.534 s)
2020-04-07 21:36:10,796 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-07 21:36:10,799 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 
2020-04-07 21:36:20,008 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-0
2020-04-07 21:36:20,008 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-1
2020-04-07 21:36:20,008 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-2
2020-04-07 21:36:20,181 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-2 to offset 122.
2020-04-07 21:36:20,182 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-0 to offset 120.
2020-04-07 21:36:20,183 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-1 to offset 122.
2020-04-07 21:36:20,184 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586266580000 ms
2020-04-07 21:36:20,185 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586266580000 ms.0 from job set of time 1586266580000 ms
2020-04-07 21:36:20,191 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:127
2020-04-07 21:36:20,192 INFO org.apache.spark.scheduler.DAGScheduler: Got job 1 (collect at Kafka2SparkStreaming2Kudu.scala:127) with 3 output partitions
2020-04-07 21:36:20,192 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 1 (collect at Kafka2SparkStreaming2Kudu.scala:127)
2020-04-07 21:36:20,192 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-07 21:36:20,192 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-07 21:36:20,193 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at map at Kafka2SparkStreaming2Kudu.scala:119), which has no missing parents
2020-04-07 21:36:20,196 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-07 21:36:20,198 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-07 21:36:20,198 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.102:62544 (size: 2.0 KB, free: 4.1 GB)
2020-04-07 21:36:20,199 INFO org.apache.spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2020-04-07 21:36:20,200 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at map at Kafka2SparkStreaming2Kudu.scala:119) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-07 21:36:20,200 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 1.0 with 3 tasks
2020-04-07 21:36:20,201 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-07 21:36:20,201 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 4, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-07 21:36:20,201 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 1.0 (TID 5, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-07 21:36:20,202 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 1.0 (TID 3)
2020-04-07 21:36:20,202 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 1.0 (TID 4)
2020-04-07 21:36:20,202 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 1.0 (TID 5)
2020-04-07 21:36:20,205 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Computing topic PABUSI2, partition 0 offsets 113 -> 120
2020-04-07 21:36:20,205 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Computing topic PABUSI2, partition 2 offsets 115 -> 122
2020-04-07 21:36:20,205 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Computing topic PABUSI2, partition 1 offsets 116 -> 122
2020-04-07 21:36:20,209 INFO org.apache.spark.streaming.kafka010.KafkaDataConsumer: Initializing cache 16 64 0.75
2020-04-07 21:36:20,211 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-testgroup
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-07 21:36:20,216 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-07 21:36:20,217 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-07 21:36:20,217 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586266580216
2020-04-07 21:36:20,217 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup] Subscribed to partition(s): PABUSI2-0
2020-04-07 21:36:20,219 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-testgroup
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-07 21:36:20,220 INFO org.apache.spark.streaming.kafka010.InternalKafkaConsumer: Initial fetch for spark-executor-testgroup PABUSI2-0 113
2020-04-07 21:36:20,221 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup] Seeking to offset 113 for partition PABUSI2-0
2020-04-07 21:36:20,225 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-07 21:36:20,225 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-07 21:36:20,225 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586266580225
2020-04-07 21:36:20,226 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-3, groupId=spark-executor-testgroup] Subscribed to partition(s): PABUSI2-2
2020-04-07 21:36:20,226 INFO org.apache.spark.streaming.kafka010.InternalKafkaConsumer: Initial fetch for spark-executor-testgroup PABUSI2-2 115
2020-04-07 21:36:20,226 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-3, groupId=spark-executor-testgroup] Seeking to offset 115 for partition PABUSI2-2
2020-04-07 21:36:20,226 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-testgroup
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-07 21:36:20,231 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-07 21:36:20,232 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-07 21:36:20,232 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586266580231
2020-04-07 21:36:20,232 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-4, groupId=spark-executor-testgroup] Subscribed to partition(s): PABUSI2-1
2020-04-07 21:36:20,232 INFO org.apache.spark.streaming.kafka010.InternalKafkaConsumer: Initial fetch for spark-executor-testgroup PABUSI2-1 116
2020-04-07 21:36:20,233 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-4, groupId=spark-executor-testgroup] Seeking to offset 116 for partition PABUSI2-1
2020-04-07 21:36:20,743 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-07 21:36:20,751 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-3, groupId=spark-executor-testgroup] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-07 21:36:20,753 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-4, groupId=spark-executor-testgroup] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-07 21:36:21,656 ERROR org.apache.spark.executor.Executor: Exception in task 0.0 in stage 1.0 (TID 3)
java.lang.ClassCastException: scala.collection.immutable.HashMap$HashTrieMap cannot be cast to java.lang.String
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:124)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:119)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:363)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1174)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:296)
	at scala.collection.AbstractIterator.to(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:275)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1174)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-07 21:36:21,656 ERROR org.apache.spark.executor.Executor: Exception in task 2.0 in stage 1.0 (TID 5)
java.lang.ClassCastException: scala.collection.immutable.HashMap$HashTrieMap cannot be cast to java.lang.String
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:124)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:119)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:363)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1174)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:296)
	at scala.collection.AbstractIterator.to(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:275)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1174)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-07 21:36:21,679 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 1.0 (TID 3, localhost, executor driver): java.lang.ClassCastException: scala.collection.immutable.HashMap$HashTrieMap cannot be cast to java.lang.String
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:124)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:119)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:363)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1174)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:296)
	at scala.collection.AbstractIterator.to(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:275)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1174)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2020-04-07 21:36:21,682 ERROR org.apache.spark.scheduler.TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job
2020-04-07 21:36:21,692 INFO org.apache.spark.scheduler.TaskSetManager: Lost task 2.0 in stage 1.0 (TID 5) on localhost, executor driver: java.lang.ClassCastException (scala.collection.immutable.HashMap$HashTrieMap cannot be cast to java.lang.String) [duplicate 1]
2020-04-07 21:36:21,693 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 19
2020-04-07 21:36:21,693 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 14
2020-04-07 21:36:21,693 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 24
2020-04-07 21:36:21,693 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 10
2020-04-07 21:36:21,693 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 17
2020-04-07 21:36:21,694 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 13
2020-04-07 21:36:21,694 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 15
2020-04-07 21:36:21,694 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 3
2020-04-07 21:36:21,694 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 12
2020-04-07 21:36:21,694 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 9
2020-04-07 21:36:21,694 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 11
2020-04-07 21:36:21,694 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 18
2020-04-07 21:36:21,694 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 2
2020-04-07 21:36:21,694 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 21
2020-04-07 21:36:21,694 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 23
2020-04-07 21:36:21,694 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 4
2020-04-07 21:36:21,694 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 22
2020-04-07 21:36:21,694 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 1
2020-04-07 21:36:21,694 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 25
2020-04-07 21:36:21,695 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 8
2020-04-07 21:36:21,695 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 6
2020-04-07 21:36:21,695 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 20
2020-04-07 21:36:21,695 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 5
2020-04-07 21:36:21,695 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Cancelling stage 1
2020-04-07 21:36:21,695 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 16
2020-04-07 21:36:21,695 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 7
2020-04-07 21:36:21,695 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Killing all running tasks in stage 1: Stage cancelled
2020-04-07 21:36:21,697 INFO org.apache.spark.executor.Executor: Executor is trying to kill task 1.0 in stage 1.0 (TID 4), reason: Stage cancelled
2020-04-07 21:36:21,698 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Stage 1 was cancelled
2020-04-07 21:36:21,698 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 1 (collect at Kafka2SparkStreaming2Kudu.scala:127) failed in 1.503 s due to Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 3, localhost, executor driver): java.lang.ClassCastException: scala.collection.immutable.HashMap$HashTrieMap cannot be cast to java.lang.String
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:124)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:119)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:363)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1174)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:296)
	at scala.collection.AbstractIterator.to(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:275)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1174)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
2020-04-07 21:36:21,699 INFO org.apache.spark.scheduler.DAGScheduler: Job 1 failed: collect at Kafka2SparkStreaming2Kudu.scala:127, took 1.508028 s
2020-04-07 21:36:21,700 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586266580000 ms.0 from job set of time 1586266580000 ms
2020-04-07 21:36:21,702 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1586266580000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 3, localhost, executor driver): java.lang.ClassCastException: scala.collection.immutable.HashMap$HashTrieMap cannot be cast to java.lang.String
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:124)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:119)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:363)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1174)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:296)
	at scala.collection.AbstractIterator.to(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:275)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1174)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:245)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:944)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1.apply(Kafka2SparkStreaming2Kudu.scala:127)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1.apply(Kafka2SparkStreaming2Kudu.scala:117)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:191)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassCastException: scala.collection.immutable.HashMap$HashTrieMap cannot be cast to java.lang.String
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:124)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:119)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:363)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1174)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:296)
	at scala.collection.AbstractIterator.to(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:275)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1174)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	... 3 more
2020-04-07 21:36:21,716 INFO org.apache.spark.streaming.StreamingContext: Invoking stop(stopGracefully=false) from shutdown hook
2020-04-07 21:36:21,719 INFO org.apache.spark.streaming.scheduler.ReceiverTracker: ReceiverTracker stopped
2020-04-07 21:36:21,720 INFO org.apache.spark.executor.Executor: Executor interrupted and killed task 1.0 in stage 1.0 (TID 4), reason: Stage cancelled
2020-04-07 21:36:21,721 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopping JobGenerator immediately
2020-04-07 21:36:21,722 INFO org.apache.spark.streaming.util.RecurringTimer: Stopped timer for JobGenerator after time 1586266580000
2020-04-07 21:36:21,724 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 1.0 in stage 1.0 (TID 4, localhost, executor driver): TaskKilled (Stage cancelled)
2020-04-07 21:36:21,724 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
2020-04-07 21:36:21,900 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Member consumer-1-c669ab9b-1ef7-4000-a74a-1696fe6b0a04 sending LeaveGroup request to coordinator cw-instances-4.vpc.cloudera.com:9092 (id: 2147483594 rack: null)
2020-04-07 21:36:22,079 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopped JobGenerator
2020-04-07 21:36:22,083 INFO org.apache.spark.streaming.scheduler.JobScheduler: Stopped JobScheduler
2020-04-07 21:36:22,090 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@27b2faa6{/streaming,null,UNAVAILABLE,@Spark}
2020-04-07 21:36:22,090 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@c2e3264{/streaming/batch,null,UNAVAILABLE,@Spark}
2020-04-07 21:36:22,092 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@6014a9ba{/static/streaming,null,UNAVAILABLE,@Spark}
2020-04-07 21:36:22,093 INFO org.apache.spark.streaming.StreamingContext: StreamingContext stopped successfully
2020-04-07 21:36:22,093 INFO org.apache.spark.SparkContext: Invoking stop() from shutdown hook
2020-04-07 21:36:22,100 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@5dbe30be{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-07 21:36:22,102 INFO org.apache.spark.ui.SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
2020-04-07 21:36:22,111 INFO org.apache.spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2020-04-07 21:36:22,128 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2020-04-07 21:36:22,128 INFO org.apache.spark.storage.BlockManager: BlockManager stopped
2020-04-07 21:36:22,129 INFO org.apache.spark.storage.BlockManagerMaster: BlockManagerMaster stopped
2020-04-07 21:36:22,131 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2020-04-07 21:36:22,143 INFO org.apache.spark.SparkContext: Successfully stopped SparkContext
2020-04-07 21:36:22,144 INFO org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2020-04-07 21:36:22,144 INFO org.apache.spark.util.ShutdownHookManager: Deleting directory /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/spark-051162c2-7ed0-40fb-8e48-d2e7db3defe1
2020-04-07 21:51:43,806 WARN org.apache.spark.util.Utils: Your hostname, MacBook-Pro-CW.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)
2020-04-07 21:51:43,808 WARN org.apache.spark.util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
2020-04-07 21:51:43,857 INFO org.apache.spark.SparkContext: Running Spark version 2.4.0.cloudera2
2020-04-07 21:51:44,112 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-04-07 21:51:44,296 INFO org.apache.spark.SparkContext: Submitted application: Kafka2SparkStreaming2Kudu-kerberos
2020-04-07 21:51:44,357 INFO org.apache.spark.SecurityManager: Changing view acls to: godiswc
2020-04-07 21:51:44,357 INFO org.apache.spark.SecurityManager: Changing modify acls to: godiswc
2020-04-07 21:51:44,358 INFO org.apache.spark.SecurityManager: Changing view acls groups to: 
2020-04-07 21:51:44,358 INFO org.apache.spark.SecurityManager: Changing modify acls groups to: 
2020-04-07 21:51:44,359 INFO org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(godiswc); groups with view permissions: Set(); users  with modify permissions: Set(godiswc); groups with modify permissions: Set()
2020-04-07 21:51:44,587 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-07 21:51:44,735 INFO org.apache.spark.util.Utils: Successfully started service 'sparkDriver' on port 49664.
2020-04-07 21:51:44,758 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
2020-04-07 21:51:44,784 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
2020-04-07 21:51:44,787 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-04-07 21:51:44,788 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2020-04-07 21:51:44,815 INFO org.apache.spark.storage.DiskBlockManager: Created local directory at /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/blockmgr-5aef18f2-5022-4385-a968-d8bd88f2e240
2020-04-07 21:51:44,860 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 4.1 GB
2020-04-07 21:51:44,877 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
2020-04-07 21:51:44,964 INFO org.spark_project.jetty.util.log: Logging initialized @2136ms
2020-04-07 21:51:45,041 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2019-04-30T04:42:08+08:00, git hash: e1bc35120a6617ee3df052294e433f3a25ce7097
2020-04-07 21:51:45,061 INFO org.spark_project.jetty.server.Server: Started @2234ms
2020-04-07 21:51:45,063 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-07 21:51:45,083 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@72bd06ca{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-07 21:51:45,083 INFO org.apache.spark.util.Utils: Successfully started service 'SparkUI' on port 4040.
2020-04-07 21:51:45,126 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@41fe9859{/jobs,null,AVAILABLE,@Spark}
2020-04-07 21:51:45,127 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@79e18e38{/jobs/json,null,AVAILABLE,@Spark}
2020-04-07 21:51:45,129 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@29a60c27{/jobs/job,null,AVAILABLE,@Spark}
2020-04-07 21:51:45,133 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1ca25c47{/jobs/job/json,null,AVAILABLE,@Spark}
2020-04-07 21:51:45,141 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fcacc0{/stages,null,AVAILABLE,@Spark}
2020-04-07 21:51:45,142 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@533b266e{/stages/json,null,AVAILABLE,@Spark}
2020-04-07 21:51:45,143 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6d1d4d7{/stages/stage,null,AVAILABLE,@Spark}
2020-04-07 21:51:45,146 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62679465{/stages/stage/json,null,AVAILABLE,@Spark}
2020-04-07 21:51:45,148 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a988392{/stages/pool,null,AVAILABLE,@Spark}
2020-04-07 21:51:45,149 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1d71006f{/stages/pool/json,null,AVAILABLE,@Spark}
2020-04-07 21:51:45,150 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5b6813df{/storage,null,AVAILABLE,@Spark}
2020-04-07 21:51:45,151 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5f2606b{/storage/json,null,AVAILABLE,@Spark}
2020-04-07 21:51:45,151 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2b58f754{/storage/rdd,null,AVAILABLE,@Spark}
2020-04-07 21:51:45,152 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3ebff828{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-04-07 21:51:45,152 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2552f2cb{/environment,null,AVAILABLE,@Spark}
2020-04-07 21:51:45,153 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@33352f32{/environment/json,null,AVAILABLE,@Spark}
2020-04-07 21:51:45,154 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5f3b9c57{/executors,null,AVAILABLE,@Spark}
2020-04-07 21:51:45,155 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1e044120{/executors/json,null,AVAILABLE,@Spark}
2020-04-07 21:51:45,156 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2cf23c81{/executors/threadDump,null,AVAILABLE,@Spark}
2020-04-07 21:51:45,158 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3624da92{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-04-07 21:51:45,172 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@35fe2125{/static,null,AVAILABLE,@Spark}
2020-04-07 21:51:45,176 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@27a5328c{/,null,AVAILABLE,@Spark}
2020-04-07 21:51:45,178 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1e5f4170{/api,null,AVAILABLE,@Spark}
2020-04-07 21:51:45,180 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1568159{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-04-07 21:51:45,182 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4fcee388{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-04-07 21:51:45,189 INFO org.apache.spark.ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
2020-04-07 21:51:45,419 INFO org.apache.spark.executor.Executor: Starting executor ID driver on host localhost
2020-04-07 21:51:45,524 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-07 21:51:45,525 INFO org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49668.
2020-04-07 21:51:45,526 INFO org.apache.spark.network.netty.NettyBlockTransferService: Server created on 192.168.0.102:49668
2020-04-07 21:51:45,527 INFO org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-04-07 21:51:45,552 INFO org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 49668, None)
2020-04-07 21:51:45,554 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:49668 with 4.1 GB RAM, BlockManagerId(driver, 192.168.0.102, 49668, None)
2020-04-07 21:51:45,556 INFO org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 49668, None)
2020-04-07 21:51:45,556 INFO org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 49668, None)
2020-04-07 21:51:45,812 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@46baf579{/metrics/json,null,AVAILABLE,@Spark}
2020-04-07 21:51:46,205 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding enable.auto.commit to false for executor
2020-04-07 21:51:46,205 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding auto.offset.reset to none for executor
2020-04-07 21:51:46,206 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding executor group.id to spark-executor-testgroup
2020-04-07 21:51:46,206 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
2020-04-07 21:51:46,873 WARN org.apache.kudu.util.NetUtil: Slow DNS lookup! Resolved IP of `cw-instances-2.vpc.cloudera.com' to cw-instances-2.vpc.cloudera.com/10.65.1.47 in 246851009ns
2020-04-07 21:51:47,181 WARN org.apache.kudu.util.NetUtil: Slow DNS lookup! Resolved IP of `cw-instances-3.vpc.cloudera.com' to cw-instances-3.vpc.cloudera.com/10.65.9.108 in 186587290ns
2020-04-07 21:51:47,347 WARN org.apache.kudu.util.NetUtil: Slow DNS lookup! Resolved IP of `cw-instances-4.vpc.cloudera.com' to cw-instances-4.vpc.cloudera.com/10.65.12.223 in 161848379ns
2020-04-07 21:51:48,376 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Slide time = 10000 ms
2020-04-07 21:51:48,376 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Storage level = Serialized 1x Replicated
2020-04-07 21:51:48,377 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Checkpoint interval = null
2020-04-07 21:51:48,377 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Remember interval = 10000 ms
2020-04-07 21:51:48,378 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@4f9f3881
2020-04-07 21:51:48,378 INFO org.apache.spark.streaming.dstream.ForEachDStream: Slide time = 10000 ms
2020-04-07 21:51:48,378 INFO org.apache.spark.streaming.dstream.ForEachDStream: Storage level = Serialized 1x Replicated
2020-04-07 21:51:48,378 INFO org.apache.spark.streaming.dstream.ForEachDStream: Checkpoint interval = null
2020-04-07 21:51:48,378 INFO org.apache.spark.streaming.dstream.ForEachDStream: Remember interval = 10000 ms
2020-04-07 21:51:48,378 INFO org.apache.spark.streaming.dstream.ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@49c9acf3
2020-04-07 21:51:48,460 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = testgroup
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-07 21:51:48,563 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-07 21:51:48,563 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-07 21:51:48,563 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586267508562
2020-04-07 21:51:48,565 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-1, groupId=testgroup] Subscribed to topic(s): PABUSI2
2020-04-07 21:51:49,320 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-1, groupId=testgroup] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-07 21:51:49,321 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Discovered group coordinator cw-instances-4.vpc.cloudera.com:9092 (id: 2147483594 rack: null)
2020-04-07 21:51:49,325 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Revoking previously assigned partitions []
2020-04-07 21:51:49,325 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] (Re-)joining group
2020-04-07 21:51:49,851 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] (Re-)joining group
2020-04-07 21:51:53,300 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Successfully joined group with generation 23
2020-04-07 21:51:53,305 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Setting newly assigned partitions: PABUSI2-2, PABUSI2-0, PABUSI2-1
2020-04-07 21:51:53,497 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Setting offset for partition PABUSI2-2 to the committed offset FetchPosition{offset=122, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-2.vpc.cloudera.com:9092 (id: 52 rack: null), epoch=0}}
2020-04-07 21:51:53,499 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Setting offset for partition PABUSI2-0 to the committed offset FetchPosition{offset=120, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-4.vpc.cloudera.com:9092 (id: 53 rack: null), epoch=0}}
2020-04-07 21:51:53,499 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Setting offset for partition PABUSI2-1 to the committed offset FetchPosition{offset=122, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-3.vpc.cloudera.com:9092 (id: 54 rack: null), epoch=0}}
2020-04-07 21:51:53,510 INFO org.apache.spark.streaming.util.RecurringTimer: Started timer for JobGenerator at time 1586267510000
2020-04-07 21:51:53,511 INFO org.apache.spark.streaming.scheduler.JobGenerator: Started JobGenerator at 1586267510000 ms
2020-04-07 21:51:53,512 INFO org.apache.spark.streaming.scheduler.JobScheduler: Started JobScheduler
2020-04-07 21:51:53,517 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@61d84e08{/streaming,null,AVAILABLE,@Spark}
2020-04-07 21:51:53,519 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2d9f64c9{/streaming/json,null,AVAILABLE,@Spark}
2020-04-07 21:51:53,520 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@27b2faa6{/streaming/batch,null,AVAILABLE,@Spark}
2020-04-07 21:51:53,522 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6428591a{/streaming/batch/json,null,AVAILABLE,@Spark}
2020-04-07 21:51:53,523 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2a685eba{/static/streaming,null,AVAILABLE,@Spark}
2020-04-07 21:51:53,524 INFO org.apache.spark.streaming.StreamingContext: StreamingContext started
2020-04-07 21:51:53,566 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-0
2020-04-07 21:51:53,567 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-1
2020-04-07 21:51:53,567 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-2
2020-04-07 21:51:54,542 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-2 to offset 122.
2020-04-07 21:51:54,543 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-1 to offset 122.
2020-04-07 21:51:54,543 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-0 to offset 120.
2020-04-07 21:51:54,568 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586267510000 ms
2020-04-07 21:51:54,571 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586267510000 ms.0 from job set of time 1586267510000 ms
2020-04-07 21:51:54,612 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:128
2020-04-07 21:51:54,625 INFO org.apache.spark.scheduler.DAGScheduler: Got job 0 (collect at Kafka2SparkStreaming2Kudu.scala:128) with 3 output partitions
2020-04-07 21:51:54,625 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 0 (collect at Kafka2SparkStreaming2Kudu.scala:128)
2020-04-07 21:51:54,625 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-07 21:51:54,626 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-07 21:51:54,630 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at Kafka2SparkStreaming2Kudu.scala:120), which has no missing parents
2020-04-07 21:51:54,670 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-07 21:51:54,923 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2042.0 B, free 4.1 GB)
2020-04-07 21:51:54,924 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:49668 (size: 2042.0 B, free: 4.1 GB)
2020-04-07 21:51:54,926 INFO org.apache.spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2020-04-07 21:51:54,938 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at Kafka2SparkStreaming2Kudu.scala:120) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-07 21:51:54,938 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 0.0 with 3 tasks
2020-04-07 21:51:54,970 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-07 21:51:54,971 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-07 21:51:54,972 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-07 21:51:54,980 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
2020-04-07 21:51:54,980 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 0.0 (TID 2)
2020-04-07 21:51:54,981 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 0.0 (TID 1)
2020-04-07 21:51:55,002 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 122 is the same as ending offset skipping PABUSI2 2
2020-04-07 21:51:55,002 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 122 is the same as ending offset skipping PABUSI2 1
2020-04-07 21:51:55,002 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 120 is the same as ending offset skipping PABUSI2 0
2020-04-07 21:51:55,012 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 751 bytes result sent to driver
2020-04-07 21:51:55,012 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 0.0 (TID 1). 751 bytes result sent to driver
2020-04-07 21:51:55,012 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 0.0 (TID 2). 751 bytes result sent to driver
2020-04-07 21:51:55,016 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 44 ms on localhost (executor driver) (1/3)
2020-04-07 21:51:55,018 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 46 ms on localhost (executor driver) (2/3)
2020-04-07 21:51:55,018 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 57 ms on localhost (executor driver) (3/3)
2020-04-07 21:51:55,018 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-04-07 21:51:55,022 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 0 (collect at Kafka2SparkStreaming2Kudu.scala:128) finished in 0.377 s
2020-04-07 21:51:55,026 INFO org.apache.spark.scheduler.DAGScheduler: Job 0 finished: collect at Kafka2SparkStreaming2Kudu.scala:128, took 0.413642 s
2020-04-07 21:51:55,031 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586267510000 ms.0 from job set of time 1586267510000 ms
2020-04-07 21:51:55,032 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 5.030 s for time 1586267510000 ms (execution: 0.460 s)
2020-04-07 21:51:55,036 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-07 21:51:55,038 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 
2020-04-07 21:52:00,010 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-0
2020-04-07 21:52:00,010 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-1
2020-04-07 21:52:00,010 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-2
2020-04-07 21:52:00,183 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-1 to offset 128.
2020-04-07 21:52:00,185 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-2 to offset 129.
2020-04-07 21:52:00,186 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-0 to offset 127.
2020-04-07 21:52:00,187 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586267520000 ms
2020-04-07 21:52:00,187 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586267520000 ms.0 from job set of time 1586267520000 ms
2020-04-07 21:52:00,195 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:128
2020-04-07 21:52:00,196 INFO org.apache.spark.scheduler.DAGScheduler: Got job 1 (collect at Kafka2SparkStreaming2Kudu.scala:128) with 3 output partitions
2020-04-07 21:52:00,196 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 1 (collect at Kafka2SparkStreaming2Kudu.scala:128)
2020-04-07 21:52:00,197 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-07 21:52:00,197 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-07 21:52:00,198 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at map at Kafka2SparkStreaming2Kudu.scala:120), which has no missing parents
2020-04-07 21:52:00,203 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-07 21:52:00,206 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-07 21:52:00,207 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.102:49668 (size: 2.0 KB, free: 4.1 GB)
2020-04-07 21:52:00,208 INFO org.apache.spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2020-04-07 21:52:00,209 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at map at Kafka2SparkStreaming2Kudu.scala:120) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-07 21:52:00,209 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 1.0 with 3 tasks
2020-04-07 21:52:00,210 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-07 21:52:00,211 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 4, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-07 21:52:00,212 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 1.0 (TID 5, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-07 21:52:00,230 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 1.0 (TID 3)
2020-04-07 21:52:00,230 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 1.0 (TID 5)
2020-04-07 21:52:00,230 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 1.0 (TID 4)
2020-04-07 21:52:00,234 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 16
2020-04-07 21:52:00,234 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 8
2020-04-07 21:52:00,234 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 9
2020-04-07 21:52:00,234 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 13
2020-04-07 21:52:00,234 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Computing topic PABUSI2, partition 2 offsets 122 -> 129
2020-04-07 21:52:00,235 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 19
2020-04-07 21:52:00,234 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Computing topic PABUSI2, partition 1 offsets 122 -> 128
2020-04-07 21:52:00,234 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Computing topic PABUSI2, partition 0 offsets 120 -> 127
2020-04-07 21:52:00,235 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 24
2020-04-07 21:52:00,235 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 18
2020-04-07 21:52:00,239 INFO org.apache.spark.streaming.kafka010.KafkaDataConsumer: Initializing cache 16 64 0.75
2020-04-07 21:52:00,247 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-testgroup
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-07 21:52:00,252 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.102:49668 in memory (size: 2042.0 B, free: 4.1 GB)
2020-04-07 21:52:00,253 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-07 21:52:00,254 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-07 21:52:00,254 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586267520253
2020-04-07 21:52:00,255 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup] Subscribed to partition(s): PABUSI2-2
2020-04-07 21:52:00,256 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 14
2020-04-07 21:52:00,256 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 7
2020-04-07 21:52:00,256 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 17
2020-04-07 21:52:00,256 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 20
2020-04-07 21:52:00,257 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 25
2020-04-07 21:52:00,257 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 3
2020-04-07 21:52:00,257 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 12
2020-04-07 21:52:00,257 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-testgroup
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-07 21:52:00,257 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 5
2020-04-07 21:52:00,257 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 10
2020-04-07 21:52:00,257 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 4
2020-04-07 21:52:00,257 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 21
2020-04-07 21:52:00,257 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 23
2020-04-07 21:52:00,257 INFO org.apache.spark.streaming.kafka010.InternalKafkaConsumer: Initial fetch for spark-executor-testgroup PABUSI2-2 122
2020-04-07 21:52:00,257 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 6
2020-04-07 21:52:00,258 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 11
2020-04-07 21:52:00,258 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 15
2020-04-07 21:52:00,258 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 2
2020-04-07 21:52:00,258 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup] Seeking to offset 122 for partition PABUSI2-2
2020-04-07 21:52:00,258 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 22
2020-04-07 21:52:00,258 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 1
2020-04-07 21:52:00,262 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-07 21:52:00,262 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-07 21:52:00,262 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586267520262
2020-04-07 21:52:00,263 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-3, groupId=spark-executor-testgroup] Subscribed to partition(s): PABUSI2-0
2020-04-07 21:52:00,263 INFO org.apache.spark.streaming.kafka010.InternalKafkaConsumer: Initial fetch for spark-executor-testgroup PABUSI2-0 120
2020-04-07 21:52:00,263 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-3, groupId=spark-executor-testgroup] Seeking to offset 120 for partition PABUSI2-0
2020-04-07 21:52:00,263 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-testgroup
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-07 21:52:00,268 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-07 21:52:00,268 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-07 21:52:00,268 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586267520268
2020-04-07 21:52:00,268 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-4, groupId=spark-executor-testgroup] Subscribed to partition(s): PABUSI2-1
2020-04-07 21:52:00,268 INFO org.apache.spark.streaming.kafka010.InternalKafkaConsumer: Initial fetch for spark-executor-testgroup PABUSI2-1 122
2020-04-07 21:52:00,268 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-4, groupId=spark-executor-testgroup] Seeking to offset 122 for partition PABUSI2-1
2020-04-07 21:52:00,782 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-07 21:52:00,782 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-3, groupId=spark-executor-testgroup] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-07 21:52:00,790 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-4, groupId=spark-executor-testgroup] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-07 21:52:01,965 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 1.0 (TID 4). 917 bytes result sent to driver
2020-04-07 21:52:01,967 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 4) in 1756 ms on localhost (executor driver) (1/3)
2020-04-07 21:52:01,990 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 1.0 (TID 5). 898 bytes result sent to driver
2020-04-07 21:52:01,992 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 1.0 (TID 5) in 1781 ms on localhost (executor driver) (2/3)
2020-04-07 21:52:02,052 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 1.0 (TID 3). 913 bytes result sent to driver
2020-04-07 21:52:02,055 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 3) in 1845 ms on localhost (executor driver) (3/3)
2020-04-07 21:52:02,056 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
2020-04-07 21:52:02,057 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 1 (collect at Kafka2SparkStreaming2Kudu.scala:128) finished in 1.855 s
2020-04-07 21:52:02,057 INFO org.apache.spark.scheduler.DAGScheduler: Job 1 finished: collect at Kafka2SparkStreaming2Kudu.scala:128, took 1.861867 s
2020-04-07 21:52:02,062 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586267520000 ms.0 from job set of time 1586267520000 ms
2020-04-07 21:52:02,062 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 2.062 s for time 1586267520000 ms (execution: 1.875 s)
2020-04-07 21:52:02,063 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 0 from persistence list
2020-04-07 21:52:02,066 INFO org.apache.spark.storage.BlockManager: Removing RDD 0
2020-04-07 21:52:02,067 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-07 21:52:02,068 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 
2020-04-07 21:52:10,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-0
2020-04-07 21:52:10,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-2
2020-04-07 21:52:10,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-1
2020-04-07 21:52:10,177 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-1 to offset 128.
2020-04-07 21:52:10,179 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-2 to offset 129.
2020-04-07 21:52:10,179 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-0 to offset 127.
2020-04-07 21:52:10,180 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586267530000 ms
2020-04-07 21:52:10,180 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586267530000 ms.0 from job set of time 1586267530000 ms
2020-04-07 21:52:10,186 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:128
2020-04-07 21:52:10,186 INFO org.apache.spark.scheduler.DAGScheduler: Got job 2 (collect at Kafka2SparkStreaming2Kudu.scala:128) with 3 output partitions
2020-04-07 21:52:10,186 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 2 (collect at Kafka2SparkStreaming2Kudu.scala:128)
2020-04-07 21:52:10,186 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-07 21:52:10,187 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-07 21:52:10,188 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[7] at map at Kafka2SparkStreaming2Kudu.scala:120), which has no missing parents
2020-04-07 21:52:10,191 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-07 21:52:10,192 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-07 21:52:10,193 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.102:49668 (size: 2.0 KB, free: 4.1 GB)
2020-04-07 21:52:10,194 INFO org.apache.spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2020-04-07 21:52:10,194 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at map at Kafka2SparkStreaming2Kudu.scala:120) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-07 21:52:10,194 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 2.0 with 3 tasks
2020-04-07 21:52:10,195 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-07 21:52:10,195 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 2.0 (TID 7, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-07 21:52:10,196 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 2.0 (TID 8, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-07 21:52:10,196 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 2.0 (TID 6)
2020-04-07 21:52:10,196 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 2.0 (TID 8)
2020-04-07 21:52:10,196 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 2.0 (TID 7)
2020-04-07 21:52:10,198 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 129 is the same as ending offset skipping PABUSI2 2
2020-04-07 21:52:10,198 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 127 is the same as ending offset skipping PABUSI2 0
2020-04-07 21:52:10,198 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 128 is the same as ending offset skipping PABUSI2 1
2020-04-07 21:52:10,199 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 2.0 (TID 7). 708 bytes result sent to driver
2020-04-07 21:52:10,200 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 2.0 (TID 8). 708 bytes result sent to driver
2020-04-07 21:52:10,200 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 2.0 (TID 6). 708 bytes result sent to driver
2020-04-07 21:52:10,200 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 2.0 (TID 7) in 5 ms on localhost (executor driver) (1/3)
2020-04-07 21:52:10,201 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 2.0 (TID 8) in 5 ms on localhost (executor driver) (2/3)
2020-04-07 21:52:10,201 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 6) in 6 ms on localhost (executor driver) (3/3)
2020-04-07 21:52:10,201 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
2020-04-07 21:52:10,201 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 2 (collect at Kafka2SparkStreaming2Kudu.scala:128) finished in 0.011 s
2020-04-07 21:52:10,202 INFO org.apache.spark.scheduler.DAGScheduler: Job 2 finished: collect at Kafka2SparkStreaming2Kudu.scala:128, took 0.016076 s
2020-04-07 21:52:10,204 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586267530000 ms.0 from job set of time 1586267530000 ms
2020-04-07 21:52:10,204 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 3 from persistence list
2020-04-07 21:52:10,204 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.204 s for time 1586267530000 ms (execution: 0.024 s)
2020-04-07 21:52:10,205 INFO org.apache.spark.storage.BlockManager: Removing RDD 3
2020-04-07 21:52:10,205 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-07 21:52:10,205 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586267510000 ms
2020-04-07 21:52:11,148 INFO org.apache.spark.streaming.StreamingContext: Invoking stop(stopGracefully=false) from shutdown hook
2020-04-07 21:52:11,151 INFO org.apache.spark.streaming.scheduler.ReceiverTracker: ReceiverTracker stopped
2020-04-07 21:52:11,152 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopping JobGenerator immediately
2020-04-07 21:52:11,153 INFO org.apache.spark.streaming.util.RecurringTimer: Stopped timer for JobGenerator after time 1586267530000
2020-04-07 21:52:11,330 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Member consumer-1-fd2322d7-7a2d-47c8-bdb8-6e37d6007213 sending LeaveGroup request to coordinator cw-instances-4.vpc.cloudera.com:9092 (id: 2147483594 rack: null)
2020-04-07 21:52:11,511 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopped JobGenerator
2020-04-07 21:52:11,514 INFO org.apache.spark.streaming.scheduler.JobScheduler: Stopped JobScheduler
2020-04-07 21:52:11,517 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@61d84e08{/streaming,null,UNAVAILABLE,@Spark}
2020-04-07 21:52:11,518 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@27b2faa6{/streaming/batch,null,UNAVAILABLE,@Spark}
2020-04-07 21:52:11,519 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@2a685eba{/static/streaming,null,UNAVAILABLE,@Spark}
2020-04-07 21:52:11,520 INFO org.apache.spark.streaming.StreamingContext: StreamingContext stopped successfully
2020-04-07 21:52:11,520 INFO org.apache.spark.SparkContext: Invoking stop() from shutdown hook
2020-04-07 21:52:11,525 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@72bd06ca{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-07 21:52:11,526 INFO org.apache.spark.ui.SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
2020-04-07 21:52:11,533 INFO org.apache.spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2020-04-07 21:52:11,545 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2020-04-07 21:52:11,545 INFO org.apache.spark.storage.BlockManager: BlockManager stopped
2020-04-07 21:52:11,546 INFO org.apache.spark.storage.BlockManagerMaster: BlockManagerMaster stopped
2020-04-07 21:52:11,548 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2020-04-07 21:52:11,557 INFO org.apache.spark.SparkContext: Successfully stopped SparkContext
2020-04-07 21:52:11,558 INFO org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2020-04-07 21:52:11,558 INFO org.apache.spark.util.ShutdownHookManager: Deleting directory /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/spark-776c3161-eb23-441a-b122-53dfb63f499e
2020-04-07 21:53:19,367 WARN org.apache.spark.util.Utils: Your hostname, MacBook-Pro-CW.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)
2020-04-07 21:53:19,368 WARN org.apache.spark.util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
2020-04-07 21:53:19,407 INFO org.apache.spark.SparkContext: Running Spark version 2.4.0.cloudera2
2020-04-07 21:53:19,605 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-04-07 21:53:19,711 INFO org.apache.spark.SparkContext: Submitted application: Kafka2SparkStreaming2Kudu-kerberos
2020-04-07 21:53:19,749 INFO org.apache.spark.SecurityManager: Changing view acls to: godiswc
2020-04-07 21:53:19,750 INFO org.apache.spark.SecurityManager: Changing modify acls to: godiswc
2020-04-07 21:53:19,750 INFO org.apache.spark.SecurityManager: Changing view acls groups to: 
2020-04-07 21:53:19,750 INFO org.apache.spark.SecurityManager: Changing modify acls groups to: 
2020-04-07 21:53:19,751 INFO org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(godiswc); groups with view permissions: Set(); users  with modify permissions: Set(godiswc); groups with modify permissions: Set()
2020-04-07 21:53:19,872 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-07 21:53:19,952 INFO org.apache.spark.util.Utils: Successfully started service 'sparkDriver' on port 50025.
2020-04-07 21:53:19,970 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
2020-04-07 21:53:19,983 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
2020-04-07 21:53:19,985 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-04-07 21:53:19,985 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2020-04-07 21:53:19,997 INFO org.apache.spark.storage.DiskBlockManager: Created local directory at /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/blockmgr-979013aa-51d6-43ba-8617-1fab7053b0c5
2020-04-07 21:53:20,032 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 4.1 GB
2020-04-07 21:53:20,041 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
2020-04-07 21:53:20,087 INFO org.spark_project.jetty.util.log: Logging initialized @1297ms
2020-04-07 21:53:20,123 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2019-04-30T04:42:08+08:00, git hash: e1bc35120a6617ee3df052294e433f3a25ce7097
2020-04-07 21:53:20,134 INFO org.spark_project.jetty.server.Server: Started @1344ms
2020-04-07 21:53:20,135 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-07 21:53:20,144 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@72bd06ca{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-07 21:53:20,145 INFO org.apache.spark.util.Utils: Successfully started service 'SparkUI' on port 4040.
2020-04-07 21:53:20,162 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@41fe9859{/jobs,null,AVAILABLE,@Spark}
2020-04-07 21:53:20,163 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@79e18e38{/jobs/json,null,AVAILABLE,@Spark}
2020-04-07 21:53:20,163 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@29a60c27{/jobs/job,null,AVAILABLE,@Spark}
2020-04-07 21:53:20,166 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1ca25c47{/jobs/job/json,null,AVAILABLE,@Spark}
2020-04-07 21:53:20,170 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fcacc0{/stages,null,AVAILABLE,@Spark}
2020-04-07 21:53:20,171 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@533b266e{/stages/json,null,AVAILABLE,@Spark}
2020-04-07 21:53:20,171 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6d1d4d7{/stages/stage,null,AVAILABLE,@Spark}
2020-04-07 21:53:20,174 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62679465{/stages/stage/json,null,AVAILABLE,@Spark}
2020-04-07 21:53:20,175 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a988392{/stages/pool,null,AVAILABLE,@Spark}
2020-04-07 21:53:20,176 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1d71006f{/stages/pool/json,null,AVAILABLE,@Spark}
2020-04-07 21:53:20,177 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5b6813df{/storage,null,AVAILABLE,@Spark}
2020-04-07 21:53:20,177 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5f2606b{/storage/json,null,AVAILABLE,@Spark}
2020-04-07 21:53:20,178 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2b58f754{/storage/rdd,null,AVAILABLE,@Spark}
2020-04-07 21:53:20,179 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3ebff828{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-04-07 21:53:20,180 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2552f2cb{/environment,null,AVAILABLE,@Spark}
2020-04-07 21:53:20,180 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@33352f32{/environment/json,null,AVAILABLE,@Spark}
2020-04-07 21:53:20,181 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5f3b9c57{/executors,null,AVAILABLE,@Spark}
2020-04-07 21:53:20,182 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1e044120{/executors/json,null,AVAILABLE,@Spark}
2020-04-07 21:53:20,182 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2cf23c81{/executors/threadDump,null,AVAILABLE,@Spark}
2020-04-07 21:53:20,183 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3624da92{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-04-07 21:53:20,190 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@35fe2125{/static,null,AVAILABLE,@Spark}
2020-04-07 21:53:20,191 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@27a5328c{/,null,AVAILABLE,@Spark}
2020-04-07 21:53:20,192 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1e5f4170{/api,null,AVAILABLE,@Spark}
2020-04-07 21:53:20,192 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1568159{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-04-07 21:53:20,193 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4fcee388{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-04-07 21:53:20,195 INFO org.apache.spark.ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
2020-04-07 21:53:20,328 INFO org.apache.spark.executor.Executor: Starting executor ID driver on host localhost
2020-04-07 21:53:20,396 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-07 21:53:20,398 INFO org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50029.
2020-04-07 21:53:20,398 INFO org.apache.spark.network.netty.NettyBlockTransferService: Server created on 192.168.0.102:50029
2020-04-07 21:53:20,400 INFO org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-04-07 21:53:20,435 INFO org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 50029, None)
2020-04-07 21:53:20,442 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:50029 with 4.1 GB RAM, BlockManagerId(driver, 192.168.0.102, 50029, None)
2020-04-07 21:53:20,445 INFO org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 50029, None)
2020-04-07 21:53:20,446 INFO org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 50029, None)
2020-04-07 21:53:20,713 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@46baf579{/metrics/json,null,AVAILABLE,@Spark}
2020-04-07 21:53:20,984 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding enable.auto.commit to false for executor
2020-04-07 21:53:20,985 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding auto.offset.reset to none for executor
2020-04-07 21:53:20,985 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding executor group.id to spark-executor-testgroup
2020-04-07 21:53:20,986 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
2020-04-07 21:53:22,737 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Slide time = 10000 ms
2020-04-07 21:53:22,737 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Storage level = Serialized 1x Replicated
2020-04-07 21:53:22,737 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Checkpoint interval = null
2020-04-07 21:53:22,738 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Remember interval = 10000 ms
2020-04-07 21:53:22,738 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@1eca36f7
2020-04-07 21:53:22,738 INFO org.apache.spark.streaming.dstream.ForEachDStream: Slide time = 10000 ms
2020-04-07 21:53:22,738 INFO org.apache.spark.streaming.dstream.ForEachDStream: Storage level = Serialized 1x Replicated
2020-04-07 21:53:22,738 INFO org.apache.spark.streaming.dstream.ForEachDStream: Checkpoint interval = null
2020-04-07 21:53:22,738 INFO org.apache.spark.streaming.dstream.ForEachDStream: Remember interval = 10000 ms
2020-04-07 21:53:22,739 INFO org.apache.spark.streaming.dstream.ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@5dba8699
2020-04-07 21:53:22,783 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = testgroup
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-07 21:53:22,843 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-07 21:53:22,843 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-07 21:53:22,843 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586267602842
2020-04-07 21:53:22,845 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-1, groupId=testgroup] Subscribed to topic(s): PABUSI2
2020-04-07 21:53:23,527 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-1, groupId=testgroup] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-07 21:53:23,528 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Discovered group coordinator cw-instances-4.vpc.cloudera.com:9092 (id: 2147483594 rack: null)
2020-04-07 21:53:23,532 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Revoking previously assigned partitions []
2020-04-07 21:53:23,532 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] (Re-)joining group
2020-04-07 21:53:24,050 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] (Re-)joining group
2020-04-07 21:53:27,399 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Successfully joined group with generation 25
2020-04-07 21:53:27,403 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Setting newly assigned partitions: PABUSI2-2, PABUSI2-0, PABUSI2-1
2020-04-07 21:53:27,590 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Setting offset for partition PABUSI2-2 to the committed offset FetchPosition{offset=129, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-2.vpc.cloudera.com:9092 (id: 52 rack: null), epoch=0}}
2020-04-07 21:53:27,591 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Setting offset for partition PABUSI2-0 to the committed offset FetchPosition{offset=127, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-4.vpc.cloudera.com:9092 (id: 53 rack: null), epoch=0}}
2020-04-07 21:53:27,591 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Setting offset for partition PABUSI2-1 to the committed offset FetchPosition{offset=128, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-3.vpc.cloudera.com:9092 (id: 54 rack: null), epoch=0}}
2020-04-07 21:53:27,600 INFO org.apache.spark.streaming.util.RecurringTimer: Started timer for JobGenerator at time 1586267610000
2020-04-07 21:53:27,600 INFO org.apache.spark.streaming.scheduler.JobGenerator: Started JobGenerator at 1586267610000 ms
2020-04-07 21:53:27,601 INFO org.apache.spark.streaming.scheduler.JobScheduler: Started JobScheduler
2020-04-07 21:53:27,606 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@61d84e08{/streaming,null,AVAILABLE,@Spark}
2020-04-07 21:53:27,608 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2d9f64c9{/streaming/json,null,AVAILABLE,@Spark}
2020-04-07 21:53:27,610 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@27b2faa6{/streaming/batch,null,AVAILABLE,@Spark}
2020-04-07 21:53:27,611 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6428591a{/streaming/batch/json,null,AVAILABLE,@Spark}
2020-04-07 21:53:27,612 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2a685eba{/static/streaming,null,AVAILABLE,@Spark}
2020-04-07 21:53:27,612 INFO org.apache.spark.streaming.StreamingContext: StreamingContext started
2020-04-07 21:53:30,048 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-0
2020-04-07 21:53:30,049 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-2
2020-04-07 21:53:30,049 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Seeking to LATEST offset of partition PABUSI2-1
2020-04-07 21:53:30,228 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-1 to offset 134.
2020-04-07 21:53:30,228 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-2 to offset 136.
2020-04-07 21:53:30,228 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup] Resetting offset for partition PABUSI2-0 to offset 134.
2020-04-07 21:53:30,253 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586267610000 ms
2020-04-07 21:53:30,256 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586267610000 ms.0 from job set of time 1586267610000 ms
2020-04-07 21:53:30,315 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:128
2020-04-07 21:53:30,332 INFO org.apache.spark.scheduler.DAGScheduler: Got job 0 (collect at Kafka2SparkStreaming2Kudu.scala:128) with 3 output partitions
2020-04-07 21:53:30,333 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 0 (collect at Kafka2SparkStreaming2Kudu.scala:128)
2020-04-07 21:53:30,333 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-07 21:53:30,334 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-07 21:53:30,341 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at Kafka2SparkStreaming2Kudu.scala:120), which has no missing parents
2020-04-07 21:53:30,395 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-07 21:53:30,655 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-07 21:53:30,656 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:50029 (size: 2.0 KB, free: 4.1 GB)
2020-04-07 21:53:30,658 INFO org.apache.spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2020-04-07 21:53:30,670 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at Kafka2SparkStreaming2Kudu.scala:120) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-07 21:53:30,671 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 0.0 with 3 tasks
2020-04-07 21:53:30,701 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-07 21:53:30,702 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-07 21:53:30,703 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-07 21:53:30,710 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 0.0 (TID 1)
2020-04-07 21:53:30,711 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
2020-04-07 21:53:30,711 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 0.0 (TID 2)
2020-04-07 21:53:30,734 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Computing topic PABUSI2, partition 0 offsets 127 -> 134
2020-04-07 21:53:30,734 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Computing topic PABUSI2, partition 1 offsets 128 -> 134
2020-04-07 21:53:30,734 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Computing topic PABUSI2, partition 2 offsets 129 -> 136
2020-04-07 21:53:30,737 INFO org.apache.spark.streaming.kafka010.KafkaDataConsumer: Initializing cache 16 64 0.75
2020-04-07 21:53:30,739 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-testgroup
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-07 21:53:30,743 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-07 21:53:30,743 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-07 21:53:30,743 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586267610743
2020-04-07 21:53:30,743 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup] Subscribed to partition(s): PABUSI2-0
2020-04-07 21:53:30,745 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-testgroup
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-07 21:53:30,745 INFO org.apache.spark.streaming.kafka010.InternalKafkaConsumer: Initial fetch for spark-executor-testgroup PABUSI2-0 127
2020-04-07 21:53:30,746 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup] Seeking to offset 127 for partition PABUSI2-0
2020-04-07 21:53:30,750 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-07 21:53:30,750 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-07 21:53:30,750 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586267610749
2020-04-07 21:53:30,750 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-3, groupId=spark-executor-testgroup] Subscribed to partition(s): PABUSI2-1
2020-04-07 21:53:30,750 INFO org.apache.spark.streaming.kafka010.InternalKafkaConsumer: Initial fetch for spark-executor-testgroup PABUSI2-1 128
2020-04-07 21:53:30,751 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-3, groupId=spark-executor-testgroup] Seeking to offset 128 for partition PABUSI2-1
2020-04-07 21:53:30,751 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-testgroup
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-07 21:53:30,755 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-07 21:53:30,755 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-07 21:53:30,755 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586267610754
2020-04-07 21:53:30,755 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-4, groupId=spark-executor-testgroup] Subscribed to partition(s): PABUSI2-2
2020-04-07 21:53:30,755 INFO org.apache.spark.streaming.kafka010.InternalKafkaConsumer: Initial fetch for spark-executor-testgroup PABUSI2-2 129
2020-04-07 21:53:30,755 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-4, groupId=spark-executor-testgroup] Seeking to offset 129 for partition PABUSI2-2
2020-04-07 21:53:31,268 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-07 21:53:31,277 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-4, groupId=spark-executor-testgroup] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-07 21:53:31,277 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-3, groupId=spark-executor-testgroup] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-07 21:53:32,470 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 0.0 (TID 1). 16239 bytes result sent to driver
2020-04-07 21:53:32,470 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 12923 bytes result sent to driver
2020-04-07 21:53:32,479 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1776 ms on localhost (executor driver) (1/3)
2020-04-07 21:53:32,480 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1788 ms on localhost (executor driver) (2/3)
2020-04-07 21:53:32,490 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 0.0 (TID 2). 17915 bytes result sent to driver
2020-04-07 21:53:32,493 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 1791 ms on localhost (executor driver) (3/3)
2020-04-07 21:53:32,494 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-04-07 21:53:32,495 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 0 (collect at Kafka2SparkStreaming2Kudu.scala:128) finished in 2.134 s
2020-04-07 21:53:32,499 INFO org.apache.spark.scheduler.DAGScheduler: Job 0 finished: collect at Kafka2SparkStreaming2Kudu.scala:128, took 2.183785 s
2020-04-07 21:53:32,507 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586267610000 ms.0 from job set of time 1586267610000 ms
2020-04-07 21:53:32,507 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 2.506 s for time 1586267610000 ms (execution: 2.251 s)
2020-04-07 21:53:32,511 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-07 21:53:32,514 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 
2020-04-07 21:53:36,076 INFO org.apache.spark.streaming.StreamingContext: Invoking stop(stopGracefully=false) from shutdown hook
2020-04-07 21:53:36,079 INFO org.apache.spark.streaming.scheduler.ReceiverTracker: ReceiverTracker stopped
2020-04-07 21:53:36,081 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopping JobGenerator immediately
2020-04-07 21:53:36,082 INFO org.apache.spark.streaming.util.RecurringTimer: Stopped timer for JobGenerator after time 1586267610000
2020-04-07 21:53:36,261 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup] Member consumer-1-aad491e4-4a3d-4e33-b47e-3a9d0777f716 sending LeaveGroup request to coordinator cw-instances-4.vpc.cloudera.com:9092 (id: 2147483594 rack: null)
2020-04-07 21:53:36,441 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopped JobGenerator
2020-04-07 21:53:36,445 INFO org.apache.spark.streaming.scheduler.JobScheduler: Stopped JobScheduler
2020-04-07 21:53:36,450 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@61d84e08{/streaming,null,UNAVAILABLE,@Spark}
2020-04-07 21:53:36,451 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@27b2faa6{/streaming/batch,null,UNAVAILABLE,@Spark}
2020-04-07 21:53:36,453 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@2a685eba{/static/streaming,null,UNAVAILABLE,@Spark}
2020-04-07 21:53:36,454 INFO org.apache.spark.streaming.StreamingContext: StreamingContext stopped successfully
2020-04-07 21:53:36,455 INFO org.apache.spark.SparkContext: Invoking stop() from shutdown hook
2020-04-07 21:53:36,461 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@72bd06ca{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-07 21:53:36,463 INFO org.apache.spark.ui.SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
2020-04-07 21:53:36,470 INFO org.apache.spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2020-04-07 21:53:36,483 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2020-04-07 21:53:36,483 INFO org.apache.spark.storage.BlockManager: BlockManager stopped
2020-04-07 21:53:36,484 INFO org.apache.spark.storage.BlockManagerMaster: BlockManagerMaster stopped
2020-04-07 21:53:36,485 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2020-04-07 21:53:36,493 INFO org.apache.spark.SparkContext: Successfully stopped SparkContext
2020-04-07 21:53:36,493 INFO org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2020-04-07 21:53:36,494 INFO org.apache.spark.util.ShutdownHookManager: Deleting directory /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/spark-e651c721-421a-4de1-b9f5-4a5734150525
2020-04-08 11:15:42,361 WARN org.apache.spark.util.Utils: Your hostname, MacBook-Pro-CW.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)
2020-04-08 11:15:42,362 WARN org.apache.spark.util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
2020-04-08 11:15:42,620 INFO org.apache.spark.SparkContext: Running Spark version 2.4.0.cloudera2
2020-04-08 11:15:43,041 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-04-08 11:15:43,224 INFO org.apache.spark.SparkContext: Submitted application: Kafka2SparkStreaming2Kudu-kerberos
2020-04-08 11:15:53,015 WARN org.apache.spark.util.Utils: Your hostname, MacBook-Pro-CW.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)
2020-04-08 11:15:53,016 WARN org.apache.spark.util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
2020-04-08 11:15:53,052 INFO org.apache.spark.SparkContext: Running Spark version 2.4.0.cloudera2
2020-04-08 11:15:53,262 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-04-08 11:15:53,376 INFO org.apache.spark.SparkContext: Submitted application: Kafka2SparkStreaming2Kudu-kerberos
2020-04-08 11:15:53,430 INFO org.apache.spark.SecurityManager: Changing view acls to: godiswc
2020-04-08 11:15:53,431 INFO org.apache.spark.SecurityManager: Changing modify acls to: godiswc
2020-04-08 11:15:53,432 INFO org.apache.spark.SecurityManager: Changing view acls groups to: 
2020-04-08 11:15:53,432 INFO org.apache.spark.SecurityManager: Changing modify acls groups to: 
2020-04-08 11:15:53,433 INFO org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(godiswc); groups with view permissions: Set(); users  with modify permissions: Set(godiswc); groups with modify permissions: Set()
2020-04-08 11:15:53,644 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 11:15:53,739 INFO org.apache.spark.util.Utils: Successfully started service 'sparkDriver' on port 56111.
2020-04-08 11:15:53,763 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
2020-04-08 11:15:53,780 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
2020-04-08 11:15:53,783 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-04-08 11:15:53,784 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2020-04-08 11:15:53,819 INFO org.apache.spark.storage.DiskBlockManager: Created local directory at /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/blockmgr-2de4f26e-321c-4b8a-8837-a04f7b9d54a5
2020-04-08 11:15:53,839 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 4.1 GB
2020-04-08 11:15:53,853 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
2020-04-08 11:15:53,962 INFO org.spark_project.jetty.util.log: Logging initialized @1595ms
2020-04-08 11:15:54,028 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2019-04-30T04:42:08+08:00, git hash: e1bc35120a6617ee3df052294e433f3a25ce7097
2020-04-08 11:15:54,043 INFO org.spark_project.jetty.server.Server: Started @1676ms
2020-04-08 11:15:54,045 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 11:15:54,058 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@29a60c27{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 11:15:54,059 INFO org.apache.spark.util.Utils: Successfully started service 'SparkUI' on port 4040.
2020-04-08 11:15:54,079 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6f80fafe{/jobs,null,AVAILABLE,@Spark}
2020-04-08 11:15:54,080 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@32cb636e{/jobs/json,null,AVAILABLE,@Spark}
2020-04-08 11:15:54,081 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@63cd604c{/jobs/job,null,AVAILABLE,@Spark}
2020-04-08 11:15:54,083 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a1d204a{/jobs/job/json,null,AVAILABLE,@Spark}
2020-04-08 11:15:54,085 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62dae245{/stages,null,AVAILABLE,@Spark}
2020-04-08 11:15:54,086 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4b6579e8{/stages/json,null,AVAILABLE,@Spark}
2020-04-08 11:15:54,086 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6fff253c{/stages/stage,null,AVAILABLE,@Spark}
2020-04-08 11:15:54,088 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3954d008{/stages/stage/json,null,AVAILABLE,@Spark}
2020-04-08 11:15:54,088 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f94c4db{/stages/pool,null,AVAILABLE,@Spark}
2020-04-08 11:15:54,089 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@593e824f{/stages/pool/json,null,AVAILABLE,@Spark}
2020-04-08 11:15:54,089 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@72ccd81a{/storage,null,AVAILABLE,@Spark}
2020-04-08 11:15:54,090 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6d8792db{/storage/json,null,AVAILABLE,@Spark}
2020-04-08 11:15:54,090 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@64bc21ac{/storage/rdd,null,AVAILABLE,@Spark}
2020-04-08 11:15:54,091 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@493dfb8e{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-04-08 11:15:54,091 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5d25e6bb{/environment,null,AVAILABLE,@Spark}
2020-04-08 11:15:54,092 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@ce5a68e{/environment/json,null,AVAILABLE,@Spark}
2020-04-08 11:15:54,093 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@9d157ff{/executors,null,AVAILABLE,@Spark}
2020-04-08 11:15:54,093 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f162cc0{/executors/json,null,AVAILABLE,@Spark}
2020-04-08 11:15:54,094 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5df417a7{/executors/threadDump,null,AVAILABLE,@Spark}
2020-04-08 11:15:54,094 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7c041b41{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-04-08 11:15:54,103 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7f69d591{/static,null,AVAILABLE,@Spark}
2020-04-08 11:15:54,104 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5af28b27{/,null,AVAILABLE,@Spark}
2020-04-08 11:15:54,105 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@71104a4{/api,null,AVAILABLE,@Spark}
2020-04-08 11:15:54,107 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@332a7fce{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-04-08 11:15:54,108 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@549621f3{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-04-08 11:15:54,110 INFO org.apache.spark.ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
2020-04-08 11:15:54,227 INFO org.apache.spark.executor.Executor: Starting executor ID driver on host localhost
2020-04-08 11:15:54,294 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 11:15:54,296 INFO org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 56112.
2020-04-08 11:15:54,296 INFO org.apache.spark.network.netty.NettyBlockTransferService: Server created on 192.168.0.102:56112
2020-04-08 11:15:54,298 INFO org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-04-08 11:15:54,338 INFO org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 56112, None)
2020-04-08 11:15:54,341 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:56112 with 4.1 GB RAM, BlockManagerId(driver, 192.168.0.102, 56112, None)
2020-04-08 11:15:54,345 INFO org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 56112, None)
2020-04-08 11:15:54,346 INFO org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 56112, None)
2020-04-08 11:15:54,564 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7bc9e6ab{/metrics/json,null,AVAILABLE,@Spark}
2020-04-08 11:15:54,878 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding enable.auto.commit to false for executor
2020-04-08 11:15:54,879 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding auto.offset.reset to none for executor
2020-04-08 11:15:54,880 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding executor group.id to spark-executor-testgroup1
2020-04-08 11:15:54,881 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
2020-04-08 11:15:56,826 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Slide time = 10000 ms
2020-04-08 11:15:56,826 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Storage level = Serialized 1x Replicated
2020-04-08 11:15:56,827 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Checkpoint interval = null
2020-04-08 11:15:56,827 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Remember interval = 10000 ms
2020-04-08 11:15:56,828 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@6ba9b66d
2020-04-08 11:15:56,828 INFO org.apache.spark.streaming.dstream.ForEachDStream: Slide time = 10000 ms
2020-04-08 11:15:56,828 INFO org.apache.spark.streaming.dstream.ForEachDStream: Storage level = Serialized 1x Replicated
2020-04-08 11:15:56,828 INFO org.apache.spark.streaming.dstream.ForEachDStream: Checkpoint interval = null
2020-04-08 11:15:56,828 INFO org.apache.spark.streaming.dstream.ForEachDStream: Remember interval = 10000 ms
2020-04-08 11:15:56,828 INFO org.apache.spark.streaming.dstream.ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@73c9d3d7
2020-04-08 11:15:56,890 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = testgroup1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-08 11:15:56,955 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-08 11:15:56,955 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-08 11:15:56,955 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586315756953
2020-04-08 11:15:56,957 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-1, groupId=testgroup1] Subscribed to topic(s): PABUSI2
2020-04-08 11:15:57,708 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-1, groupId=testgroup1] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-08 11:15:57,709 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Discovered group coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 11:15:57,712 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Revoking previously assigned partitions []
2020-04-08 11:15:57,712 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 11:15:58,245 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 11:16:01,632 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Successfully joined group with generation 1
2020-04-08 11:16:01,637 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting newly assigned partitions: PABUSI2-2, PABUSI2-0, PABUSI2-1
2020-04-08 11:16:01,824 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Found no committed offset for partition PABUSI2-2
2020-04-08 11:16:01,825 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Found no committed offset for partition PABUSI2-0
2020-04-08 11:16:01,825 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Found no committed offset for partition PABUSI2-1
2020-04-08 11:16:02,369 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 0.
2020-04-08 11:16:02,371 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 0.
2020-04-08 11:16:02,371 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 0.
2020-04-08 11:16:02,372 INFO org.apache.spark.streaming.util.RecurringTimer: Started timer for JobGenerator at time 1586315760000
2020-04-08 11:16:02,373 INFO org.apache.spark.streaming.scheduler.JobGenerator: Started JobGenerator at 1586315760000 ms
2020-04-08 11:16:02,375 INFO org.apache.spark.streaming.scheduler.JobScheduler: Started JobScheduler
2020-04-08 11:16:02,379 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@341672e{/streaming,null,AVAILABLE,@Spark}
2020-04-08 11:16:02,380 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2bebb74f{/streaming/json,null,AVAILABLE,@Spark}
2020-04-08 11:16:02,381 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@10641c09{/streaming/batch,null,AVAILABLE,@Spark}
2020-04-08 11:16:02,381 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@34f392be{/streaming/batch/json,null,AVAILABLE,@Spark}
2020-04-08 11:16:02,383 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@fabb651{/static/streaming,null,AVAILABLE,@Spark}
2020-04-08 11:16:02,384 INFO org.apache.spark.streaming.StreamingContext: StreamingContext started
2020-04-08 11:16:02,422 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 11:16:02,423 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 11:16:02,423 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 11:16:02,606 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 136.
2020-04-08 11:16:02,606 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 134.
2020-04-08 11:16:02,606 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 134.
2020-04-08 11:16:02,644 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586315760000 ms
2020-04-08 11:16:02,650 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586315760000 ms.0 from job set of time 1586315760000 ms
2020-04-08 11:16:02,722 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586315760000 ms.0 from job set of time 1586315760000 ms
2020-04-08 11:16:02,724 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 2.722 s for time 1586315760000 ms (execution: 0.074 s)
2020-04-08 11:16:02,731 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 11:16:02,736 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 
2020-04-08 11:16:10,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 11:16:10,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 11:16:10,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 11:16:10,181 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 134.
2020-04-08 11:16:10,184 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 136.
2020-04-08 11:16:10,184 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 134.
2020-04-08 11:16:10,185 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586315770000 ms
2020-04-08 11:16:10,186 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586315770000 ms.0 from job set of time 1586315770000 ms
2020-04-08 11:16:10,193 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586315770000 ms.0 from job set of time 1586315770000 ms
2020-04-08 11:16:10,193 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.192 s for time 1586315770000 ms (execution: 0.006 s)
2020-04-08 11:16:10,194 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 0 from persistence list
2020-04-08 11:16:10,200 INFO org.apache.spark.storage.BlockManager: Removing RDD 0
2020-04-08 11:16:10,201 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 11:16:10,201 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 
2020-04-08 11:16:20,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 11:16:20,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 11:16:20,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 11:16:20,183 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 141.
2020-04-08 11:16:20,183 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 141.
2020-04-08 11:16:20,184 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 142.
2020-04-08 11:16:20,185 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586315780000 ms
2020-04-08 11:16:20,185 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586315780000 ms.0 from job set of time 1586315780000 ms
2020-04-08 11:16:20,191 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586315780000 ms.0 from job set of time 1586315780000 ms
2020-04-08 11:16:20,192 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 2 from persistence list
2020-04-08 11:16:20,192 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.191 s for time 1586315780000 ms (execution: 0.006 s)
2020-04-08 11:16:20,192 INFO org.apache.spark.storage.BlockManager: Removing RDD 2
2020-04-08 11:16:20,192 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 11:16:20,193 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586315760000 ms
2020-04-08 11:16:30,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 11:16:30,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 11:16:30,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 11:16:30,186 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 141.
2020-04-08 11:16:30,186 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 141.
2020-04-08 11:16:30,187 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 142.
2020-04-08 11:16:30,188 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586315790000 ms
2020-04-08 11:16:30,188 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586315790000 ms.0 from job set of time 1586315790000 ms
2020-04-08 11:16:30,195 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586315790000 ms.0 from job set of time 1586315790000 ms
2020-04-08 11:16:30,196 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.195 s for time 1586315790000 ms (execution: 0.007 s)
2020-04-08 11:16:30,196 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 4 from persistence list
2020-04-08 11:16:30,196 INFO org.apache.spark.storage.BlockManager: Removing RDD 4
2020-04-08 11:16:30,196 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 11:16:30,197 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586315770000 ms
2020-04-08 11:16:40,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 11:16:40,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 11:16:40,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 11:16:40,189 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 141.
2020-04-08 11:16:40,190 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 142.
2020-04-08 11:16:40,190 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 141.
2020-04-08 11:16:40,191 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586315800000 ms
2020-04-08 11:16:40,191 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586315800000 ms.0 from job set of time 1586315800000 ms
2020-04-08 11:16:40,197 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586315800000 ms.0 from job set of time 1586315800000 ms
2020-04-08 11:16:40,197 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 6 from persistence list
2020-04-08 11:16:40,197 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.197 s for time 1586315800000 ms (execution: 0.006 s)
2020-04-08 11:16:40,198 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 11:16:40,198 INFO org.apache.spark.storage.BlockManager: Removing RDD 6
2020-04-08 11:16:40,198 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586315780000 ms
2020-04-08 11:16:50,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 11:16:50,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 11:16:50,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 11:16:50,187 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 142.
2020-04-08 11:16:50,187 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 141.
2020-04-08 11:16:50,187 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 141.
2020-04-08 11:16:50,188 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586315810000 ms
2020-04-08 11:16:50,188 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586315810000 ms.0 from job set of time 1586315810000 ms
2020-04-08 11:16:50,195 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586315810000 ms.0 from job set of time 1586315810000 ms
2020-04-08 11:16:50,195 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 8 from persistence list
2020-04-08 11:16:50,195 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.195 s for time 1586315810000 ms (execution: 0.007 s)
2020-04-08 11:16:50,196 INFO org.apache.spark.storage.BlockManager: Removing RDD 8
2020-04-08 11:16:50,196 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 11:16:50,196 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586315790000 ms
2020-04-08 11:16:52,001 INFO org.apache.spark.streaming.StreamingContext: Invoking stop(stopGracefully=false) from shutdown hook
2020-04-08 11:16:52,004 INFO org.apache.spark.streaming.scheduler.ReceiverTracker: ReceiverTracker stopped
2020-04-08 11:16:52,005 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopping JobGenerator immediately
2020-04-08 11:16:52,007 INFO org.apache.spark.streaming.util.RecurringTimer: Stopped timer for JobGenerator after time 1586315810000
2020-04-08 11:16:52,188 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Member consumer-1-320b8bbc-1e60-420b-8be4-ef6a88864643 sending LeaveGroup request to coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 11:16:52,378 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopped JobGenerator
2020-04-08 11:16:52,381 INFO org.apache.spark.streaming.scheduler.JobScheduler: Stopped JobScheduler
2020-04-08 11:16:52,385 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@341672e{/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 11:16:52,386 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@10641c09{/streaming/batch,null,UNAVAILABLE,@Spark}
2020-04-08 11:16:52,387 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@fabb651{/static/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 11:16:52,388 INFO org.apache.spark.streaming.StreamingContext: StreamingContext stopped successfully
2020-04-08 11:16:52,388 INFO org.apache.spark.SparkContext: Invoking stop() from shutdown hook
2020-04-08 11:16:52,393 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@29a60c27{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 11:16:52,394 INFO org.apache.spark.ui.SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
2020-04-08 11:16:52,401 INFO org.apache.spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2020-04-08 11:16:52,414 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2020-04-08 11:16:52,415 INFO org.apache.spark.storage.BlockManager: BlockManager stopped
2020-04-08 11:16:52,415 INFO org.apache.spark.storage.BlockManagerMaster: BlockManagerMaster stopped
2020-04-08 11:16:52,418 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2020-04-08 11:16:52,426 INFO org.apache.spark.SparkContext: Successfully stopped SparkContext
2020-04-08 11:16:52,427 INFO org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2020-04-08 11:16:52,427 INFO org.apache.spark.util.ShutdownHookManager: Deleting directory /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/spark-6549a4a7-eb53-462c-8793-be8b5959dbe5
2020-04-08 11:17:52,848 WARN org.apache.spark.util.Utils: Your hostname, MacBook-Pro-CW.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)
2020-04-08 11:17:52,849 WARN org.apache.spark.util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
2020-04-08 11:17:52,888 INFO org.apache.spark.SparkContext: Running Spark version 2.4.0.cloudera2
2020-04-08 11:17:53,087 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-04-08 11:17:53,203 INFO org.apache.spark.SparkContext: Submitted application: Kafka2SparkStreaming2Kudu-kerberos
2020-04-08 11:17:53,242 INFO org.apache.spark.SecurityManager: Changing view acls to: godiswc
2020-04-08 11:17:53,242 INFO org.apache.spark.SecurityManager: Changing modify acls to: godiswc
2020-04-08 11:17:53,243 INFO org.apache.spark.SecurityManager: Changing view acls groups to: 
2020-04-08 11:17:53,243 INFO org.apache.spark.SecurityManager: Changing modify acls groups to: 
2020-04-08 11:17:53,243 INFO org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(godiswc); groups with view permissions: Set(); users  with modify permissions: Set(godiswc); groups with modify permissions: Set()
2020-04-08 11:17:53,366 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 11:17:53,440 INFO org.apache.spark.util.Utils: Successfully started service 'sparkDriver' on port 56567.
2020-04-08 11:17:53,457 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
2020-04-08 11:17:53,468 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
2020-04-08 11:17:53,470 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-04-08 11:17:53,470 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2020-04-08 11:17:53,502 INFO org.apache.spark.storage.DiskBlockManager: Created local directory at /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/blockmgr-c34e63c4-230e-4163-8408-1ae226af761a
2020-04-08 11:17:53,516 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 4.1 GB
2020-04-08 11:17:53,526 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
2020-04-08 11:17:53,594 INFO org.spark_project.jetty.util.log: Logging initialized @1533ms
2020-04-08 11:17:53,681 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2019-04-30T04:42:08+08:00, git hash: e1bc35120a6617ee3df052294e433f3a25ce7097
2020-04-08 11:17:53,697 INFO org.spark_project.jetty.server.Server: Started @1640ms
2020-04-08 11:17:53,699 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 11:17:53,713 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@29a60c27{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 11:17:53,713 INFO org.apache.spark.util.Utils: Successfully started service 'SparkUI' on port 4040.
2020-04-08 11:17:53,734 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6f80fafe{/jobs,null,AVAILABLE,@Spark}
2020-04-08 11:17:53,735 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@32cb636e{/jobs/json,null,AVAILABLE,@Spark}
2020-04-08 11:17:53,735 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@63cd604c{/jobs/job,null,AVAILABLE,@Spark}
2020-04-08 11:17:53,738 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a1d204a{/jobs/job/json,null,AVAILABLE,@Spark}
2020-04-08 11:17:53,740 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62dae245{/stages,null,AVAILABLE,@Spark}
2020-04-08 11:17:53,741 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4b6579e8{/stages/json,null,AVAILABLE,@Spark}
2020-04-08 11:17:53,742 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6fff253c{/stages/stage,null,AVAILABLE,@Spark}
2020-04-08 11:17:53,743 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3954d008{/stages/stage/json,null,AVAILABLE,@Spark}
2020-04-08 11:17:53,743 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f94c4db{/stages/pool,null,AVAILABLE,@Spark}
2020-04-08 11:17:53,744 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@593e824f{/stages/pool/json,null,AVAILABLE,@Spark}
2020-04-08 11:17:53,744 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@72ccd81a{/storage,null,AVAILABLE,@Spark}
2020-04-08 11:17:53,745 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6d8792db{/storage/json,null,AVAILABLE,@Spark}
2020-04-08 11:17:53,745 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@64bc21ac{/storage/rdd,null,AVAILABLE,@Spark}
2020-04-08 11:17:53,746 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@493dfb8e{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-04-08 11:17:53,746 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5d25e6bb{/environment,null,AVAILABLE,@Spark}
2020-04-08 11:17:53,747 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@ce5a68e{/environment/json,null,AVAILABLE,@Spark}
2020-04-08 11:17:53,748 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@9d157ff{/executors,null,AVAILABLE,@Spark}
2020-04-08 11:17:53,748 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f162cc0{/executors/json,null,AVAILABLE,@Spark}
2020-04-08 11:17:53,749 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5df417a7{/executors/threadDump,null,AVAILABLE,@Spark}
2020-04-08 11:17:53,750 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7c041b41{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-04-08 11:17:53,756 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7f69d591{/static,null,AVAILABLE,@Spark}
2020-04-08 11:17:53,757 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5af28b27{/,null,AVAILABLE,@Spark}
2020-04-08 11:17:53,758 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@71104a4{/api,null,AVAILABLE,@Spark}
2020-04-08 11:17:53,759 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@332a7fce{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-04-08 11:17:53,760 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@549621f3{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-04-08 11:17:53,762 INFO org.apache.spark.ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
2020-04-08 11:17:53,924 INFO org.apache.spark.executor.Executor: Starting executor ID driver on host localhost
2020-04-08 11:17:53,994 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 11:17:53,996 INFO org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 56569.
2020-04-08 11:17:53,996 INFO org.apache.spark.network.netty.NettyBlockTransferService: Server created on 192.168.0.102:56569
2020-04-08 11:17:53,997 INFO org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-04-08 11:17:54,020 INFO org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 56569, None)
2020-04-08 11:17:54,022 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:56569 with 4.1 GB RAM, BlockManagerId(driver, 192.168.0.102, 56569, None)
2020-04-08 11:17:54,024 INFO org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 56569, None)
2020-04-08 11:17:54,024 INFO org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 56569, None)
2020-04-08 11:17:54,197 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7bc9e6ab{/metrics/json,null,AVAILABLE,@Spark}
2020-04-08 11:17:54,477 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding enable.auto.commit to false for executor
2020-04-08 11:17:54,477 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding auto.offset.reset to none for executor
2020-04-08 11:17:54,478 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding executor group.id to spark-executor-testgroup1
2020-04-08 11:17:54,478 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
2020-04-08 11:17:56,216 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Slide time = 10000 ms
2020-04-08 11:17:56,217 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Storage level = Serialized 1x Replicated
2020-04-08 11:17:56,217 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Checkpoint interval = null
2020-04-08 11:17:56,218 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Remember interval = 10000 ms
2020-04-08 11:17:56,218 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@4bfb3051
2020-04-08 11:17:56,218 INFO org.apache.spark.streaming.dstream.ForEachDStream: Slide time = 10000 ms
2020-04-08 11:17:56,218 INFO org.apache.spark.streaming.dstream.ForEachDStream: Storage level = Serialized 1x Replicated
2020-04-08 11:17:56,218 INFO org.apache.spark.streaming.dstream.ForEachDStream: Checkpoint interval = null
2020-04-08 11:17:56,218 INFO org.apache.spark.streaming.dstream.ForEachDStream: Remember interval = 10000 ms
2020-04-08 11:17:56,218 INFO org.apache.spark.streaming.dstream.ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@215a145c
2020-04-08 11:17:56,265 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = testgroup1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-08 11:17:56,311 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-08 11:17:56,311 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-08 11:17:56,311 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586315876310
2020-04-08 11:17:56,313 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-1, groupId=testgroup1] Subscribed to topic(s): PABUSI2
2020-04-08 11:17:57,013 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-1, groupId=testgroup1] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-08 11:17:57,014 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Discovered group coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 11:17:57,018 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Revoking previously assigned partitions []
2020-04-08 11:17:57,018 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 11:17:57,551 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 11:18:00,908 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Successfully joined group with generation 3
2020-04-08 11:18:00,911 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting newly assigned partitions: PABUSI2-2, PABUSI2-0, PABUSI2-1
2020-04-08 11:18:01,095 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-2 to the committed offset FetchPosition{offset=142, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-2.vpc.cloudera.com:9092 (id: 52 rack: null), epoch=0}}
2020-04-08 11:18:01,096 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-0 to the committed offset FetchPosition{offset=141, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-4.vpc.cloudera.com:9092 (id: 53 rack: null), epoch=2}}
2020-04-08 11:18:01,096 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-1 to the committed offset FetchPosition{offset=141, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-3.vpc.cloudera.com:9092 (id: 54 rack: null), epoch=2}}
2020-04-08 11:18:01,103 INFO org.apache.spark.streaming.util.RecurringTimer: Started timer for JobGenerator at time 1586315880000
2020-04-08 11:18:01,103 INFO org.apache.spark.streaming.scheduler.JobGenerator: Started JobGenerator at 1586315880000 ms
2020-04-08 11:18:01,104 INFO org.apache.spark.streaming.scheduler.JobScheduler: Started JobScheduler
2020-04-08 11:18:01,109 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@341672e{/streaming,null,AVAILABLE,@Spark}
2020-04-08 11:18:01,109 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2bebb74f{/streaming/json,null,AVAILABLE,@Spark}
2020-04-08 11:18:01,110 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@10641c09{/streaming/batch,null,AVAILABLE,@Spark}
2020-04-08 11:18:01,111 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@34f392be{/streaming/batch/json,null,AVAILABLE,@Spark}
2020-04-08 11:18:01,112 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@fabb651{/static/streaming,null,AVAILABLE,@Spark}
2020-04-08 11:18:01,113 INFO org.apache.spark.streaming.StreamingContext: StreamingContext started
2020-04-08 11:18:01,150 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 11:18:01,151 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 11:18:01,151 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 11:18:02,252 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 142.
2020-04-08 11:18:02,252 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 141.
2020-04-08 11:18:02,252 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 141.
2020-04-08 11:18:02,276 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586315880000 ms
2020-04-08 11:18:02,279 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586315880000 ms.0 from job set of time 1586315880000 ms
2020-04-08 11:18:02,322 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586315880000 ms.0 from job set of time 1586315880000 ms
2020-04-08 11:18:02,323 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 2.321 s for time 1586315880000 ms (execution: 0.043 s)
2020-04-08 11:18:02,327 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 11:18:02,330 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 
2020-04-08 11:18:10,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 11:18:10,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 11:18:10,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 11:18:10,200 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 142.
2020-04-08 11:18:10,200 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 141.
2020-04-08 11:18:10,200 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 141.
2020-04-08 11:18:10,561 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586315890000 ms
2020-04-08 11:18:10,562 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586315890000 ms.0 from job set of time 1586315890000 ms
2020-04-08 11:18:10,568 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586315890000 ms.0 from job set of time 1586315890000 ms
2020-04-08 11:18:10,569 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.568 s for time 1586315890000 ms (execution: 0.006 s)
2020-04-08 11:18:10,569 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 0 from persistence list
2020-04-08 11:18:10,573 INFO org.apache.spark.storage.BlockManager: Removing RDD 0
2020-04-08 11:18:10,574 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 11:18:10,574 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 
2020-04-08 11:18:20,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 11:18:20,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 11:18:20,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 11:18:20,179 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 149.
2020-04-08 11:18:20,181 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 148.
2020-04-08 11:18:20,181 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 147.
2020-04-08 11:18:20,183 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586315900000 ms
2020-04-08 11:18:20,183 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586315900000 ms.0 from job set of time 1586315900000 ms
2020-04-08 11:18:20,191 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586315900000 ms.0 from job set of time 1586315900000 ms
2020-04-08 11:18:20,191 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 2 from persistence list
2020-04-08 11:18:20,191 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.191 s for time 1586315900000 ms (execution: 0.008 s)
2020-04-08 11:18:20,192 INFO org.apache.spark.storage.BlockManager: Removing RDD 2
2020-04-08 11:18:20,192 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 11:18:20,192 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586315880000 ms
2020-04-08 11:18:30,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 11:18:30,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 11:18:30,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 11:18:30,179 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 149.
2020-04-08 11:18:30,179 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 148.
2020-04-08 11:18:30,180 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 147.
2020-04-08 11:18:30,181 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586315910000 ms
2020-04-08 11:18:30,182 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586315910000 ms.0 from job set of time 1586315910000 ms
2020-04-08 11:18:30,188 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586315910000 ms.0 from job set of time 1586315910000 ms
2020-04-08 11:18:30,188 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.188 s for time 1586315910000 ms (execution: 0.007 s)
2020-04-08 11:18:30,188 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 4 from persistence list
2020-04-08 11:18:30,189 INFO org.apache.spark.storage.BlockManager: Removing RDD 4
2020-04-08 11:18:30,189 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 11:18:30,189 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586315890000 ms
2020-04-08 11:18:40,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 11:18:40,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 11:18:40,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 11:18:40,184 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 149.
2020-04-08 11:18:40,184 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 148.
2020-04-08 11:18:40,185 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 147.
2020-04-08 11:18:40,186 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586315920000 ms
2020-04-08 11:18:40,186 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586315920000 ms.0 from job set of time 1586315920000 ms
2020-04-08 11:18:40,192 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586315920000 ms.0 from job set of time 1586315920000 ms
2020-04-08 11:18:40,192 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.192 s for time 1586315920000 ms (execution: 0.006 s)
2020-04-08 11:18:40,192 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 6 from persistence list
2020-04-08 11:18:40,193 INFO org.apache.spark.storage.BlockManager: Removing RDD 6
2020-04-08 11:18:40,193 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 11:18:40,193 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586315900000 ms
2020-04-08 11:18:46,956 INFO org.apache.spark.streaming.StreamingContext: Invoking stop(stopGracefully=false) from shutdown hook
2020-04-08 11:18:46,973 INFO org.apache.spark.streaming.scheduler.ReceiverTracker: ReceiverTracker stopped
2020-04-08 11:18:46,977 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopping JobGenerator immediately
2020-04-08 11:18:46,978 INFO org.apache.spark.streaming.util.RecurringTimer: Stopped timer for JobGenerator after time 1586315920000
2020-04-08 11:18:47,158 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Member consumer-1-3a0fb5d2-e02d-4bf5-8f5d-647482a3a86a sending LeaveGroup request to coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 11:18:47,345 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopped JobGenerator
2020-04-08 11:18:47,348 INFO org.apache.spark.streaming.scheduler.JobScheduler: Stopped JobScheduler
2020-04-08 11:18:47,352 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@341672e{/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 11:18:47,353 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@10641c09{/streaming/batch,null,UNAVAILABLE,@Spark}
2020-04-08 11:18:47,354 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@fabb651{/static/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 11:18:47,354 INFO org.apache.spark.streaming.StreamingContext: StreamingContext stopped successfully
2020-04-08 11:18:47,355 INFO org.apache.spark.SparkContext: Invoking stop() from shutdown hook
2020-04-08 11:18:47,360 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@29a60c27{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 11:18:47,361 INFO org.apache.spark.ui.SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
2020-04-08 11:18:47,370 INFO org.apache.spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2020-04-08 11:18:47,385 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2020-04-08 11:18:47,386 INFO org.apache.spark.storage.BlockManager: BlockManager stopped
2020-04-08 11:18:47,387 INFO org.apache.spark.storage.BlockManagerMaster: BlockManagerMaster stopped
2020-04-08 11:18:47,392 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2020-04-08 11:18:47,402 INFO org.apache.spark.SparkContext: Successfully stopped SparkContext
2020-04-08 11:18:47,403 INFO org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2020-04-08 11:18:47,404 INFO org.apache.spark.util.ShutdownHookManager: Deleting directory /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/spark-66b4a1bb-acdc-4f2d-bbc9-e1e7bf0d11d2
2020-04-08 11:27:25,460 WARN org.apache.spark.util.Utils: Your hostname, MacBook-Pro-CW.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)
2020-04-08 11:27:25,461 WARN org.apache.spark.util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
2020-04-08 11:27:25,501 INFO org.apache.spark.SparkContext: Running Spark version 2.4.0.cloudera2
2020-04-08 11:27:25,695 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-04-08 11:27:25,824 INFO org.apache.spark.SparkContext: Submitted application: Kafka2SparkStreaming2Kudu-kerberos
2020-04-08 11:27:25,865 INFO org.apache.spark.SecurityManager: Changing view acls to: godiswc
2020-04-08 11:27:25,865 INFO org.apache.spark.SecurityManager: Changing modify acls to: godiswc
2020-04-08 11:27:25,865 INFO org.apache.spark.SecurityManager: Changing view acls groups to: 
2020-04-08 11:27:25,866 INFO org.apache.spark.SecurityManager: Changing modify acls groups to: 
2020-04-08 11:27:25,866 INFO org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(godiswc); groups with view permissions: Set(); users  with modify permissions: Set(godiswc); groups with modify permissions: Set()
2020-04-08 11:27:26,013 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 11:27:26,093 INFO org.apache.spark.util.Utils: Successfully started service 'sparkDriver' on port 58898.
2020-04-08 11:27:26,110 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
2020-04-08 11:27:26,121 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
2020-04-08 11:27:26,123 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-04-08 11:27:26,124 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2020-04-08 11:27:26,154 INFO org.apache.spark.storage.DiskBlockManager: Created local directory at /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/blockmgr-4e5c2f42-ca2b-4c69-80d8-63ab6dd3191c
2020-04-08 11:27:26,166 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 4.1 GB
2020-04-08 11:27:26,177 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
2020-04-08 11:27:26,253 INFO org.spark_project.jetty.util.log: Logging initialized @1571ms
2020-04-08 11:27:26,322 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2019-04-30T04:42:08+08:00, git hash: e1bc35120a6617ee3df052294e433f3a25ce7097
2020-04-08 11:27:26,337 INFO org.spark_project.jetty.server.Server: Started @1657ms
2020-04-08 11:27:26,339 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 11:27:26,351 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@29a60c27{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 11:27:26,351 INFO org.apache.spark.util.Utils: Successfully started service 'SparkUI' on port 4040.
2020-04-08 11:27:26,370 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6f80fafe{/jobs,null,AVAILABLE,@Spark}
2020-04-08 11:27:26,371 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@32cb636e{/jobs/json,null,AVAILABLE,@Spark}
2020-04-08 11:27:26,371 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@63cd604c{/jobs/job,null,AVAILABLE,@Spark}
2020-04-08 11:27:26,374 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a1d204a{/jobs/job/json,null,AVAILABLE,@Spark}
2020-04-08 11:27:26,377 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62dae245{/stages,null,AVAILABLE,@Spark}
2020-04-08 11:27:26,378 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4b6579e8{/stages/json,null,AVAILABLE,@Spark}
2020-04-08 11:27:26,378 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6fff253c{/stages/stage,null,AVAILABLE,@Spark}
2020-04-08 11:27:26,379 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3954d008{/stages/stage/json,null,AVAILABLE,@Spark}
2020-04-08 11:27:26,380 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f94c4db{/stages/pool,null,AVAILABLE,@Spark}
2020-04-08 11:27:26,381 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@593e824f{/stages/pool/json,null,AVAILABLE,@Spark}
2020-04-08 11:27:26,381 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@72ccd81a{/storage,null,AVAILABLE,@Spark}
2020-04-08 11:27:26,382 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6d8792db{/storage/json,null,AVAILABLE,@Spark}
2020-04-08 11:27:26,382 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@64bc21ac{/storage/rdd,null,AVAILABLE,@Spark}
2020-04-08 11:27:26,383 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@493dfb8e{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-04-08 11:27:26,383 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5d25e6bb{/environment,null,AVAILABLE,@Spark}
2020-04-08 11:27:26,383 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@ce5a68e{/environment/json,null,AVAILABLE,@Spark}
2020-04-08 11:27:26,384 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@9d157ff{/executors,null,AVAILABLE,@Spark}
2020-04-08 11:27:26,384 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f162cc0{/executors/json,null,AVAILABLE,@Spark}
2020-04-08 11:27:26,385 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5df417a7{/executors/threadDump,null,AVAILABLE,@Spark}
2020-04-08 11:27:26,385 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7c041b41{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-04-08 11:27:26,390 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7f69d591{/static,null,AVAILABLE,@Spark}
2020-04-08 11:27:26,391 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5af28b27{/,null,AVAILABLE,@Spark}
2020-04-08 11:27:26,392 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@71104a4{/api,null,AVAILABLE,@Spark}
2020-04-08 11:27:26,393 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@332a7fce{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-04-08 11:27:26,394 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@549621f3{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-04-08 11:27:26,395 INFO org.apache.spark.ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
2020-04-08 11:27:26,488 INFO org.apache.spark.executor.Executor: Starting executor ID driver on host localhost
2020-04-08 11:27:26,574 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 11:27:26,575 INFO org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 58899.
2020-04-08 11:27:26,576 INFO org.apache.spark.network.netty.NettyBlockTransferService: Server created on 192.168.0.102:58899
2020-04-08 11:27:26,578 INFO org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-04-08 11:27:26,606 INFO org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 58899, None)
2020-04-08 11:27:26,608 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:58899 with 4.1 GB RAM, BlockManagerId(driver, 192.168.0.102, 58899, None)
2020-04-08 11:27:26,610 INFO org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 58899, None)
2020-04-08 11:27:26,611 INFO org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 58899, None)
2020-04-08 11:27:26,818 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7bc9e6ab{/metrics/json,null,AVAILABLE,@Spark}
2020-04-08 11:27:27,091 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding enable.auto.commit to false for executor
2020-04-08 11:27:27,091 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding auto.offset.reset to none for executor
2020-04-08 11:27:27,092 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding executor group.id to spark-executor-testgroup1
2020-04-08 11:27:27,092 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
2020-04-08 11:27:27,613 WARN org.apache.kudu.util.NetUtil: Slow DNS lookup! Resolved IP of `cw-instances-2.vpc.cloudera.com' to cw-instances-2.vpc.cloudera.com/10.65.1.47 in 161034270ns
2020-04-08 11:27:27,850 WARN org.apache.kudu.util.NetUtil: Slow DNS lookup! Resolved IP of `cw-instances-3.vpc.cloudera.com' to cw-instances-3.vpc.cloudera.com/10.65.9.108 in 157242209ns
2020-04-08 11:27:28,009 WARN org.apache.kudu.util.NetUtil: Slow DNS lookup! Resolved IP of `cw-instances-4.vpc.cloudera.com' to cw-instances-4.vpc.cloudera.com/10.65.12.223 in 155741473ns
2020-04-08 11:27:29,166 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Slide time = 10000 ms
2020-04-08 11:27:29,167 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Storage level = Serialized 1x Replicated
2020-04-08 11:27:29,167 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Checkpoint interval = null
2020-04-08 11:27:29,167 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Remember interval = 10000 ms
2020-04-08 11:27:29,168 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@69adfabe
2020-04-08 11:27:29,168 INFO org.apache.spark.streaming.dstream.ForEachDStream: Slide time = 10000 ms
2020-04-08 11:27:29,168 INFO org.apache.spark.streaming.dstream.ForEachDStream: Storage level = Serialized 1x Replicated
2020-04-08 11:27:29,168 INFO org.apache.spark.streaming.dstream.ForEachDStream: Checkpoint interval = null
2020-04-08 11:27:29,168 INFO org.apache.spark.streaming.dstream.ForEachDStream: Remember interval = 10000 ms
2020-04-08 11:27:29,168 INFO org.apache.spark.streaming.dstream.ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@2880d55d
2020-04-08 11:27:29,224 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = testgroup1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-08 11:27:29,275 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-08 11:27:29,275 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-08 11:27:29,275 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586316449274
2020-04-08 11:27:29,276 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-1, groupId=testgroup1] Subscribed to topic(s): PABUSI2
2020-04-08 11:27:29,967 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-1, groupId=testgroup1] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-08 11:27:29,968 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Discovered group coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 11:27:29,971 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Revoking previously assigned partitions []
2020-04-08 11:27:29,971 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 11:27:30,514 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 11:27:33,869 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Successfully joined group with generation 5
2020-04-08 11:27:33,874 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting newly assigned partitions: PABUSI2-2, PABUSI2-0, PABUSI2-1
2020-04-08 11:27:34,061 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-2 to the committed offset FetchPosition{offset=149, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-2.vpc.cloudera.com:9092 (id: 52 rack: null), epoch=0}}
2020-04-08 11:27:34,062 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-0 to the committed offset FetchPosition{offset=148, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-4.vpc.cloudera.com:9092 (id: 53 rack: null), epoch=2}}
2020-04-08 11:27:34,062 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-1 to the committed offset FetchPosition{offset=147, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-3.vpc.cloudera.com:9092 (id: 54 rack: null), epoch=2}}
2020-04-08 11:27:34,070 INFO org.apache.spark.streaming.util.RecurringTimer: Started timer for JobGenerator at time 1586316450000
2020-04-08 11:27:34,071 INFO org.apache.spark.streaming.scheduler.JobGenerator: Started JobGenerator at 1586316450000 ms
2020-04-08 11:27:34,072 INFO org.apache.spark.streaming.scheduler.JobScheduler: Started JobScheduler
2020-04-08 11:27:34,077 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@fabb651{/streaming,null,AVAILABLE,@Spark}
2020-04-08 11:27:34,078 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@27e5b378{/streaming/json,null,AVAILABLE,@Spark}
2020-04-08 11:27:34,079 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@58496c97{/streaming/batch,null,AVAILABLE,@Spark}
2020-04-08 11:27:34,080 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@ad3324b{/streaming/batch/json,null,AVAILABLE,@Spark}
2020-04-08 11:27:34,081 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@12968227{/static/streaming,null,AVAILABLE,@Spark}
2020-04-08 11:27:34,081 INFO org.apache.spark.streaming.StreamingContext: StreamingContext started
2020-04-08 11:27:34,115 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 11:27:34,116 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 11:27:34,116 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 11:27:35,108 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 149.
2020-04-08 11:27:35,109 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 148.
2020-04-08 11:27:35,109 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 147.
2020-04-08 11:27:35,132 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586316450000 ms
2020-04-08 11:27:35,134 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586316450000 ms.0 from job set of time 1586316450000 ms
2020-04-08 11:27:35,245 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:134
2020-04-08 11:27:35,259 INFO org.apache.spark.scheduler.DAGScheduler: Got job 0 (collect at Kafka2SparkStreaming2Kudu.scala:134) with 3 output partitions
2020-04-08 11:27:35,260 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 0 (collect at Kafka2SparkStreaming2Kudu.scala:134)
2020-04-08 11:27:35,260 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 11:27:35,263 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 11:27:35,268 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at Kafka2SparkStreaming2Kudu.scala:122), which has no missing parents
2020-04-08 11:27:35,317 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.1 KB, free 4.1 GB)
2020-04-08 11:27:35,577 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.4 KB, free 4.1 GB)
2020-04-08 11:27:35,579 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:58899 (size: 2.4 KB, free: 4.1 GB)
2020-04-08 11:27:35,580 INFO org.apache.spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2020-04-08 11:27:35,593 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at Kafka2SparkStreaming2Kudu.scala:122) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 11:27:35,594 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 0.0 with 3 tasks
2020-04-08 11:27:35,625 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:27:35,626 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:27:35,627 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:27:35,634 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
2020-04-08 11:27:35,635 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 0.0 (TID 2)
2020-04-08 11:27:35,635 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 0.0 (TID 1)
2020-04-08 11:27:35,674 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 149 is the same as ending offset skipping PABUSI2 2
2020-04-08 11:27:35,674 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 147 is the same as ending offset skipping PABUSI2 1
2020-04-08 11:27:35,674 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 148 is the same as ending offset skipping PABUSI2 0
2020-04-08 11:27:35,686 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 806 bytes result sent to driver
2020-04-08 11:27:35,686 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 0.0 (TID 2). 806 bytes result sent to driver
2020-04-08 11:27:35,686 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 0.0 (TID 1). 806 bytes result sent to driver
2020-04-08 11:27:35,690 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 74 ms on localhost (executor driver) (1/3)
2020-04-08 11:27:35,692 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 66 ms on localhost (executor driver) (2/3)
2020-04-08 11:27:35,692 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 66 ms on localhost (executor driver) (3/3)
2020-04-08 11:27:35,693 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-04-08 11:27:35,697 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 0 (collect at Kafka2SparkStreaming2Kudu.scala:134) finished in 0.411 s
2020-04-08 11:27:35,701 INFO org.apache.spark.scheduler.DAGScheduler: Job 0 finished: collect at Kafka2SparkStreaming2Kudu.scala:134, took 0.455792 s
2020-04-08 11:27:35,715 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586316450000 ms.0 from job set of time 1586316450000 ms
2020-04-08 11:27:35,716 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 5.714 s for time 1586316450000 ms (execution: 0.581 s)
2020-04-08 11:27:35,720 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 11:27:35,723 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 
2020-04-08 11:27:40,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 11:27:40,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 11:27:40,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 11:27:40,186 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 149.
2020-04-08 11:27:40,187 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 148.
2020-04-08 11:27:40,188 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 147.
2020-04-08 11:27:40,189 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586316460000 ms
2020-04-08 11:27:40,189 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586316460000 ms.0 from job set of time 1586316460000 ms
2020-04-08 11:27:40,200 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:134
2020-04-08 11:27:40,201 INFO org.apache.spark.scheduler.DAGScheduler: Got job 1 (collect at Kafka2SparkStreaming2Kudu.scala:134) with 3 output partitions
2020-04-08 11:27:40,201 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 1 (collect at Kafka2SparkStreaming2Kudu.scala:134)
2020-04-08 11:27:40,201 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 11:27:40,201 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 11:27:40,202 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at map at Kafka2SparkStreaming2Kudu.scala:122), which has no missing parents
2020-04-08 11:27:40,204 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 4.1 KB, free 4.1 GB)
2020-04-08 11:27:40,206 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.4 KB, free 4.1 GB)
2020-04-08 11:27:40,206 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.102:58899 (size: 2.4 KB, free: 4.1 GB)
2020-04-08 11:27:40,207 INFO org.apache.spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2020-04-08 11:27:40,208 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at map at Kafka2SparkStreaming2Kudu.scala:122) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 11:27:40,208 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 1.0 with 3 tasks
2020-04-08 11:27:40,209 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:27:40,209 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 4, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:27:40,209 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 1.0 (TID 5, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:27:40,210 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 1.0 (TID 4)
2020-04-08 11:27:40,210 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 1.0 (TID 5)
2020-04-08 11:27:40,210 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 1.0 (TID 3)
2020-04-08 11:27:40,213 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 149 is the same as ending offset skipping PABUSI2 2
2020-04-08 11:27:40,213 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 147 is the same as ending offset skipping PABUSI2 1
2020-04-08 11:27:40,213 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 148 is the same as ending offset skipping PABUSI2 0
2020-04-08 11:27:40,215 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 1.0 (TID 3). 720 bytes result sent to driver
2020-04-08 11:27:40,215 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 1.0 (TID 4). 720 bytes result sent to driver
2020-04-08 11:27:40,215 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 1.0 (TID 5). 720 bytes result sent to driver
2020-04-08 11:27:40,216 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 3) in 8 ms on localhost (executor driver) (1/3)
2020-04-08 11:27:40,217 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 4) in 8 ms on localhost (executor driver) (2/3)
2020-04-08 11:27:40,217 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 1.0 (TID 5) in 8 ms on localhost (executor driver) (3/3)
2020-04-08 11:27:40,217 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
2020-04-08 11:27:40,218 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 1 (collect at Kafka2SparkStreaming2Kudu.scala:134) finished in 0.015 s
2020-04-08 11:27:40,218 INFO org.apache.spark.scheduler.DAGScheduler: Job 1 finished: collect at Kafka2SparkStreaming2Kudu.scala:134, took 0.018126 s
2020-04-08 11:27:40,224 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586316460000 ms.0 from job set of time 1586316460000 ms
2020-04-08 11:27:40,224 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.224 s for time 1586316460000 ms (execution: 0.035 s)
2020-04-08 11:27:40,225 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 0 from persistence list
2020-04-08 11:27:40,230 INFO org.apache.spark.storage.BlockManager: Removing RDD 0
2020-04-08 11:27:40,230 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 11:27:40,230 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 
2020-04-08 11:27:50,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 11:27:50,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 11:27:50,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 11:27:50,185 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 149.
2020-04-08 11:27:50,185 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 147.
2020-04-08 11:27:50,186 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 148.
2020-04-08 11:27:50,186 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586316470000 ms
2020-04-08 11:27:50,187 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586316470000 ms.0 from job set of time 1586316470000 ms
2020-04-08 11:27:50,195 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:134
2020-04-08 11:27:50,196 INFO org.apache.spark.scheduler.DAGScheduler: Got job 2 (collect at Kafka2SparkStreaming2Kudu.scala:134) with 3 output partitions
2020-04-08 11:27:50,196 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 2 (collect at Kafka2SparkStreaming2Kudu.scala:134)
2020-04-08 11:27:50,196 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 11:27:50,196 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 11:27:50,197 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[7] at map at Kafka2SparkStreaming2Kudu.scala:122), which has no missing parents
2020-04-08 11:27:50,199 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 4.1 KB, free 4.1 GB)
2020-04-08 11:27:50,201 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.4 KB, free 4.1 GB)
2020-04-08 11:27:50,201 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.102:58899 (size: 2.4 KB, free: 4.1 GB)
2020-04-08 11:27:50,202 INFO org.apache.spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2020-04-08 11:27:50,202 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at map at Kafka2SparkStreaming2Kudu.scala:122) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 11:27:50,202 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 2.0 with 3 tasks
2020-04-08 11:27:50,203 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:27:50,203 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 2.0 (TID 7, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:27:50,204 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 2.0 (TID 8, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:27:50,204 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 2.0 (TID 6)
2020-04-08 11:27:50,204 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 2.0 (TID 7)
2020-04-08 11:27:50,204 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 2.0 (TID 8)
2020-04-08 11:27:50,207 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 147 is the same as ending offset skipping PABUSI2 1
2020-04-08 11:27:50,207 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 148 is the same as ending offset skipping PABUSI2 0
2020-04-08 11:27:50,208 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 149 is the same as ending offset skipping PABUSI2 2
2020-04-08 11:27:50,208 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 2.0 (TID 6). 720 bytes result sent to driver
2020-04-08 11:27:50,208 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 2.0 (TID 8). 720 bytes result sent to driver
2020-04-08 11:27:50,209 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 2.0 (TID 7). 720 bytes result sent to driver
2020-04-08 11:27:50,209 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 6) in 6 ms on localhost (executor driver) (1/3)
2020-04-08 11:27:50,210 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 2.0 (TID 8) in 7 ms on localhost (executor driver) (2/3)
2020-04-08 11:27:50,210 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 2.0 (TID 7) in 7 ms on localhost (executor driver) (3/3)
2020-04-08 11:27:50,210 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
2020-04-08 11:27:50,211 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 2 (collect at Kafka2SparkStreaming2Kudu.scala:134) finished in 0.013 s
2020-04-08 11:27:50,211 INFO org.apache.spark.scheduler.DAGScheduler: Job 2 finished: collect at Kafka2SparkStreaming2Kudu.scala:134, took 0.015997 s
2020-04-08 11:27:50,217 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586316470000 ms.0 from job set of time 1586316470000 ms
2020-04-08 11:27:50,217 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.217 s for time 1586316470000 ms (execution: 0.030 s)
2020-04-08 11:27:50,218 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 3 from persistence list
2020-04-08 11:27:50,218 INFO org.apache.spark.storage.BlockManager: Removing RDD 3
2020-04-08 11:27:50,218 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 11:27:50,218 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586316450000 ms
2020-04-08 11:28:00,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 11:28:00,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 11:28:00,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 11:28:00,188 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 147.
2020-04-08 11:28:00,188 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 149.
2020-04-08 11:28:00,189 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 148.
2020-04-08 11:28:00,190 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586316480000 ms
2020-04-08 11:28:00,190 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586316480000 ms.0 from job set of time 1586316480000 ms
2020-04-08 11:28:00,199 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:134
2020-04-08 11:28:00,200 INFO org.apache.spark.scheduler.DAGScheduler: Got job 3 (collect at Kafka2SparkStreaming2Kudu.scala:134) with 3 output partitions
2020-04-08 11:28:00,200 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 3 (collect at Kafka2SparkStreaming2Kudu.scala:134)
2020-04-08 11:28:00,200 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 11:28:00,200 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 11:28:00,201 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[10] at map at Kafka2SparkStreaming2Kudu.scala:122), which has no missing parents
2020-04-08 11:28:00,203 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 4.1 KB, free 4.1 GB)
2020-04-08 11:28:00,205 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.4 KB, free 4.1 GB)
2020-04-08 11:28:00,205 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.102:58899 (size: 2.4 KB, free: 4.1 GB)
2020-04-08 11:28:00,206 INFO org.apache.spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2020-04-08 11:28:00,206 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 3 (MapPartitionsRDD[10] at map at Kafka2SparkStreaming2Kudu.scala:122) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 11:28:00,206 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 3.0 with 3 tasks
2020-04-08 11:28:00,207 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 9, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:28:00,207 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 3.0 (TID 10, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:28:00,208 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 3.0 (TID 11, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:28:00,208 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 3.0 (TID 11)
2020-04-08 11:28:00,208 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 3.0 (TID 9)
2020-04-08 11:28:00,208 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 3.0 (TID 10)
2020-04-08 11:28:00,210 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 149 is the same as ending offset skipping PABUSI2 2
2020-04-08 11:28:00,211 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 148 is the same as ending offset skipping PABUSI2 0
2020-04-08 11:28:00,211 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 147 is the same as ending offset skipping PABUSI2 1
2020-04-08 11:28:00,211 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 3.0 (TID 10). 720 bytes result sent to driver
2020-04-08 11:28:00,212 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 3.0 (TID 9). 720 bytes result sent to driver
2020-04-08 11:28:00,212 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 3.0 (TID 11). 720 bytes result sent to driver
2020-04-08 11:28:00,213 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 9) in 5 ms on localhost (executor driver) (1/3)
2020-04-08 11:28:00,213 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 3.0 (TID 10) in 6 ms on localhost (executor driver) (2/3)
2020-04-08 11:28:00,213 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 3.0 (TID 11) in 6 ms on localhost (executor driver) (3/3)
2020-04-08 11:28:00,213 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
2020-04-08 11:28:00,214 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 3 (collect at Kafka2SparkStreaming2Kudu.scala:134) finished in 0.012 s
2020-04-08 11:28:00,214 INFO org.apache.spark.scheduler.DAGScheduler: Job 3 finished: collect at Kafka2SparkStreaming2Kudu.scala:134, took 0.015130 s
2020-04-08 11:28:00,220 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586316480000 ms.0 from job set of time 1586316480000 ms
2020-04-08 11:28:00,220 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.220 s for time 1586316480000 ms (execution: 0.030 s)
2020-04-08 11:28:00,220 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 6 from persistence list
2020-04-08 11:28:00,221 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 11:28:00,221 INFO org.apache.spark.storage.BlockManager: Removing RDD 6
2020-04-08 11:28:00,221 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586316460000 ms
2020-04-08 11:28:10,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 11:28:10,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 11:28:10,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 11:28:10,187 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 150.
2020-04-08 11:28:10,187 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 147.
2020-04-08 11:28:10,187 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 148.
2020-04-08 11:28:10,188 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586316490000 ms
2020-04-08 11:28:10,189 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586316490000 ms.0 from job set of time 1586316490000 ms
2020-04-08 11:28:10,197 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:134
2020-04-08 11:28:10,198 INFO org.apache.spark.scheduler.DAGScheduler: Got job 4 (collect at Kafka2SparkStreaming2Kudu.scala:134) with 3 output partitions
2020-04-08 11:28:10,198 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 4 (collect at Kafka2SparkStreaming2Kudu.scala:134)
2020-04-08 11:28:10,198 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 11:28:10,198 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 11:28:10,199 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[13] at map at Kafka2SparkStreaming2Kudu.scala:122), which has no missing parents
2020-04-08 11:28:10,201 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 4.1 KB, free 4.1 GB)
2020-04-08 11:28:10,202 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.4 KB, free 4.1 GB)
2020-04-08 11:28:10,203 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.102:58899 (size: 2.4 KB, free: 4.1 GB)
2020-04-08 11:28:10,203 INFO org.apache.spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1161
2020-04-08 11:28:10,204 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 4 (MapPartitionsRDD[13] at map at Kafka2SparkStreaming2Kudu.scala:122) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 11:28:10,204 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 4.0 with 3 tasks
2020-04-08 11:28:10,205 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 12, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:28:10,205 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 4.0 (TID 13, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:28:10,205 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 4.0 (TID 14, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:28:10,206 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 4.0 (TID 12)
2020-04-08 11:28:10,206 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 4.0 (TID 13)
2020-04-08 11:28:10,206 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 4.0 (TID 14)
2020-04-08 11:28:10,208 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 148 is the same as ending offset skipping PABUSI2 0
2020-04-08 11:28:10,208 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 147 is the same as ending offset skipping PABUSI2 1
2020-04-08 11:28:10,208 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Computing topic PABUSI2, partition 2 offsets 149 -> 150
2020-04-08 11:28:10,209 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 4.0 (TID 14). 720 bytes result sent to driver
2020-04-08 11:28:10,209 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 4.0 (TID 13). 720 bytes result sent to driver
2020-04-08 11:28:10,210 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 4.0 (TID 14) in 5 ms on localhost (executor driver) (1/3)
2020-04-08 11:28:10,210 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 4.0 (TID 13) in 5 ms on localhost (executor driver) (2/3)
2020-04-08 11:28:10,213 INFO org.apache.spark.streaming.kafka010.KafkaDataConsumer: Initializing cache 16 64 0.75
2020-04-08 11:28:10,217 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-testgroup1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-08 11:28:10,226 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-08 11:28:10,226 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-08 11:28:10,226 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586316490225
2020-04-08 11:28:10,227 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Subscribed to partition(s): PABUSI2-2
2020-04-08 11:28:10,230 INFO org.apache.spark.streaming.kafka010.InternalKafkaConsumer: Initial fetch for spark-executor-testgroup1 PABUSI2-2 149
2020-04-08 11:28:10,230 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Seeking to offset 149 for partition PABUSI2-2
2020-04-08 11:28:10,768 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-08 11:28:11,968 ERROR org.apache.spark.executor.Executor: Exception in task 0.0 in stage 4.0 (TID 12)
java.lang.NullPointerException
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:130)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:122)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:363)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1174)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:296)
	at scala.collection.AbstractIterator.to(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:275)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1174)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 11:28:12,083 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 4.0 (TID 12, localhost, executor driver): java.lang.NullPointerException
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:130)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:122)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:363)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1174)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:296)
	at scala.collection.AbstractIterator.to(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:275)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1174)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2020-04-08 11:28:12,084 ERROR org.apache.spark.scheduler.TaskSetManager: Task 0 in stage 4.0 failed 1 times; aborting job
2020-04-08 11:28:12,085 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
2020-04-08 11:28:12,088 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Cancelling stage 4
2020-04-08 11:28:12,088 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Killing all running tasks in stage 4: Stage cancelled
2020-04-08 11:28:12,090 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 4 (collect at Kafka2SparkStreaming2Kudu.scala:134) failed in 1.889 s due to Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 12, localhost, executor driver): java.lang.NullPointerException
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:130)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:122)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:363)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1174)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:296)
	at scala.collection.AbstractIterator.to(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:275)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1174)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
2020-04-08 11:28:12,090 INFO org.apache.spark.scheduler.DAGScheduler: Job 4 failed: collect at Kafka2SparkStreaming2Kudu.scala:134, took 1.892744 s
2020-04-08 11:28:12,091 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586316490000 ms.0 from job set of time 1586316490000 ms
2020-04-08 11:28:12,093 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1586316490000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 12, localhost, executor driver): java.lang.NullPointerException
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:130)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:122)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:363)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1174)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:296)
	at scala.collection.AbstractIterator.to(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:275)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1174)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:245)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:944)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1.apply(Kafka2SparkStreaming2Kudu.scala:134)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1.apply(Kafka2SparkStreaming2Kudu.scala:120)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:191)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.NullPointerException
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:130)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:122)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:363)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1174)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:296)
	at scala.collection.AbstractIterator.to(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:275)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1174)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	... 3 more
2020-04-08 11:28:12,103 INFO org.apache.spark.streaming.StreamingContext: Invoking stop(stopGracefully=false) from shutdown hook
2020-04-08 11:28:12,105 INFO org.apache.spark.streaming.scheduler.ReceiverTracker: ReceiverTracker stopped
2020-04-08 11:28:12,106 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopping JobGenerator immediately
2020-04-08 11:28:12,106 INFO org.apache.spark.streaming.util.RecurringTimer: Stopped timer for JobGenerator after time 1586316490000
2020-04-08 11:28:12,338 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Member consumer-1-6f428c12-aa67-4f3a-815c-efe0feb7ac8e sending LeaveGroup request to coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 11:28:12,520 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopped JobGenerator
2020-04-08 11:28:12,524 INFO org.apache.spark.streaming.scheduler.JobScheduler: Stopped JobScheduler
2020-04-08 11:28:12,527 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@fabb651{/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 11:28:12,528 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@58496c97{/streaming/batch,null,UNAVAILABLE,@Spark}
2020-04-08 11:28:12,529 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@12968227{/static/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 11:28:12,529 INFO org.apache.spark.streaming.StreamingContext: StreamingContext stopped successfully
2020-04-08 11:28:12,530 INFO org.apache.spark.SparkContext: Invoking stop() from shutdown hook
2020-04-08 11:28:12,533 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@29a60c27{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 11:28:12,535 INFO org.apache.spark.ui.SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
2020-04-08 11:28:12,542 INFO org.apache.spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2020-04-08 11:28:12,554 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2020-04-08 11:28:12,554 INFO org.apache.spark.storage.BlockManager: BlockManager stopped
2020-04-08 11:28:12,555 INFO org.apache.spark.storage.BlockManagerMaster: BlockManagerMaster stopped
2020-04-08 11:28:12,557 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2020-04-08 11:28:12,565 INFO org.apache.spark.SparkContext: Successfully stopped SparkContext
2020-04-08 11:28:12,565 INFO org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2020-04-08 11:28:12,566 INFO org.apache.spark.util.ShutdownHookManager: Deleting directory /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/spark-d114b87c-6654-4316-8ef3-ee0324ec6ce5
2020-04-08 11:30:26,942 WARN org.apache.spark.util.Utils: Your hostname, MacBook-Pro-CW.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)
2020-04-08 11:30:26,943 WARN org.apache.spark.util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
2020-04-08 11:30:26,977 INFO org.apache.spark.SparkContext: Running Spark version 2.4.0.cloudera2
2020-04-08 11:30:27,191 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-04-08 11:30:27,304 INFO org.apache.spark.SparkContext: Submitted application: Kafka2SparkStreaming2Kudu-kerberos
2020-04-08 11:30:27,341 INFO org.apache.spark.SecurityManager: Changing view acls to: godiswc
2020-04-08 11:30:27,342 INFO org.apache.spark.SecurityManager: Changing modify acls to: godiswc
2020-04-08 11:30:27,342 INFO org.apache.spark.SecurityManager: Changing view acls groups to: 
2020-04-08 11:30:27,342 INFO org.apache.spark.SecurityManager: Changing modify acls groups to: 
2020-04-08 11:30:27,343 INFO org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(godiswc); groups with view permissions: Set(); users  with modify permissions: Set(godiswc); groups with modify permissions: Set()
2020-04-08 11:30:27,469 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 11:30:27,552 INFO org.apache.spark.util.Utils: Successfully started service 'sparkDriver' on port 59581.
2020-04-08 11:30:27,569 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
2020-04-08 11:30:27,579 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
2020-04-08 11:30:27,581 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-04-08 11:30:27,581 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2020-04-08 11:30:27,613 INFO org.apache.spark.storage.DiskBlockManager: Created local directory at /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/blockmgr-92c56b20-8957-4360-a1b7-5e638efa4fd4
2020-04-08 11:30:27,627 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 4.1 GB
2020-04-08 11:30:27,638 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
2020-04-08 11:30:27,701 INFO org.spark_project.jetty.util.log: Logging initialized @1561ms
2020-04-08 11:30:27,769 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2019-04-30T04:42:08+08:00, git hash: e1bc35120a6617ee3df052294e433f3a25ce7097
2020-04-08 11:30:27,788 INFO org.spark_project.jetty.server.Server: Started @1650ms
2020-04-08 11:30:27,790 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 11:30:27,803 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@71e5f61d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 11:30:27,803 INFO org.apache.spark.util.Utils: Successfully started service 'SparkUI' on port 4040.
2020-04-08 11:30:27,823 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fd62371{/jobs,null,AVAILABLE,@Spark}
2020-04-08 11:30:27,823 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1d9bec4d{/jobs/json,null,AVAILABLE,@Spark}
2020-04-08 11:30:27,824 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5c48c0c0{/jobs/job,null,AVAILABLE,@Spark}
2020-04-08 11:30:27,826 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@25f7391e{/jobs/job/json,null,AVAILABLE,@Spark}
2020-04-08 11:30:27,829 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3f23a3a0{/stages,null,AVAILABLE,@Spark}
2020-04-08 11:30:27,829 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ab14cb9{/stages/json,null,AVAILABLE,@Spark}
2020-04-08 11:30:27,830 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fb97279{/stages/stage,null,AVAILABLE,@Spark}
2020-04-08 11:30:27,831 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@31024624{/stages/stage/json,null,AVAILABLE,@Spark}
2020-04-08 11:30:27,831 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@25bcd0c7{/stages/pool,null,AVAILABLE,@Spark}
2020-04-08 11:30:27,832 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@32cb636e{/stages/pool/json,null,AVAILABLE,@Spark}
2020-04-08 11:30:27,832 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@63cd604c{/storage,null,AVAILABLE,@Spark}
2020-04-08 11:30:27,833 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@40dd3977{/storage/json,null,AVAILABLE,@Spark}
2020-04-08 11:30:27,833 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3a4e343{/storage/rdd,null,AVAILABLE,@Spark}
2020-04-08 11:30:27,834 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a1d204a{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-04-08 11:30:27,834 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62dae245{/environment,null,AVAILABLE,@Spark}
2020-04-08 11:30:27,835 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4b6579e8{/environment/json,null,AVAILABLE,@Spark}
2020-04-08 11:30:27,835 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6fff253c{/executors,null,AVAILABLE,@Spark}
2020-04-08 11:30:27,836 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c6357f9{/executors/json,null,AVAILABLE,@Spark}
2020-04-08 11:30:27,836 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@591e58fa{/executors/threadDump,null,AVAILABLE,@Spark}
2020-04-08 11:30:27,837 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3954d008{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-04-08 11:30:27,845 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f94c4db{/static,null,AVAILABLE,@Spark}
2020-04-08 11:30:27,845 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@669513d8{/,null,AVAILABLE,@Spark}
2020-04-08 11:30:27,846 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3a1d593e{/api,null,AVAILABLE,@Spark}
2020-04-08 11:30:27,847 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@285d851a{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-04-08 11:30:27,848 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@314b8f2d{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-04-08 11:30:27,849 INFO org.apache.spark.ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
2020-04-08 11:30:27,978 INFO org.apache.spark.executor.Executor: Starting executor ID driver on host localhost
2020-04-08 11:30:28,058 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 11:30:28,059 INFO org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59586.
2020-04-08 11:30:28,059 INFO org.apache.spark.network.netty.NettyBlockTransferService: Server created on 192.168.0.102:59586
2020-04-08 11:30:28,060 INFO org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-04-08 11:30:28,080 INFO org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 59586, None)
2020-04-08 11:30:28,082 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:59586 with 4.1 GB RAM, BlockManagerId(driver, 192.168.0.102, 59586, None)
2020-04-08 11:30:28,084 INFO org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 59586, None)
2020-04-08 11:30:28,085 INFO org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 59586, None)
2020-04-08 11:30:28,256 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@49b07ee3{/metrics/json,null,AVAILABLE,@Spark}
2020-04-08 11:30:28,339 WARN org.apache.spark.streaming.StreamingContext: spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
2020-04-08 11:30:28,532 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding enable.auto.commit to false for executor
2020-04-08 11:30:28,533 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding auto.offset.reset to none for executor
2020-04-08 11:30:28,533 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding executor group.id to spark-executor-testgroup1
2020-04-08 11:30:28,534 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
2020-04-08 11:30:30,290 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Slide time = 10000 ms
2020-04-08 11:30:30,291 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Storage level = Serialized 1x Replicated
2020-04-08 11:30:30,291 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Checkpoint interval = null
2020-04-08 11:30:30,291 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Remember interval = 10000 ms
2020-04-08 11:30:30,292 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@6aa0826d
2020-04-08 11:30:30,292 INFO org.apache.spark.streaming.dstream.ForEachDStream: Slide time = 10000 ms
2020-04-08 11:30:30,292 INFO org.apache.spark.streaming.dstream.ForEachDStream: Storage level = Serialized 1x Replicated
2020-04-08 11:30:30,292 INFO org.apache.spark.streaming.dstream.ForEachDStream: Checkpoint interval = null
2020-04-08 11:30:30,292 INFO org.apache.spark.streaming.dstream.ForEachDStream: Remember interval = 10000 ms
2020-04-08 11:30:30,292 INFO org.apache.spark.streaming.dstream.ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@78b7041c
2020-04-08 11:30:30,368 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = testgroup1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-08 11:30:30,429 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-08 11:30:30,429 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-08 11:30:30,430 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586316630428
2020-04-08 11:30:30,432 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-1, groupId=testgroup1] Subscribed to topic(s): PABUSI2
2020-04-08 11:30:31,179 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-1, groupId=testgroup1] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-08 11:30:31,180 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Discovered group coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 11:30:31,183 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Revoking previously assigned partitions []
2020-04-08 11:30:31,183 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 11:30:31,714 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 11:30:35,074 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Successfully joined group with generation 7
2020-04-08 11:30:35,077 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting newly assigned partitions: PABUSI2-2, PABUSI2-0, PABUSI2-1
2020-04-08 11:30:35,266 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-2 to the committed offset FetchPosition{offset=150, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-2.vpc.cloudera.com:9092 (id: 52 rack: null), epoch=0}}
2020-04-08 11:30:35,268 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-0 to the committed offset FetchPosition{offset=148, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-4.vpc.cloudera.com:9092 (id: 53 rack: null), epoch=2}}
2020-04-08 11:30:35,268 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-1 to the committed offset FetchPosition{offset=147, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-3.vpc.cloudera.com:9092 (id: 54 rack: null), epoch=2}}
2020-04-08 11:30:35,277 INFO org.apache.spark.streaming.util.RecurringTimer: Started timer for JobGenerator at time 1586316640000
2020-04-08 11:30:35,278 INFO org.apache.spark.streaming.scheduler.JobGenerator: Started JobGenerator at 1586316640000 ms
2020-04-08 11:30:35,279 INFO org.apache.spark.streaming.scheduler.JobScheduler: Started JobScheduler
2020-04-08 11:30:35,284 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@24934262{/streaming,null,AVAILABLE,@Spark}
2020-04-08 11:30:35,285 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@93f432e{/streaming/json,null,AVAILABLE,@Spark}
2020-04-08 11:30:35,287 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5b051a5c{/streaming/batch,null,AVAILABLE,@Spark}
2020-04-08 11:30:35,288 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@639aba11{/streaming/batch/json,null,AVAILABLE,@Spark}
2020-04-08 11:30:35,289 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@58cf8f94{/static/streaming,null,AVAILABLE,@Spark}
2020-04-08 11:30:35,290 INFO org.apache.spark.streaming.StreamingContext: StreamingContext started
2020-04-08 11:30:40,045 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 11:30:40,045 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 11:30:40,046 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 11:30:40,236 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 150.
2020-04-08 11:30:40,236 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 148.
2020-04-08 11:30:40,236 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 148.
2020-04-08 11:30:40,265 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586316640000 ms
2020-04-08 11:30:40,268 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586316640000 ms.0 from job set of time 1586316640000 ms
2020-04-08 11:30:40,388 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:134
2020-04-08 11:30:40,403 INFO org.apache.spark.scheduler.DAGScheduler: Got job 0 (collect at Kafka2SparkStreaming2Kudu.scala:134) with 3 output partitions
2020-04-08 11:30:40,404 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 0 (collect at Kafka2SparkStreaming2Kudu.scala:134)
2020-04-08 11:30:40,404 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 11:30:40,405 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 11:30:40,409 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at Kafka2SparkStreaming2Kudu.scala:122), which has no missing parents
2020-04-08 11:30:40,449 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.1 KB, free 4.1 GB)
2020-04-08 11:30:40,702 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.4 KB, free 4.1 GB)
2020-04-08 11:30:40,704 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:59586 (size: 2.4 KB, free: 4.1 GB)
2020-04-08 11:30:40,705 INFO org.apache.spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2020-04-08 11:30:40,718 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at Kafka2SparkStreaming2Kudu.scala:122) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 11:30:40,719 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 0.0 with 3 tasks
2020-04-08 11:30:40,747 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:30:40,754 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
2020-04-08 11:30:40,774 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 150 is the same as ending offset skipping PABUSI2 2
2020-04-08 11:30:40,800 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 720 bytes result sent to driver
2020-04-08 11:30:40,803 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:30:40,803 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 0.0 (TID 1)
2020-04-08 11:30:40,805 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 66 ms on localhost (executor driver) (1/3)
2020-04-08 11:30:40,807 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Computing topic PABUSI2, partition 1 offsets 147 -> 148
2020-04-08 11:30:40,810 INFO org.apache.spark.streaming.kafka010.KafkaDataConsumer: Initializing cache 16 64 0.75
2020-04-08 11:30:40,813 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-testgroup1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-08 11:30:40,817 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-08 11:30:40,818 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-08 11:30:40,818 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586316640817
2020-04-08 11:30:40,818 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Subscribed to partition(s): PABUSI2-1
2020-04-08 11:30:40,821 INFO org.apache.spark.streaming.kafka010.InternalKafkaConsumer: Initial fetch for spark-executor-testgroup1 PABUSI2-1 147
2020-04-08 11:30:40,821 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Seeking to offset 147 for partition PABUSI2-1
2020-04-08 11:30:41,379 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-08 11:30:42,090 ERROR org.apache.spark.executor.Executor: Exception in task 1.0 in stage 0.0 (TID 1)
java.lang.NullPointerException
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:130)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:122)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:363)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1174)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:296)
	at scala.collection.AbstractIterator.to(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:275)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1174)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 11:30:42,102 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:30:42,103 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 0.0 (TID 2)
2020-04-08 11:30:42,105 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1, localhost, executor driver): java.lang.NullPointerException
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:130)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:122)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:363)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1174)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:296)
	at scala.collection.AbstractIterator.to(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:275)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1174)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2020-04-08 11:30:42,106 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 148 is the same as ending offset skipping PABUSI2 0
2020-04-08 11:30:42,106 ERROR org.apache.spark.scheduler.TaskSetManager: Task 1 in stage 0.0 failed 1 times; aborting job
2020-04-08 11:30:42,107 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 0.0 (TID 2). 720 bytes result sent to driver
2020-04-08 11:30:42,108 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 6 ms on localhost (executor driver) (2/3)
2020-04-08 11:30:42,109 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-04-08 11:30:42,110 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Cancelling stage 0
2020-04-08 11:30:42,110 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Killing all running tasks in stage 0: Stage cancelled
2020-04-08 11:30:42,111 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 0 (collect at Kafka2SparkStreaming2Kudu.scala:134) failed in 1.687 s due to Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1, localhost, executor driver): java.lang.NullPointerException
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:130)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:122)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:363)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1174)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:296)
	at scala.collection.AbstractIterator.to(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:275)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1174)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
2020-04-08 11:30:42,115 INFO org.apache.spark.scheduler.DAGScheduler: Job 0 failed: collect at Kafka2SparkStreaming2Kudu.scala:134, took 1.725741 s
2020-04-08 11:30:42,116 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586316640000 ms.0 from job set of time 1586316640000 ms
2020-04-08 11:30:42,118 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1586316640000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1, localhost, executor driver): java.lang.NullPointerException
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:130)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:122)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:363)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1174)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:296)
	at scala.collection.AbstractIterator.to(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:275)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1174)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:245)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:944)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1.apply(Kafka2SparkStreaming2Kudu.scala:134)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1.apply(Kafka2SparkStreaming2Kudu.scala:120)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:191)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.NullPointerException
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:130)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:122)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:363)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1174)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:296)
	at scala.collection.AbstractIterator.to(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:275)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1174)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	... 3 more
2020-04-08 11:30:42,124 INFO org.apache.spark.streaming.StreamingContext: Invoking stop(stopGracefully=false) from shutdown hook
2020-04-08 11:30:42,125 INFO org.apache.spark.streaming.scheduler.ReceiverTracker: ReceiverTracker stopped
2020-04-08 11:30:42,126 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopping JobGenerator immediately
2020-04-08 11:30:42,127 INFO org.apache.spark.streaming.util.RecurringTimer: Stopped timer for JobGenerator after time 1586316640000
2020-04-08 11:30:42,339 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Member consumer-1-e1419115-2f78-4e8d-aae3-edbcd9bcba0b sending LeaveGroup request to coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 11:30:42,549 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopped JobGenerator
2020-04-08 11:30:42,554 INFO org.apache.spark.streaming.scheduler.JobScheduler: Stopped JobScheduler
2020-04-08 11:30:42,558 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@24934262{/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 11:30:42,559 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@5b051a5c{/streaming/batch,null,UNAVAILABLE,@Spark}
2020-04-08 11:30:42,560 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@58cf8f94{/static/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 11:30:42,561 INFO org.apache.spark.streaming.StreamingContext: StreamingContext stopped successfully
2020-04-08 11:30:42,561 INFO org.apache.spark.SparkContext: Invoking stop() from shutdown hook
2020-04-08 11:30:42,567 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@71e5f61d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 11:30:42,568 INFO org.apache.spark.ui.SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
2020-04-08 11:30:42,575 INFO org.apache.spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2020-04-08 11:30:42,584 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2020-04-08 11:30:42,585 INFO org.apache.spark.storage.BlockManager: BlockManager stopped
2020-04-08 11:30:42,585 INFO org.apache.spark.storage.BlockManagerMaster: BlockManagerMaster stopped
2020-04-08 11:30:42,587 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2020-04-08 11:30:42,594 INFO org.apache.spark.SparkContext: Successfully stopped SparkContext
2020-04-08 11:30:42,594 INFO org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2020-04-08 11:30:42,595 INFO org.apache.spark.util.ShutdownHookManager: Deleting directory /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/spark-0825b771-60b1-4990-b1b7-53086426dad8
2020-04-08 11:34:06,661 WARN org.apache.spark.util.Utils: Your hostname, MacBook-Pro-CW.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)
2020-04-08 11:34:06,662 WARN org.apache.spark.util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
2020-04-08 11:34:06,690 INFO org.apache.spark.SparkContext: Running Spark version 2.4.0.cloudera2
2020-04-08 11:34:06,881 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-04-08 11:34:07,014 INFO org.apache.spark.SparkContext: Submitted application: Kafka2SparkStreaming2Kudu-kerberos
2020-04-08 11:34:07,055 INFO org.apache.spark.SecurityManager: Changing view acls to: godiswc
2020-04-08 11:34:07,055 INFO org.apache.spark.SecurityManager: Changing modify acls to: godiswc
2020-04-08 11:34:07,055 INFO org.apache.spark.SecurityManager: Changing view acls groups to: 
2020-04-08 11:34:07,056 INFO org.apache.spark.SecurityManager: Changing modify acls groups to: 
2020-04-08 11:34:07,056 INFO org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(godiswc); groups with view permissions: Set(); users  with modify permissions: Set(godiswc); groups with modify permissions: Set()
2020-04-08 11:34:07,177 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 11:34:07,252 INFO org.apache.spark.util.Utils: Successfully started service 'sparkDriver' on port 60417.
2020-04-08 11:34:07,268 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
2020-04-08 11:34:07,280 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
2020-04-08 11:34:07,282 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-04-08 11:34:07,282 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2020-04-08 11:34:07,313 INFO org.apache.spark.storage.DiskBlockManager: Created local directory at /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/blockmgr-efb87ce3-1794-4742-abba-0921e7e4ebff
2020-04-08 11:34:07,326 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 4.1 GB
2020-04-08 11:34:07,334 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
2020-04-08 11:34:07,388 INFO org.spark_project.jetty.util.log: Logging initialized @1511ms
2020-04-08 11:34:07,465 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2019-04-30T04:42:08+08:00, git hash: e1bc35120a6617ee3df052294e433f3a25ce7097
2020-04-08 11:34:07,488 INFO org.spark_project.jetty.server.Server: Started @1613ms
2020-04-08 11:34:07,491 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 11:34:07,504 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@71e5f61d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 11:34:07,505 INFO org.apache.spark.util.Utils: Successfully started service 'SparkUI' on port 4040.
2020-04-08 11:34:07,525 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fd62371{/jobs,null,AVAILABLE,@Spark}
2020-04-08 11:34:07,525 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1d9bec4d{/jobs/json,null,AVAILABLE,@Spark}
2020-04-08 11:34:07,526 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5c48c0c0{/jobs/job,null,AVAILABLE,@Spark}
2020-04-08 11:34:07,529 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@25f7391e{/jobs/job/json,null,AVAILABLE,@Spark}
2020-04-08 11:34:07,531 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3f23a3a0{/stages,null,AVAILABLE,@Spark}
2020-04-08 11:34:07,532 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ab14cb9{/stages/json,null,AVAILABLE,@Spark}
2020-04-08 11:34:07,532 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fb97279{/stages/stage,null,AVAILABLE,@Spark}
2020-04-08 11:34:07,534 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@31024624{/stages/stage/json,null,AVAILABLE,@Spark}
2020-04-08 11:34:07,535 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@25bcd0c7{/stages/pool,null,AVAILABLE,@Spark}
2020-04-08 11:34:07,535 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@32cb636e{/stages/pool/json,null,AVAILABLE,@Spark}
2020-04-08 11:34:07,536 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@63cd604c{/storage,null,AVAILABLE,@Spark}
2020-04-08 11:34:07,537 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@40dd3977{/storage/json,null,AVAILABLE,@Spark}
2020-04-08 11:34:07,537 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3a4e343{/storage/rdd,null,AVAILABLE,@Spark}
2020-04-08 11:34:07,538 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a1d204a{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-04-08 11:34:07,539 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62dae245{/environment,null,AVAILABLE,@Spark}
2020-04-08 11:34:07,539 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4b6579e8{/environment/json,null,AVAILABLE,@Spark}
2020-04-08 11:34:07,540 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6fff253c{/executors,null,AVAILABLE,@Spark}
2020-04-08 11:34:07,540 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c6357f9{/executors/json,null,AVAILABLE,@Spark}
2020-04-08 11:34:07,541 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@591e58fa{/executors/threadDump,null,AVAILABLE,@Spark}
2020-04-08 11:34:07,541 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3954d008{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-04-08 11:34:07,549 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f94c4db{/static,null,AVAILABLE,@Spark}
2020-04-08 11:34:07,550 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@669513d8{/,null,AVAILABLE,@Spark}
2020-04-08 11:34:07,551 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3a1d593e{/api,null,AVAILABLE,@Spark}
2020-04-08 11:34:07,552 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@285d851a{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-04-08 11:34:07,553 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@314b8f2d{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-04-08 11:34:07,555 INFO org.apache.spark.ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
2020-04-08 11:34:07,743 INFO org.apache.spark.executor.Executor: Starting executor ID driver on host localhost
2020-04-08 11:34:07,797 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 11:34:07,798 INFO org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 60422.
2020-04-08 11:34:07,799 INFO org.apache.spark.network.netty.NettyBlockTransferService: Server created on 192.168.0.102:60422
2020-04-08 11:34:07,800 INFO org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-04-08 11:34:07,818 INFO org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 60422, None)
2020-04-08 11:34:07,820 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:60422 with 4.1 GB RAM, BlockManagerId(driver, 192.168.0.102, 60422, None)
2020-04-08 11:34:07,821 INFO org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 60422, None)
2020-04-08 11:34:07,822 INFO org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 60422, None)
2020-04-08 11:34:07,970 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@49b07ee3{/metrics/json,null,AVAILABLE,@Spark}
2020-04-08 11:34:08,083 WARN org.apache.spark.streaming.StreamingContext: spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
2020-04-08 11:34:08,305 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding enable.auto.commit to false for executor
2020-04-08 11:34:08,305 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding auto.offset.reset to none for executor
2020-04-08 11:34:08,306 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding executor group.id to spark-executor-testgroup1
2020-04-08 11:34:08,307 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
2020-04-08 11:34:08,838 WARN org.apache.kudu.util.NetUtil: Slow DNS lookup! Resolved IP of `cw-instances-2.vpc.cloudera.com' to cw-instances-2.vpc.cloudera.com/10.65.1.47 in 161505605ns
2020-04-08 11:34:09,075 WARN org.apache.kudu.util.NetUtil: Slow DNS lookup! Resolved IP of `cw-instances-3.vpc.cloudera.com' to cw-instances-3.vpc.cloudera.com/10.65.9.108 in 156903397ns
2020-04-08 11:34:09,235 WARN org.apache.kudu.util.NetUtil: Slow DNS lookup! Resolved IP of `cw-instances-4.vpc.cloudera.com' to cw-instances-4.vpc.cloudera.com/10.65.12.223 in 157869944ns
2020-04-08 11:34:10,388 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Slide time = 10000 ms
2020-04-08 11:34:10,389 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Storage level = Serialized 1x Replicated
2020-04-08 11:34:10,389 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Checkpoint interval = null
2020-04-08 11:34:10,390 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Remember interval = 10000 ms
2020-04-08 11:34:10,390 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@3a83d30d
2020-04-08 11:34:10,391 INFO org.apache.spark.streaming.dstream.ForEachDStream: Slide time = 10000 ms
2020-04-08 11:34:10,391 INFO org.apache.spark.streaming.dstream.ForEachDStream: Storage level = Serialized 1x Replicated
2020-04-08 11:34:10,391 INFO org.apache.spark.streaming.dstream.ForEachDStream: Checkpoint interval = null
2020-04-08 11:34:10,391 INFO org.apache.spark.streaming.dstream.ForEachDStream: Remember interval = 10000 ms
2020-04-08 11:34:10,391 INFO org.apache.spark.streaming.dstream.ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@618fb8ee
2020-04-08 11:34:10,445 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = testgroup1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-08 11:34:10,492 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-08 11:34:10,492 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-08 11:34:10,492 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586316850491
2020-04-08 11:34:10,494 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-1, groupId=testgroup1] Subscribed to topic(s): PABUSI2
2020-04-08 11:34:11,179 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-1, groupId=testgroup1] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-08 11:34:11,180 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Discovered group coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 11:34:11,184 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Revoking previously assigned partitions []
2020-04-08 11:34:11,184 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 11:34:11,738 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 11:34:15,100 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Successfully joined group with generation 9
2020-04-08 11:34:15,106 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting newly assigned partitions: PABUSI2-2, PABUSI2-0, PABUSI2-1
2020-04-08 11:34:15,295 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-2 to the committed offset FetchPosition{offset=150, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-2.vpc.cloudera.com:9092 (id: 52 rack: null), epoch=0}}
2020-04-08 11:34:15,296 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-0 to the committed offset FetchPosition{offset=148, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-4.vpc.cloudera.com:9092 (id: 53 rack: null), epoch=2}}
2020-04-08 11:34:15,297 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-1 to the committed offset FetchPosition{offset=148, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-3.vpc.cloudera.com:9092 (id: 54 rack: null), epoch=2}}
2020-04-08 11:34:15,306 INFO org.apache.spark.streaming.util.RecurringTimer: Started timer for JobGenerator at time 1586316860000
2020-04-08 11:34:15,307 INFO org.apache.spark.streaming.scheduler.JobGenerator: Started JobGenerator at 1586316860000 ms
2020-04-08 11:34:15,308 INFO org.apache.spark.streaming.scheduler.JobScheduler: Started JobScheduler
2020-04-08 11:34:15,312 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@24934262{/streaming,null,AVAILABLE,@Spark}
2020-04-08 11:34:15,313 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@93f432e{/streaming/json,null,AVAILABLE,@Spark}
2020-04-08 11:34:15,314 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5b051a5c{/streaming/batch,null,AVAILABLE,@Spark}
2020-04-08 11:34:15,314 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@639aba11{/streaming/batch/json,null,AVAILABLE,@Spark}
2020-04-08 11:34:15,315 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@58cf8f94{/static/streaming,null,AVAILABLE,@Spark}
2020-04-08 11:34:15,315 INFO org.apache.spark.streaming.StreamingContext: StreamingContext started
2020-04-08 11:34:20,046 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 11:34:20,047 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 11:34:20,047 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 11:34:20,234 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 150.
2020-04-08 11:34:20,234 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 148.
2020-04-08 11:34:20,235 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 149.
2020-04-08 11:34:20,263 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586316860000 ms
2020-04-08 11:34:20,265 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586316860000 ms.0 from job set of time 1586316860000 ms
2020-04-08 11:34:20,348 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:135
2020-04-08 11:34:20,362 INFO org.apache.spark.scheduler.DAGScheduler: Got job 0 (collect at Kafka2SparkStreaming2Kudu.scala:135) with 3 output partitions
2020-04-08 11:34:20,362 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 0 (collect at Kafka2SparkStreaming2Kudu.scala:135)
2020-04-08 11:34:20,363 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 11:34:20,364 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 11:34:20,368 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at Kafka2SparkStreaming2Kudu.scala:122), which has no missing parents
2020-04-08 11:34:20,411 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 3.8 KB, free 4.1 GB)
2020-04-08 11:34:20,676 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.3 KB, free 4.1 GB)
2020-04-08 11:34:20,677 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:60422 (size: 2.3 KB, free: 4.1 GB)
2020-04-08 11:34:20,679 INFO org.apache.spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2020-04-08 11:34:20,691 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at Kafka2SparkStreaming2Kudu.scala:122) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 11:34:20,692 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 0.0 with 3 tasks
2020-04-08 11:34:20,720 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:34:20,727 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
2020-04-08 11:34:20,747 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 150 is the same as ending offset skipping PABUSI2 2
2020-04-08 11:34:20,755 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 722 bytes result sent to driver
2020-04-08 11:34:20,757 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:34:20,758 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 0.0 (TID 1)
2020-04-08 11:34:20,760 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 46 ms on localhost (executor driver) (1/3)
2020-04-08 11:34:20,760 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 148 is the same as ending offset skipping PABUSI2 1
2020-04-08 11:34:20,762 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 0.0 (TID 1). 722 bytes result sent to driver
2020-04-08 11:34:20,763 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:34:20,763 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 0.0 (TID 2)
2020-04-08 11:34:20,763 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 6 ms on localhost (executor driver) (2/3)
2020-04-08 11:34:20,766 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Computing topic PABUSI2, partition 0 offsets 148 -> 149
2020-04-08 11:34:20,768 INFO org.apache.spark.streaming.kafka010.KafkaDataConsumer: Initializing cache 16 64 0.75
2020-04-08 11:34:20,770 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-testgroup1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-08 11:34:20,775 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-08 11:34:20,775 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-08 11:34:20,775 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586316860775
2020-04-08 11:34:20,776 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Subscribed to partition(s): PABUSI2-0
2020-04-08 11:34:20,778 INFO org.apache.spark.streaming.kafka010.InternalKafkaConsumer: Initial fetch for spark-executor-testgroup1 PABUSI2-0 148
2020-04-08 11:34:20,778 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Seeking to offset 148 for partition PABUSI2-0
2020-04-08 11:34:21,312 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-08 11:34:22,026 ERROR org.apache.spark.executor.Executor: Exception in task 2.0 in stage 0.0 (TID 2)
java.lang.NullPointerException
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:130)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:122)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:363)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1174)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:296)
	at scala.collection.AbstractIterator.to(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:275)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1174)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 11:34:22,045 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 2.0 in stage 0.0 (TID 2, localhost, executor driver): java.lang.NullPointerException
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:130)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:122)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:363)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1174)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:296)
	at scala.collection.AbstractIterator.to(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:275)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1174)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2020-04-08 11:34:22,047 ERROR org.apache.spark.scheduler.TaskSetManager: Task 2 in stage 0.0 failed 1 times; aborting job
2020-04-08 11:34:22,048 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-04-08 11:34:22,050 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Cancelling stage 0
2020-04-08 11:34:22,051 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Killing all running tasks in stage 0: Stage cancelled
2020-04-08 11:34:22,052 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 0 (collect at Kafka2SparkStreaming2Kudu.scala:135) failed in 1.665 s due to Job aborted due to stage failure: Task 2 in stage 0.0 failed 1 times, most recent failure: Lost task 2.0 in stage 0.0 (TID 2, localhost, executor driver): java.lang.NullPointerException
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:130)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:122)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:363)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1174)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:296)
	at scala.collection.AbstractIterator.to(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:275)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1174)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
2020-04-08 11:34:22,056 INFO org.apache.spark.scheduler.DAGScheduler: Job 0 failed: collect at Kafka2SparkStreaming2Kudu.scala:135, took 1.707442 s
2020-04-08 11:34:22,058 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586316860000 ms.0 from job set of time 1586316860000 ms
2020-04-08 11:34:22,059 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1586316860000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 0.0 failed 1 times, most recent failure: Lost task 2.0 in stage 0.0 (TID 2, localhost, executor driver): java.lang.NullPointerException
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:130)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:122)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:363)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1174)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:296)
	at scala.collection.AbstractIterator.to(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:275)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1174)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:245)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:944)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1.apply(Kafka2SparkStreaming2Kudu.scala:135)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1.apply(Kafka2SparkStreaming2Kudu.scala:120)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:191)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.NullPointerException
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:130)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:122)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:363)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1174)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:296)
	at scala.collection.AbstractIterator.to(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:275)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1174)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	... 3 more
2020-04-08 11:34:22,066 INFO org.apache.spark.streaming.StreamingContext: Invoking stop(stopGracefully=false) from shutdown hook
2020-04-08 11:34:22,067 INFO org.apache.spark.streaming.scheduler.ReceiverTracker: ReceiverTracker stopped
2020-04-08 11:34:22,068 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopping JobGenerator immediately
2020-04-08 11:34:22,069 INFO org.apache.spark.streaming.util.RecurringTimer: Stopped timer for JobGenerator after time 1586316860000
2020-04-08 11:34:22,260 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Member consumer-1-25912711-b64b-404e-a2dd-e072977c4c03 sending LeaveGroup request to coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 11:34:22,452 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopped JobGenerator
2020-04-08 11:34:22,455 INFO org.apache.spark.streaming.scheduler.JobScheduler: Stopped JobScheduler
2020-04-08 11:34:22,458 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@24934262{/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 11:34:22,459 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@5b051a5c{/streaming/batch,null,UNAVAILABLE,@Spark}
2020-04-08 11:34:22,460 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@58cf8f94{/static/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 11:34:22,460 INFO org.apache.spark.streaming.StreamingContext: StreamingContext stopped successfully
2020-04-08 11:34:22,461 INFO org.apache.spark.SparkContext: Invoking stop() from shutdown hook
2020-04-08 11:34:22,465 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@71e5f61d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 11:34:22,466 INFO org.apache.spark.ui.SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
2020-04-08 11:34:22,475 INFO org.apache.spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2020-04-08 11:34:22,488 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2020-04-08 11:34:22,488 INFO org.apache.spark.storage.BlockManager: BlockManager stopped
2020-04-08 11:34:22,492 INFO org.apache.spark.storage.BlockManagerMaster: BlockManagerMaster stopped
2020-04-08 11:34:22,493 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2020-04-08 11:34:22,503 INFO org.apache.spark.SparkContext: Successfully stopped SparkContext
2020-04-08 11:34:22,504 INFO org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2020-04-08 11:34:22,505 INFO org.apache.spark.util.ShutdownHookManager: Deleting directory /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/spark-b31bcf74-8965-4e98-9323-1c916d3c76ae
2020-04-08 11:34:51,126 WARN org.apache.spark.util.Utils: Your hostname, MacBook-Pro-CW.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)
2020-04-08 11:34:51,127 WARN org.apache.spark.util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
2020-04-08 11:34:51,152 INFO org.apache.spark.SparkContext: Running Spark version 2.4.0.cloudera2
2020-04-08 11:34:51,354 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-04-08 11:34:51,462 INFO org.apache.spark.SparkContext: Submitted application: Kafka2SparkStreaming2Kudu-kerberos
2020-04-08 11:34:51,512 INFO org.apache.spark.SecurityManager: Changing view acls to: godiswc
2020-04-08 11:34:51,512 INFO org.apache.spark.SecurityManager: Changing modify acls to: godiswc
2020-04-08 11:34:51,513 INFO org.apache.spark.SecurityManager: Changing view acls groups to: 
2020-04-08 11:34:51,513 INFO org.apache.spark.SecurityManager: Changing modify acls groups to: 
2020-04-08 11:34:51,514 INFO org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(godiswc); groups with view permissions: Set(); users  with modify permissions: Set(godiswc); groups with modify permissions: Set()
2020-04-08 11:34:51,673 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 11:34:51,742 INFO org.apache.spark.util.Utils: Successfully started service 'sparkDriver' on port 60597.
2020-04-08 11:34:51,758 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
2020-04-08 11:34:51,769 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
2020-04-08 11:34:51,771 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-04-08 11:34:51,771 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2020-04-08 11:34:51,802 INFO org.apache.spark.storage.DiskBlockManager: Created local directory at /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/blockmgr-022ce5ac-ec78-4afd-8fc0-9e87fe6fd5c1
2020-04-08 11:34:51,815 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 4.1 GB
2020-04-08 11:34:51,824 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
2020-04-08 11:34:51,870 INFO org.spark_project.jetty.util.log: Logging initialized @1376ms
2020-04-08 11:34:51,925 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2019-04-30T04:42:08+08:00, git hash: e1bc35120a6617ee3df052294e433f3a25ce7097
2020-04-08 11:34:51,943 INFO org.spark_project.jetty.server.Server: Started @1449ms
2020-04-08 11:34:51,945 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 11:34:51,961 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@71e5f61d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 11:34:51,961 INFO org.apache.spark.util.Utils: Successfully started service 'SparkUI' on port 4040.
2020-04-08 11:34:51,994 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fd62371{/jobs,null,AVAILABLE,@Spark}
2020-04-08 11:34:51,995 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1d9bec4d{/jobs/json,null,AVAILABLE,@Spark}
2020-04-08 11:34:51,996 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5c48c0c0{/jobs/job,null,AVAILABLE,@Spark}
2020-04-08 11:34:52,000 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@25f7391e{/jobs/job/json,null,AVAILABLE,@Spark}
2020-04-08 11:34:52,008 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3f23a3a0{/stages,null,AVAILABLE,@Spark}
2020-04-08 11:34:52,009 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ab14cb9{/stages/json,null,AVAILABLE,@Spark}
2020-04-08 11:34:52,009 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fb97279{/stages/stage,null,AVAILABLE,@Spark}
2020-04-08 11:34:52,011 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@31024624{/stages/stage/json,null,AVAILABLE,@Spark}
2020-04-08 11:34:52,011 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@25bcd0c7{/stages/pool,null,AVAILABLE,@Spark}
2020-04-08 11:34:52,012 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@32cb636e{/stages/pool/json,null,AVAILABLE,@Spark}
2020-04-08 11:34:52,012 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@63cd604c{/storage,null,AVAILABLE,@Spark}
2020-04-08 11:34:52,013 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@40dd3977{/storage/json,null,AVAILABLE,@Spark}
2020-04-08 11:34:52,013 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3a4e343{/storage/rdd,null,AVAILABLE,@Spark}
2020-04-08 11:34:52,014 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a1d204a{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-04-08 11:34:52,015 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62dae245{/environment,null,AVAILABLE,@Spark}
2020-04-08 11:34:52,015 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4b6579e8{/environment/json,null,AVAILABLE,@Spark}
2020-04-08 11:34:52,016 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6fff253c{/executors,null,AVAILABLE,@Spark}
2020-04-08 11:34:52,016 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c6357f9{/executors/json,null,AVAILABLE,@Spark}
2020-04-08 11:34:52,017 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@591e58fa{/executors/threadDump,null,AVAILABLE,@Spark}
2020-04-08 11:34:52,018 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3954d008{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-04-08 11:34:52,024 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f94c4db{/static,null,AVAILABLE,@Spark}
2020-04-08 11:34:52,025 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@669513d8{/,null,AVAILABLE,@Spark}
2020-04-08 11:34:52,026 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3a1d593e{/api,null,AVAILABLE,@Spark}
2020-04-08 11:34:52,026 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@285d851a{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-04-08 11:34:52,027 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@314b8f2d{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-04-08 11:34:52,029 INFO org.apache.spark.ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
2020-04-08 11:34:52,139 INFO org.apache.spark.executor.Executor: Starting executor ID driver on host localhost
2020-04-08 11:34:52,237 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 11:34:52,240 INFO org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 60601.
2020-04-08 11:34:52,242 INFO org.apache.spark.network.netty.NettyBlockTransferService: Server created on 192.168.0.102:60601
2020-04-08 11:34:52,243 INFO org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-04-08 11:34:52,278 INFO org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 60601, None)
2020-04-08 11:34:52,281 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:60601 with 4.1 GB RAM, BlockManagerId(driver, 192.168.0.102, 60601, None)
2020-04-08 11:34:52,284 INFO org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 60601, None)
2020-04-08 11:34:52,284 INFO org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 60601, None)
2020-04-08 11:34:52,469 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@49b07ee3{/metrics/json,null,AVAILABLE,@Spark}
2020-04-08 11:34:52,536 WARN org.apache.spark.streaming.StreamingContext: spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
2020-04-08 11:34:52,720 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding enable.auto.commit to false for executor
2020-04-08 11:34:52,720 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding auto.offset.reset to none for executor
2020-04-08 11:34:52,721 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding executor group.id to spark-executor-testgroup1
2020-04-08 11:34:52,721 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
2020-04-08 11:34:54,454 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Slide time = 10000 ms
2020-04-08 11:34:54,454 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Storage level = Serialized 1x Replicated
2020-04-08 11:34:54,455 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Checkpoint interval = null
2020-04-08 11:34:54,455 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Remember interval = 10000 ms
2020-04-08 11:34:54,455 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@2939f83b
2020-04-08 11:34:54,455 INFO org.apache.spark.streaming.dstream.ForEachDStream: Slide time = 10000 ms
2020-04-08 11:34:54,456 INFO org.apache.spark.streaming.dstream.ForEachDStream: Storage level = Serialized 1x Replicated
2020-04-08 11:34:54,456 INFO org.apache.spark.streaming.dstream.ForEachDStream: Checkpoint interval = null
2020-04-08 11:34:54,456 INFO org.apache.spark.streaming.dstream.ForEachDStream: Remember interval = 10000 ms
2020-04-08 11:34:54,456 INFO org.apache.spark.streaming.dstream.ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@5e516aa6
2020-04-08 11:34:54,504 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = testgroup1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-08 11:34:54,569 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-08 11:34:54,569 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-08 11:34:54,569 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586316894567
2020-04-08 11:34:54,571 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-1, groupId=testgroup1] Subscribed to topic(s): PABUSI2
2020-04-08 11:34:55,315 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-1, groupId=testgroup1] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-08 11:34:55,316 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Discovered group coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 11:34:55,321 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Revoking previously assigned partitions []
2020-04-08 11:34:55,321 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 11:34:55,847 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 11:34:59,202 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Successfully joined group with generation 11
2020-04-08 11:34:59,207 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting newly assigned partitions: PABUSI2-2, PABUSI2-0, PABUSI2-1
2020-04-08 11:34:59,391 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-2 to the committed offset FetchPosition{offset=150, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-2.vpc.cloudera.com:9092 (id: 52 rack: null), epoch=0}}
2020-04-08 11:34:59,392 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-0 to the committed offset FetchPosition{offset=149, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-4.vpc.cloudera.com:9092 (id: 53 rack: null), epoch=2}}
2020-04-08 11:34:59,393 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-1 to the committed offset FetchPosition{offset=148, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-3.vpc.cloudera.com:9092 (id: 54 rack: null), epoch=2}}
2020-04-08 11:34:59,400 INFO org.apache.spark.streaming.util.RecurringTimer: Started timer for JobGenerator at time 1586316900000
2020-04-08 11:34:59,400 INFO org.apache.spark.streaming.scheduler.JobGenerator: Started JobGenerator at 1586316900000 ms
2020-04-08 11:34:59,401 INFO org.apache.spark.streaming.scheduler.JobScheduler: Started JobScheduler
2020-04-08 11:34:59,405 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@33a3c44a{/streaming,null,AVAILABLE,@Spark}
2020-04-08 11:34:59,406 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fcfca62{/streaming/json,null,AVAILABLE,@Spark}
2020-04-08 11:34:59,407 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@288214b1{/streaming/batch,null,AVAILABLE,@Spark}
2020-04-08 11:34:59,407 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@16eedaa6{/streaming/batch/json,null,AVAILABLE,@Spark}
2020-04-08 11:34:59,408 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@639aba11{/static/streaming,null,AVAILABLE,@Spark}
2020-04-08 11:34:59,408 INFO org.apache.spark.streaming.StreamingContext: StreamingContext started
2020-04-08 11:35:00,072 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 11:35:00,073 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 11:35:00,073 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 11:35:00,362 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 150.
2020-04-08 11:35:00,699 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 150.
2020-04-08 11:35:00,714 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 148.
2020-04-08 11:35:00,771 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586316900000 ms
2020-04-08 11:35:00,789 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586316900000 ms.0 from job set of time 1586316900000 ms
2020-04-08 11:35:01,026 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:135
2020-04-08 11:35:01,088 INFO org.apache.spark.scheduler.DAGScheduler: Got job 0 (collect at Kafka2SparkStreaming2Kudu.scala:135) with 3 output partitions
2020-04-08 11:35:01,089 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 0 (collect at Kafka2SparkStreaming2Kudu.scala:135)
2020-04-08 11:35:01,089 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 11:35:01,091 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 11:35:01,096 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at Kafka2SparkStreaming2Kudu.scala:122), which has no missing parents
2020-04-08 11:35:01,257 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 11:35:01,664 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 11:35:01,666 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:60601 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 11:35:01,668 INFO org.apache.spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2020-04-08 11:35:01,682 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at Kafka2SparkStreaming2Kudu.scala:122) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 11:35:01,683 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 0.0 with 3 tasks
2020-04-08 11:35:01,769 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:35:01,788 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
2020-04-08 11:35:01,829 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 150 is the same as ending offset skipping PABUSI2 2
2020-04-08 11:35:01,857 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 722 bytes result sent to driver
2020-04-08 11:35:01,860 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:35:01,860 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 0.0 (TID 1)
2020-04-08 11:35:01,864 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 127 ms on localhost (executor driver) (1/3)
2020-04-08 11:35:01,865 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 148 is the same as ending offset skipping PABUSI2 1
2020-04-08 11:35:01,868 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 0.0 (TID 1). 722 bytes result sent to driver
2020-04-08 11:35:01,870 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:35:01,871 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 11 ms on localhost (executor driver) (2/3)
2020-04-08 11:35:01,871 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 0.0 (TID 2)
2020-04-08 11:35:01,884 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Computing topic PABUSI2, partition 0 offsets 149 -> 150
2020-04-08 11:35:01,893 INFO org.apache.spark.streaming.kafka010.KafkaDataConsumer: Initializing cache 16 64 0.75
2020-04-08 11:35:01,908 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-testgroup1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-08 11:35:01,928 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-08 11:35:01,928 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-08 11:35:01,929 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586316901928
2020-04-08 11:35:01,932 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Subscribed to partition(s): PABUSI2-0
2020-04-08 11:35:01,942 INFO org.apache.spark.streaming.kafka010.InternalKafkaConsumer: Initial fetch for spark-executor-testgroup1 PABUSI2-0 149
2020-04-08 11:35:01,946 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Seeking to offset 149 for partition PABUSI2-0
2020-04-08 11:35:02,492 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-08 11:35:03,209 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 0.0 (TID 2). 5665 bytes result sent to driver
2020-04-08 11:35:03,214 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 1345 ms on localhost (executor driver) (3/3)
2020-04-08 11:35:03,215 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-04-08 11:35:03,216 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 0 (collect at Kafka2SparkStreaming2Kudu.scala:135) finished in 2.042 s
2020-04-08 11:35:03,220 INFO org.apache.spark.scheduler.DAGScheduler: Job 0 finished: collect at Kafka2SparkStreaming2Kudu.scala:135, took 2.193375 s
2020-04-08 11:35:03,244 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586316900000 ms.0 from job set of time 1586316900000 ms
2020-04-08 11:35:03,245 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 3.244 s for time 1586316900000 ms (execution: 2.466 s)
2020-04-08 11:35:03,248 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 11:35:03,250 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 
2020-04-08 11:35:10,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 11:35:10,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 11:35:10,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 11:35:10,182 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 150.
2020-04-08 11:35:10,182 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 148.
2020-04-08 11:35:10,182 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 150.
2020-04-08 11:35:10,183 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586316910000 ms
2020-04-08 11:35:10,184 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586316910000 ms.0 from job set of time 1586316910000 ms
2020-04-08 11:35:10,189 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:135
2020-04-08 11:35:10,190 INFO org.apache.spark.scheduler.DAGScheduler: Got job 1 (collect at Kafka2SparkStreaming2Kudu.scala:135) with 3 output partitions
2020-04-08 11:35:10,190 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 1 (collect at Kafka2SparkStreaming2Kudu.scala:135)
2020-04-08 11:35:10,190 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 11:35:10,190 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 11:35:10,191 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at map at Kafka2SparkStreaming2Kudu.scala:122), which has no missing parents
2020-04-08 11:35:10,193 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 11:35:10,195 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 11:35:10,195 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.102:60601 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 11:35:10,196 INFO org.apache.spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2020-04-08 11:35:10,197 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at map at Kafka2SparkStreaming2Kudu.scala:122) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 11:35:10,197 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 1.0 with 3 tasks
2020-04-08 11:35:10,198 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:35:10,198 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 1.0 (TID 3)
2020-04-08 11:35:10,200 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 150 is the same as ending offset skipping PABUSI2 2
2020-04-08 11:35:10,201 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 1.0 (TID 3). 722 bytes result sent to driver
2020-04-08 11:35:10,202 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 4, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:35:10,202 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 1.0 (TID 4)
2020-04-08 11:35:10,202 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 3) in 5 ms on localhost (executor driver) (1/3)
2020-04-08 11:35:10,204 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 148 is the same as ending offset skipping PABUSI2 1
2020-04-08 11:35:10,205 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 1.0 (TID 4). 679 bytes result sent to driver
2020-04-08 11:35:10,205 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 1.0 (TID 5, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:35:10,206 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 1.0 (TID 5)
2020-04-08 11:35:10,206 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 4) in 4 ms on localhost (executor driver) (2/3)
2020-04-08 11:35:10,207 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 150 is the same as ending offset skipping PABUSI2 0
2020-04-08 11:35:10,208 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 1.0 (TID 5). 679 bytes result sent to driver
2020-04-08 11:35:10,209 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 1.0 (TID 5) in 4 ms on localhost (executor driver) (3/3)
2020-04-08 11:35:10,209 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
2020-04-08 11:35:10,210 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 1 (collect at Kafka2SparkStreaming2Kudu.scala:135) finished in 0.017 s
2020-04-08 11:35:10,210 INFO org.apache.spark.scheduler.DAGScheduler: Job 1 finished: collect at Kafka2SparkStreaming2Kudu.scala:135, took 0.020383 s
2020-04-08 11:35:10,216 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586316910000 ms.0 from job set of time 1586316910000 ms
2020-04-08 11:35:10,217 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.216 s for time 1586316910000 ms (execution: 0.033 s)
2020-04-08 11:35:10,217 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 0 from persistence list
2020-04-08 11:35:10,222 INFO org.apache.spark.storage.BlockManager: Removing RDD 0
2020-04-08 11:35:10,222 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 11:35:10,223 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 
2020-04-08 11:35:20,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 11:35:20,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 11:35:20,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 11:35:20,179 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 150.
2020-04-08 11:35:20,179 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 148.
2020-04-08 11:35:20,180 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 150.
2020-04-08 11:35:20,181 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586316920000 ms
2020-04-08 11:35:20,181 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586316920000 ms.0 from job set of time 1586316920000 ms
2020-04-08 11:35:20,186 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:135
2020-04-08 11:35:20,187 INFO org.apache.spark.scheduler.DAGScheduler: Got job 2 (collect at Kafka2SparkStreaming2Kudu.scala:135) with 3 output partitions
2020-04-08 11:35:20,187 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 2 (collect at Kafka2SparkStreaming2Kudu.scala:135)
2020-04-08 11:35:20,187 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 11:35:20,187 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 11:35:20,188 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[7] at map at Kafka2SparkStreaming2Kudu.scala:122), which has no missing parents
2020-04-08 11:35:20,190 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 11:35:20,191 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 11:35:20,192 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.102:60601 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 11:35:20,192 INFO org.apache.spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2020-04-08 11:35:20,193 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at map at Kafka2SparkStreaming2Kudu.scala:122) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 11:35:20,193 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 2.0 with 3 tasks
2020-04-08 11:35:20,194 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:35:20,194 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 2.0 (TID 6)
2020-04-08 11:35:20,196 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 150 is the same as ending offset skipping PABUSI2 2
2020-04-08 11:35:20,196 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 2.0 (TID 6). 722 bytes result sent to driver
2020-04-08 11:35:20,197 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 2.0 (TID 7, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:35:20,197 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 2.0 (TID 7)
2020-04-08 11:35:20,197 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 6) in 4 ms on localhost (executor driver) (1/3)
2020-04-08 11:35:20,199 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 148 is the same as ending offset skipping PABUSI2 1
2020-04-08 11:35:20,200 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 2.0 (TID 7). 679 bytes result sent to driver
2020-04-08 11:35:20,200 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 2.0 (TID 8, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:35:20,201 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 2.0 (TID 8)
2020-04-08 11:35:20,201 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 2.0 (TID 7) in 4 ms on localhost (executor driver) (2/3)
2020-04-08 11:35:20,202 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 150 is the same as ending offset skipping PABUSI2 0
2020-04-08 11:35:20,203 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 2.0 (TID 8). 679 bytes result sent to driver
2020-04-08 11:35:20,203 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 2.0 (TID 8) in 3 ms on localhost (executor driver) (3/3)
2020-04-08 11:35:20,203 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
2020-04-08 11:35:20,204 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 2 (collect at Kafka2SparkStreaming2Kudu.scala:135) finished in 0.015 s
2020-04-08 11:35:20,204 INFO org.apache.spark.scheduler.DAGScheduler: Job 2 finished: collect at Kafka2SparkStreaming2Kudu.scala:135, took 0.017938 s
2020-04-08 11:35:20,209 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586316920000 ms.0 from job set of time 1586316920000 ms
2020-04-08 11:35:20,209 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.209 s for time 1586316920000 ms (execution: 0.028 s)
2020-04-08 11:35:20,209 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 3 from persistence list
2020-04-08 11:35:20,210 INFO org.apache.spark.storage.BlockManager: Removing RDD 3
2020-04-08 11:35:20,210 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 11:35:20,210 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586316900000 ms
2020-04-08 11:35:30,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 11:35:30,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 11:35:30,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 11:35:30,179 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 150.
2020-04-08 11:35:30,182 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 150.
2020-04-08 11:35:30,182 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 148.
2020-04-08 11:35:30,183 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586316930000 ms
2020-04-08 11:35:30,183 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586316930000 ms.0 from job set of time 1586316930000 ms
2020-04-08 11:35:30,188 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:135
2020-04-08 11:35:30,189 INFO org.apache.spark.scheduler.DAGScheduler: Got job 3 (collect at Kafka2SparkStreaming2Kudu.scala:135) with 3 output partitions
2020-04-08 11:35:30,189 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 3 (collect at Kafka2SparkStreaming2Kudu.scala:135)
2020-04-08 11:35:30,189 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 11:35:30,189 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 11:35:30,190 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[10] at map at Kafka2SparkStreaming2Kudu.scala:122), which has no missing parents
2020-04-08 11:35:30,192 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 11:35:30,193 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 11:35:30,194 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.102:60601 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 11:35:30,194 INFO org.apache.spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2020-04-08 11:35:30,195 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 3 (MapPartitionsRDD[10] at map at Kafka2SparkStreaming2Kudu.scala:122) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 11:35:30,195 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 3.0 with 3 tasks
2020-04-08 11:35:30,196 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 9, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:35:30,196 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 3.0 (TID 9)
2020-04-08 11:35:30,197 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 150 is the same as ending offset skipping PABUSI2 2
2020-04-08 11:35:30,198 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 3.0 (TID 9). 722 bytes result sent to driver
2020-04-08 11:35:30,199 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 3.0 (TID 10, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:35:30,199 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 3.0 (TID 10)
2020-04-08 11:35:30,199 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 9) in 3 ms on localhost (executor driver) (1/3)
2020-04-08 11:35:30,200 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 148 is the same as ending offset skipping PABUSI2 1
2020-04-08 11:35:30,202 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 3.0 (TID 10). 722 bytes result sent to driver
2020-04-08 11:35:30,202 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 3.0 (TID 11, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:35:30,202 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 3.0 (TID 11)
2020-04-08 11:35:30,202 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 3.0 (TID 10) in 3 ms on localhost (executor driver) (2/3)
2020-04-08 11:35:30,203 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 150 is the same as ending offset skipping PABUSI2 0
2020-04-08 11:35:30,204 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 3.0 (TID 11). 722 bytes result sent to driver
2020-04-08 11:35:30,205 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 3.0 (TID 11) in 2 ms on localhost (executor driver) (3/3)
2020-04-08 11:35:30,205 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
2020-04-08 11:35:30,205 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 3 (collect at Kafka2SparkStreaming2Kudu.scala:135) finished in 0.014 s
2020-04-08 11:35:30,205 INFO org.apache.spark.scheduler.DAGScheduler: Job 3 finished: collect at Kafka2SparkStreaming2Kudu.scala:135, took 0.017162 s
2020-04-08 11:35:30,212 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586316930000 ms.0 from job set of time 1586316930000 ms
2020-04-08 11:35:30,213 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.212 s for time 1586316930000 ms (execution: 0.029 s)
2020-04-08 11:35:30,213 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 6 from persistence list
2020-04-08 11:35:30,214 INFO org.apache.spark.storage.BlockManager: Removing RDD 6
2020-04-08 11:35:30,214 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 11:35:30,215 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586316910000 ms
2020-04-08 11:35:40,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 11:35:40,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 11:35:40,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 11:35:40,178 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 150.
2020-04-08 11:35:40,181 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 148.
2020-04-08 11:35:40,181 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 150.
2020-04-08 11:35:40,182 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586316940000 ms
2020-04-08 11:35:40,182 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586316940000 ms.0 from job set of time 1586316940000 ms
2020-04-08 11:35:40,186 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:135
2020-04-08 11:35:40,186 INFO org.apache.spark.scheduler.DAGScheduler: Got job 4 (collect at Kafka2SparkStreaming2Kudu.scala:135) with 3 output partitions
2020-04-08 11:35:40,186 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 4 (collect at Kafka2SparkStreaming2Kudu.scala:135)
2020-04-08 11:35:40,186 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 11:35:40,187 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 11:35:40,187 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[13] at map at Kafka2SparkStreaming2Kudu.scala:122), which has no missing parents
2020-04-08 11:35:40,189 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 11:35:40,191 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 11:35:40,191 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.102:60601 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 11:35:40,192 INFO org.apache.spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1161
2020-04-08 11:35:40,192 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 4 (MapPartitionsRDD[13] at map at Kafka2SparkStreaming2Kudu.scala:122) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 11:35:40,192 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 4.0 with 3 tasks
2020-04-08 11:35:40,193 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 12, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:35:40,193 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 4.0 (TID 12)
2020-04-08 11:35:40,194 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 150 is the same as ending offset skipping PABUSI2 2
2020-04-08 11:35:40,195 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 4.0 (TID 12). 679 bytes result sent to driver
2020-04-08 11:35:40,195 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 4.0 (TID 13, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:35:40,195 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 4.0 (TID 13)
2020-04-08 11:35:40,195 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 12) in 2 ms on localhost (executor driver) (1/3)
2020-04-08 11:35:40,196 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 148 is the same as ending offset skipping PABUSI2 1
2020-04-08 11:35:40,197 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 4.0 (TID 13). 722 bytes result sent to driver
2020-04-08 11:35:40,197 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 4.0 (TID 14, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:35:40,197 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 4.0 (TID 13) in 2 ms on localhost (executor driver) (2/3)
2020-04-08 11:35:40,197 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 4.0 (TID 14)
2020-04-08 11:35:40,198 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 150 is the same as ending offset skipping PABUSI2 0
2020-04-08 11:35:40,199 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 4.0 (TID 14). 722 bytes result sent to driver
2020-04-08 11:35:40,199 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 4.0 (TID 14) in 2 ms on localhost (executor driver) (3/3)
2020-04-08 11:35:40,199 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
2020-04-08 11:35:40,200 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 4 (collect at Kafka2SparkStreaming2Kudu.scala:135) finished in 0.012 s
2020-04-08 11:35:40,200 INFO org.apache.spark.scheduler.DAGScheduler: Job 4 finished: collect at Kafka2SparkStreaming2Kudu.scala:135, took 0.014278 s
2020-04-08 11:35:40,204 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586316940000 ms.0 from job set of time 1586316940000 ms
2020-04-08 11:35:40,205 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.204 s for time 1586316940000 ms (execution: 0.022 s)
2020-04-08 11:35:40,205 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 9 from persistence list
2020-04-08 11:35:40,205 INFO org.apache.spark.storage.BlockManager: Removing RDD 9
2020-04-08 11:35:40,205 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 11:35:40,206 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586316920000 ms
2020-04-08 11:35:50,001 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 11:35:50,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 11:35:50,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 11:35:50,230 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 150.
2020-04-08 11:35:50,230 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 148.
2020-04-08 11:35:50,234 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 150.
2020-04-08 11:35:50,235 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586316950000 ms
2020-04-08 11:35:50,235 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586316950000 ms.0 from job set of time 1586316950000 ms
2020-04-08 11:35:50,241 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:135
2020-04-08 11:35:50,242 INFO org.apache.spark.scheduler.DAGScheduler: Got job 5 (collect at Kafka2SparkStreaming2Kudu.scala:135) with 3 output partitions
2020-04-08 11:35:50,242 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 5 (collect at Kafka2SparkStreaming2Kudu.scala:135)
2020-04-08 11:35:50,243 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 11:35:50,243 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 11:35:50,243 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[16] at map at Kafka2SparkStreaming2Kudu.scala:122), which has no missing parents
2020-04-08 11:35:50,246 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 11:35:50,248 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 11:35:50,249 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.102:60601 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 11:35:50,249 INFO org.apache.spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1161
2020-04-08 11:35:50,250 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 5 (MapPartitionsRDD[16] at map at Kafka2SparkStreaming2Kudu.scala:122) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 11:35:50,250 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 5.0 with 3 tasks
2020-04-08 11:35:50,251 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 15, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:35:50,252 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 5.0 (TID 15)
2020-04-08 11:35:50,254 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 150 is the same as ending offset skipping PABUSI2 2
2020-04-08 11:35:50,256 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 5.0 (TID 15). 722 bytes result sent to driver
2020-04-08 11:35:50,257 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 5.0 (TID 16, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:35:50,257 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 15) in 6 ms on localhost (executor driver) (1/3)
2020-04-08 11:35:50,257 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 5.0 (TID 16)
2020-04-08 11:35:50,259 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 148 is the same as ending offset skipping PABUSI2 1
2020-04-08 11:35:50,260 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 5.0 (TID 16). 722 bytes result sent to driver
2020-04-08 11:35:50,261 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 5.0 (TID 17, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:35:50,261 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 5.0 (TID 17)
2020-04-08 11:35:50,261 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 5.0 (TID 16) in 5 ms on localhost (executor driver) (2/3)
2020-04-08 11:35:50,263 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 150 is the same as ending offset skipping PABUSI2 0
2020-04-08 11:35:50,264 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 5.0 (TID 17). 679 bytes result sent to driver
2020-04-08 11:35:50,265 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 5.0 (TID 17) in 4 ms on localhost (executor driver) (3/3)
2020-04-08 11:35:50,265 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
2020-04-08 11:35:50,265 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 5 (collect at Kafka2SparkStreaming2Kudu.scala:135) finished in 0.021 s
2020-04-08 11:35:50,266 INFO org.apache.spark.scheduler.DAGScheduler: Job 5 finished: collect at Kafka2SparkStreaming2Kudu.scala:135, took 0.024264 s
2020-04-08 11:35:50,274 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586316950000 ms.0 from job set of time 1586316950000 ms
2020-04-08 11:35:50,274 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 12 from persistence list
2020-04-08 11:35:50,274 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.274 s for time 1586316950000 ms (execution: 0.039 s)
2020-04-08 11:35:50,275 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 11:35:50,275 INFO org.apache.spark.storage.BlockManager: Removing RDD 12
2020-04-08 11:35:50,275 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586316930000 ms
2020-04-08 11:35:58,205 INFO org.apache.spark.streaming.StreamingContext: Invoking stop(stopGracefully=false) from shutdown hook
2020-04-08 11:35:58,208 INFO org.apache.spark.streaming.scheduler.ReceiverTracker: ReceiverTracker stopped
2020-04-08 11:35:58,209 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopping JobGenerator immediately
2020-04-08 11:35:58,210 INFO org.apache.spark.streaming.util.RecurringTimer: Stopped timer for JobGenerator after time 1586316950000
2020-04-08 11:35:58,444 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Member consumer-1-baf963d4-1faf-47eb-9363-e25a022d0460 sending LeaveGroup request to coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 11:35:58,626 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopped JobGenerator
2020-04-08 11:35:58,630 INFO org.apache.spark.streaming.scheduler.JobScheduler: Stopped JobScheduler
2020-04-08 11:35:58,635 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@33a3c44a{/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 11:35:58,636 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@288214b1{/streaming/batch,null,UNAVAILABLE,@Spark}
2020-04-08 11:35:58,637 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@639aba11{/static/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 11:35:58,638 INFO org.apache.spark.streaming.StreamingContext: StreamingContext stopped successfully
2020-04-08 11:35:58,638 INFO org.apache.spark.SparkContext: Invoking stop() from shutdown hook
2020-04-08 11:35:58,645 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@71e5f61d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 11:35:58,648 INFO org.apache.spark.ui.SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
2020-04-08 11:35:58,655 INFO org.apache.spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2020-04-08 11:35:58,671 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2020-04-08 11:35:58,672 INFO org.apache.spark.storage.BlockManager: BlockManager stopped
2020-04-08 11:35:58,673 INFO org.apache.spark.storage.BlockManagerMaster: BlockManagerMaster stopped
2020-04-08 11:35:58,675 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2020-04-08 11:35:58,688 INFO org.apache.spark.SparkContext: Successfully stopped SparkContext
2020-04-08 11:35:58,688 INFO org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2020-04-08 11:35:58,689 INFO org.apache.spark.util.ShutdownHookManager: Deleting directory /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/spark-ba63ca0c-a1ff-4167-9351-1d1b08afe991
2020-04-08 11:36:04,868 WARN org.apache.spark.util.Utils: Your hostname, MacBook-Pro-CW.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)
2020-04-08 11:36:04,869 WARN org.apache.spark.util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
2020-04-08 11:36:04,897 INFO org.apache.spark.SparkContext: Running Spark version 2.4.0.cloudera2
2020-04-08 11:36:05,117 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-04-08 11:36:05,216 INFO org.apache.spark.SparkContext: Submitted application: Kafka2SparkStreaming2Kudu-kerberos
2020-04-08 11:36:05,257 INFO org.apache.spark.SecurityManager: Changing view acls to: godiswc
2020-04-08 11:36:05,257 INFO org.apache.spark.SecurityManager: Changing modify acls to: godiswc
2020-04-08 11:36:05,258 INFO org.apache.spark.SecurityManager: Changing view acls groups to: 
2020-04-08 11:36:05,258 INFO org.apache.spark.SecurityManager: Changing modify acls groups to: 
2020-04-08 11:36:05,258 INFO org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(godiswc); groups with view permissions: Set(); users  with modify permissions: Set(godiswc); groups with modify permissions: Set()
2020-04-08 11:36:05,406 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 11:36:05,494 INFO org.apache.spark.util.Utils: Successfully started service 'sparkDriver' on port 60894.
2020-04-08 11:36:05,512 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
2020-04-08 11:36:05,523 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
2020-04-08 11:36:05,525 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-04-08 11:36:05,526 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2020-04-08 11:36:05,558 INFO org.apache.spark.storage.DiskBlockManager: Created local directory at /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/blockmgr-89978b26-fe61-45d1-9625-df7eba8b00c6
2020-04-08 11:36:05,572 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 4.1 GB
2020-04-08 11:36:05,582 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
2020-04-08 11:36:05,645 INFO org.spark_project.jetty.util.log: Logging initialized @1394ms
2020-04-08 11:36:05,709 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2019-04-30T04:42:08+08:00, git hash: e1bc35120a6617ee3df052294e433f3a25ce7097
2020-04-08 11:36:05,724 INFO org.spark_project.jetty.server.Server: Started @1475ms
2020-04-08 11:36:05,726 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 11:36:05,738 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@71e5f61d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 11:36:05,738 INFO org.apache.spark.util.Utils: Successfully started service 'SparkUI' on port 4040.
2020-04-08 11:36:05,761 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fd62371{/jobs,null,AVAILABLE,@Spark}
2020-04-08 11:36:05,762 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1d9bec4d{/jobs/json,null,AVAILABLE,@Spark}
2020-04-08 11:36:05,763 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5c48c0c0{/jobs/job,null,AVAILABLE,@Spark}
2020-04-08 11:36:05,766 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@25f7391e{/jobs/job/json,null,AVAILABLE,@Spark}
2020-04-08 11:36:05,768 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3f23a3a0{/stages,null,AVAILABLE,@Spark}
2020-04-08 11:36:05,769 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ab14cb9{/stages/json,null,AVAILABLE,@Spark}
2020-04-08 11:36:05,769 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fb97279{/stages/stage,null,AVAILABLE,@Spark}
2020-04-08 11:36:05,771 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@31024624{/stages/stage/json,null,AVAILABLE,@Spark}
2020-04-08 11:36:05,772 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@25bcd0c7{/stages/pool,null,AVAILABLE,@Spark}
2020-04-08 11:36:05,772 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@32cb636e{/stages/pool/json,null,AVAILABLE,@Spark}
2020-04-08 11:36:05,773 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@63cd604c{/storage,null,AVAILABLE,@Spark}
2020-04-08 11:36:05,773 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@40dd3977{/storage/json,null,AVAILABLE,@Spark}
2020-04-08 11:36:05,774 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3a4e343{/storage/rdd,null,AVAILABLE,@Spark}
2020-04-08 11:36:05,774 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a1d204a{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-04-08 11:36:05,775 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62dae245{/environment,null,AVAILABLE,@Spark}
2020-04-08 11:36:05,775 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4b6579e8{/environment/json,null,AVAILABLE,@Spark}
2020-04-08 11:36:05,776 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6fff253c{/executors,null,AVAILABLE,@Spark}
2020-04-08 11:36:05,776 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c6357f9{/executors/json,null,AVAILABLE,@Spark}
2020-04-08 11:36:05,777 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@591e58fa{/executors/threadDump,null,AVAILABLE,@Spark}
2020-04-08 11:36:05,777 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3954d008{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-04-08 11:36:05,784 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f94c4db{/static,null,AVAILABLE,@Spark}
2020-04-08 11:36:05,785 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@669513d8{/,null,AVAILABLE,@Spark}
2020-04-08 11:36:05,786 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3a1d593e{/api,null,AVAILABLE,@Spark}
2020-04-08 11:36:05,786 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@285d851a{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-04-08 11:36:05,787 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@314b8f2d{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-04-08 11:36:05,789 INFO org.apache.spark.ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
2020-04-08 11:36:05,915 INFO org.apache.spark.executor.Executor: Starting executor ID driver on host localhost
2020-04-08 11:36:05,994 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 11:36:05,995 INFO org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 60898.
2020-04-08 11:36:05,996 INFO org.apache.spark.network.netty.NettyBlockTransferService: Server created on 192.168.0.102:60898
2020-04-08 11:36:05,997 INFO org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-04-08 11:36:06,022 INFO org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 60898, None)
2020-04-08 11:36:06,025 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:60898 with 4.1 GB RAM, BlockManagerId(driver, 192.168.0.102, 60898, None)
2020-04-08 11:36:06,028 INFO org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 60898, None)
2020-04-08 11:36:06,028 INFO org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 60898, None)
2020-04-08 11:36:06,194 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@49b07ee3{/metrics/json,null,AVAILABLE,@Spark}
2020-04-08 11:36:06,278 WARN org.apache.spark.streaming.StreamingContext: spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
2020-04-08 11:36:06,471 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding enable.auto.commit to false for executor
2020-04-08 11:36:06,472 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding auto.offset.reset to none for executor
2020-04-08 11:36:06,472 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding executor group.id to spark-executor-testgroup1
2020-04-08 11:36:06,473 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
2020-04-08 11:36:08,222 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Slide time = 10000 ms
2020-04-08 11:36:08,222 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Storage level = Serialized 1x Replicated
2020-04-08 11:36:08,222 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Checkpoint interval = null
2020-04-08 11:36:08,223 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Remember interval = 10000 ms
2020-04-08 11:36:08,223 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@3c86d8e3
2020-04-08 11:36:08,223 INFO org.apache.spark.streaming.dstream.ForEachDStream: Slide time = 10000 ms
2020-04-08 11:36:08,223 INFO org.apache.spark.streaming.dstream.ForEachDStream: Storage level = Serialized 1x Replicated
2020-04-08 11:36:08,223 INFO org.apache.spark.streaming.dstream.ForEachDStream: Checkpoint interval = null
2020-04-08 11:36:08,224 INFO org.apache.spark.streaming.dstream.ForEachDStream: Remember interval = 10000 ms
2020-04-08 11:36:08,224 INFO org.apache.spark.streaming.dstream.ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@2de81321
2020-04-08 11:36:08,275 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = testgroup1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-08 11:36:08,331 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-08 11:36:08,331 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-08 11:36:08,331 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586316968330
2020-04-08 11:36:08,332 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-1, groupId=testgroup1] Subscribed to topic(s): PABUSI2
2020-04-08 11:36:09,039 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-1, groupId=testgroup1] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-08 11:36:09,040 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Discovered group coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 11:36:09,044 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Revoking previously assigned partitions []
2020-04-08 11:36:09,044 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 11:36:09,602 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 11:36:12,992 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Successfully joined group with generation 13
2020-04-08 11:36:12,997 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting newly assigned partitions: PABUSI2-2, PABUSI2-0, PABUSI2-1
2020-04-08 11:36:13,184 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-2 to the committed offset FetchPosition{offset=150, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-2.vpc.cloudera.com:9092 (id: 52 rack: null), epoch=0}}
2020-04-08 11:36:13,186 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-0 to the committed offset FetchPosition{offset=150, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-4.vpc.cloudera.com:9092 (id: 53 rack: null), epoch=2}}
2020-04-08 11:36:13,186 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-1 to the committed offset FetchPosition{offset=148, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-3.vpc.cloudera.com:9092 (id: 54 rack: null), epoch=2}}
2020-04-08 11:36:13,194 INFO org.apache.spark.streaming.util.RecurringTimer: Started timer for JobGenerator at time 1586316970000
2020-04-08 11:36:13,194 INFO org.apache.spark.streaming.scheduler.JobGenerator: Started JobGenerator at 1586316970000 ms
2020-04-08 11:36:13,195 INFO org.apache.spark.streaming.scheduler.JobScheduler: Started JobScheduler
2020-04-08 11:36:13,200 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@33a3c44a{/streaming,null,AVAILABLE,@Spark}
2020-04-08 11:36:13,201 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fcfca62{/streaming/json,null,AVAILABLE,@Spark}
2020-04-08 11:36:13,202 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@288214b1{/streaming/batch,null,AVAILABLE,@Spark}
2020-04-08 11:36:13,203 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@16eedaa6{/streaming/batch/json,null,AVAILABLE,@Spark}
2020-04-08 11:36:13,204 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@639aba11{/static/streaming,null,AVAILABLE,@Spark}
2020-04-08 11:36:13,204 INFO org.apache.spark.streaming.StreamingContext: StreamingContext started
2020-04-08 11:36:13,242 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 11:36:13,242 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 11:36:13,242 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 11:36:13,907 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 151.
2020-04-08 11:36:14,236 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 150.
2020-04-08 11:36:14,238 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 148.
2020-04-08 11:36:14,261 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586316970000 ms
2020-04-08 11:36:14,264 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586316970000 ms.0 from job set of time 1586316970000 ms
2020-04-08 11:36:14,306 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:135
2020-04-08 11:36:14,320 INFO org.apache.spark.scheduler.DAGScheduler: Got job 0 (collect at Kafka2SparkStreaming2Kudu.scala:135) with 3 output partitions
2020-04-08 11:36:14,321 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 0 (collect at Kafka2SparkStreaming2Kudu.scala:135)
2020-04-08 11:36:14,321 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 11:36:14,322 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 11:36:14,325 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at Kafka2SparkStreaming2Kudu.scala:122), which has no missing parents
2020-04-08 11:36:14,369 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 11:36:14,638 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 11:36:14,639 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:60898 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 11:36:14,641 INFO org.apache.spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2020-04-08 11:36:14,656 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at Kafka2SparkStreaming2Kudu.scala:122) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 11:36:14,657 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 0.0 with 3 tasks
2020-04-08 11:36:14,695 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:36:14,702 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
2020-04-08 11:36:14,730 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 150 is the same as ending offset skipping PABUSI2 2
2020-04-08 11:36:14,741 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 712 bytes result sent to driver
2020-04-08 11:36:14,744 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:36:14,744 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 0.0 (TID 1)
2020-04-08 11:36:14,747 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 63 ms on localhost (executor driver) (1/3)
2020-04-08 11:36:14,748 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 148 is the same as ending offset skipping PABUSI2 1
2020-04-08 11:36:14,749 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 0.0 (TID 1). 712 bytes result sent to driver
2020-04-08 11:36:14,751 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:36:14,751 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 0.0 (TID 2)
2020-04-08 11:36:14,751 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 8 ms on localhost (executor driver) (2/3)
2020-04-08 11:36:14,754 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Computing topic PABUSI2, partition 0 offsets 150 -> 151
2020-04-08 11:36:14,758 INFO org.apache.spark.streaming.kafka010.KafkaDataConsumer: Initializing cache 16 64 0.75
2020-04-08 11:36:14,761 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-testgroup1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-08 11:36:14,767 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-08 11:36:14,767 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-08 11:36:14,767 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586316974767
2020-04-08 11:36:14,767 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Subscribed to partition(s): PABUSI2-0
2020-04-08 11:36:14,770 INFO org.apache.spark.streaming.kafka010.InternalKafkaConsumer: Initial fetch for spark-executor-testgroup1 PABUSI2-0 150
2020-04-08 11:36:14,770 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Seeking to offset 150 for partition PABUSI2-0
2020-04-08 11:36:15,297 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-08 11:36:16,075 ERROR org.apache.spark.executor.Executor: Exception in task 2.0 in stage 0.0 (TID 2)
java.io.NotSerializableException: scala.collection.MapLike$$anon$2
Serialization stack:

	at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:40)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:456)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 11:36:16,093 ERROR org.apache.spark.scheduler.TaskSetManager: Task 2.0 in stage 0.0 (TID 2) had a not serializable result: scala.collection.MapLike$$anon$2
Serialization stack:
; not retrying
2020-04-08 11:36:16,094 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-04-08 11:36:16,095 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Cancelling stage 0
2020-04-08 11:36:16,096 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Killing all running tasks in stage 0: Stage cancelled
2020-04-08 11:36:16,097 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 0 (collect at Kafka2SparkStreaming2Kudu.scala:135) failed in 1.756 s due to Job aborted due to stage failure: Task 2.0 in stage 0.0 (TID 2) had a not serializable result: scala.collection.MapLike$$anon$2
Serialization stack:

2020-04-08 11:36:16,101 INFO org.apache.spark.scheduler.DAGScheduler: Job 0 failed: collect at Kafka2SparkStreaming2Kudu.scala:135, took 1.794929 s
2020-04-08 11:36:16,103 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586316970000 ms.0 from job set of time 1586316970000 ms
2020-04-08 11:36:16,104 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1586316970000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 2.0 in stage 0.0 (TID 2) had a not serializable result: scala.collection.MapLike$$anon$2
Serialization stack:

	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:245)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:944)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1.apply(Kafka2SparkStreaming2Kudu.scala:135)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1.apply(Kafka2SparkStreaming2Kudu.scala:120)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:191)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 11:36:16,116 INFO org.apache.spark.streaming.StreamingContext: Invoking stop(stopGracefully=false) from shutdown hook
2020-04-08 11:36:16,117 INFO org.apache.spark.streaming.scheduler.ReceiverTracker: ReceiverTracker stopped
2020-04-08 11:36:16,118 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopping JobGenerator immediately
2020-04-08 11:36:16,119 INFO org.apache.spark.streaming.util.RecurringTimer: Stopped timer for JobGenerator after time 1586316970000
2020-04-08 11:36:16,299 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Member consumer-1-f00cc81f-c9e5-4860-b7cd-cf3e71dd77c1 sending LeaveGroup request to coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 11:36:16,483 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopped JobGenerator
2020-04-08 11:36:16,486 INFO org.apache.spark.streaming.scheduler.JobScheduler: Stopped JobScheduler
2020-04-08 11:36:16,490 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@33a3c44a{/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 11:36:16,491 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@288214b1{/streaming/batch,null,UNAVAILABLE,@Spark}
2020-04-08 11:36:16,492 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@639aba11{/static/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 11:36:16,492 INFO org.apache.spark.streaming.StreamingContext: StreamingContext stopped successfully
2020-04-08 11:36:16,493 INFO org.apache.spark.SparkContext: Invoking stop() from shutdown hook
2020-04-08 11:36:16,500 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@71e5f61d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 11:36:16,502 INFO org.apache.spark.ui.SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
2020-04-08 11:36:16,511 INFO org.apache.spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2020-04-08 11:36:16,527 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2020-04-08 11:36:16,527 INFO org.apache.spark.storage.BlockManager: BlockManager stopped
2020-04-08 11:36:16,531 INFO org.apache.spark.storage.BlockManagerMaster: BlockManagerMaster stopped
2020-04-08 11:36:16,533 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2020-04-08 11:36:16,545 INFO org.apache.spark.SparkContext: Successfully stopped SparkContext
2020-04-08 11:36:16,545 INFO org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2020-04-08 11:36:16,546 INFO org.apache.spark.util.ShutdownHookManager: Deleting directory /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/spark-0eb6d7c9-1836-48e4-be09-661c17b1731c
2020-04-08 11:37:02,236 WARN org.apache.spark.util.Utils: Your hostname, MacBook-Pro-CW.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)
2020-04-08 11:37:02,237 WARN org.apache.spark.util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
2020-04-08 11:37:02,264 INFO org.apache.spark.SparkContext: Running Spark version 2.4.0.cloudera2
2020-04-08 11:37:02,465 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-04-08 11:37:02,589 INFO org.apache.spark.SparkContext: Submitted application: Kafka2SparkStreaming2Kudu-kerberos
2020-04-08 11:37:02,635 INFO org.apache.spark.SecurityManager: Changing view acls to: godiswc
2020-04-08 11:37:02,635 INFO org.apache.spark.SecurityManager: Changing modify acls to: godiswc
2020-04-08 11:37:02,636 INFO org.apache.spark.SecurityManager: Changing view acls groups to: 
2020-04-08 11:37:02,636 INFO org.apache.spark.SecurityManager: Changing modify acls groups to: 
2020-04-08 11:37:02,636 INFO org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(godiswc); groups with view permissions: Set(); users  with modify permissions: Set(godiswc); groups with modify permissions: Set()
2020-04-08 11:37:02,781 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 11:37:02,860 INFO org.apache.spark.util.Utils: Successfully started service 'sparkDriver' on port 61121.
2020-04-08 11:37:02,882 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
2020-04-08 11:37:02,895 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
2020-04-08 11:37:02,897 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-04-08 11:37:02,897 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2020-04-08 11:37:02,930 INFO org.apache.spark.storage.DiskBlockManager: Created local directory at /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/blockmgr-6c6163de-3d91-4f5e-a024-b2fa292ad5da
2020-04-08 11:37:02,943 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 4.1 GB
2020-04-08 11:37:02,952 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
2020-04-08 11:37:03,011 INFO org.spark_project.jetty.util.log: Logging initialized @1442ms
2020-04-08 11:37:03,091 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2019-04-30T04:42:08+08:00, git hash: e1bc35120a6617ee3df052294e433f3a25ce7097
2020-04-08 11:37:03,113 INFO org.spark_project.jetty.server.Server: Started @1546ms
2020-04-08 11:37:03,116 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 11:37:03,131 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@71e5f61d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 11:37:03,131 INFO org.apache.spark.util.Utils: Successfully started service 'SparkUI' on port 4040.
2020-04-08 11:37:03,161 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fd62371{/jobs,null,AVAILABLE,@Spark}
2020-04-08 11:37:03,162 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1d9bec4d{/jobs/json,null,AVAILABLE,@Spark}
2020-04-08 11:37:03,162 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5c48c0c0{/jobs/job,null,AVAILABLE,@Spark}
2020-04-08 11:37:03,166 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@25f7391e{/jobs/job/json,null,AVAILABLE,@Spark}
2020-04-08 11:37:03,168 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3f23a3a0{/stages,null,AVAILABLE,@Spark}
2020-04-08 11:37:03,169 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ab14cb9{/stages/json,null,AVAILABLE,@Spark}
2020-04-08 11:37:03,169 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fb97279{/stages/stage,null,AVAILABLE,@Spark}
2020-04-08 11:37:03,171 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@31024624{/stages/stage/json,null,AVAILABLE,@Spark}
2020-04-08 11:37:03,171 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@25bcd0c7{/stages/pool,null,AVAILABLE,@Spark}
2020-04-08 11:37:03,172 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@32cb636e{/stages/pool/json,null,AVAILABLE,@Spark}
2020-04-08 11:37:03,172 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@63cd604c{/storage,null,AVAILABLE,@Spark}
2020-04-08 11:37:03,172 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@40dd3977{/storage/json,null,AVAILABLE,@Spark}
2020-04-08 11:37:03,173 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3a4e343{/storage/rdd,null,AVAILABLE,@Spark}
2020-04-08 11:37:03,174 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a1d204a{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-04-08 11:37:03,175 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62dae245{/environment,null,AVAILABLE,@Spark}
2020-04-08 11:37:03,175 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4b6579e8{/environment/json,null,AVAILABLE,@Spark}
2020-04-08 11:37:03,176 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6fff253c{/executors,null,AVAILABLE,@Spark}
2020-04-08 11:37:03,181 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c6357f9{/executors/json,null,AVAILABLE,@Spark}
2020-04-08 11:37:03,182 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@591e58fa{/executors/threadDump,null,AVAILABLE,@Spark}
2020-04-08 11:37:03,183 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3954d008{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-04-08 11:37:03,190 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f94c4db{/static,null,AVAILABLE,@Spark}
2020-04-08 11:37:03,191 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@669513d8{/,null,AVAILABLE,@Spark}
2020-04-08 11:37:03,192 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3a1d593e{/api,null,AVAILABLE,@Spark}
2020-04-08 11:37:03,193 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@285d851a{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-04-08 11:37:03,193 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@314b8f2d{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-04-08 11:37:03,195 INFO org.apache.spark.ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
2020-04-08 11:37:03,346 INFO org.apache.spark.executor.Executor: Starting executor ID driver on host localhost
2020-04-08 11:37:03,401 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 11:37:03,402 INFO org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 61123.
2020-04-08 11:37:03,402 INFO org.apache.spark.network.netty.NettyBlockTransferService: Server created on 192.168.0.102:61123
2020-04-08 11:37:03,403 INFO org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-04-08 11:37:03,422 INFO org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 61123, None)
2020-04-08 11:37:03,424 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:61123 with 4.1 GB RAM, BlockManagerId(driver, 192.168.0.102, 61123, None)
2020-04-08 11:37:03,425 INFO org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 61123, None)
2020-04-08 11:37:03,425 INFO org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 61123, None)
2020-04-08 11:37:03,554 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@49b07ee3{/metrics/json,null,AVAILABLE,@Spark}
2020-04-08 11:37:03,695 WARN org.apache.spark.streaming.StreamingContext: spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
2020-04-08 11:37:03,894 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding enable.auto.commit to false for executor
2020-04-08 11:37:03,895 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding auto.offset.reset to none for executor
2020-04-08 11:37:03,895 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding executor group.id to spark-executor-testgroup1
2020-04-08 11:37:03,896 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
2020-04-08 11:37:05,639 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Slide time = 10000 ms
2020-04-08 11:37:05,639 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Storage level = Serialized 1x Replicated
2020-04-08 11:37:05,639 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Checkpoint interval = null
2020-04-08 11:37:05,640 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Remember interval = 10000 ms
2020-04-08 11:37:05,640 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@79eb74cf
2020-04-08 11:37:05,640 INFO org.apache.spark.streaming.dstream.ForEachDStream: Slide time = 10000 ms
2020-04-08 11:37:05,640 INFO org.apache.spark.streaming.dstream.ForEachDStream: Storage level = Serialized 1x Replicated
2020-04-08 11:37:05,641 INFO org.apache.spark.streaming.dstream.ForEachDStream: Checkpoint interval = null
2020-04-08 11:37:05,641 INFO org.apache.spark.streaming.dstream.ForEachDStream: Remember interval = 10000 ms
2020-04-08 11:37:05,641 INFO org.apache.spark.streaming.dstream.ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@713e9a2e
2020-04-08 11:37:05,698 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = testgroup1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-08 11:37:05,751 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-08 11:37:05,751 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-08 11:37:05,752 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586317025750
2020-04-08 11:37:05,753 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-1, groupId=testgroup1] Subscribed to topic(s): PABUSI2
2020-04-08 11:37:06,560 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-1, groupId=testgroup1] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-08 11:37:06,561 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Discovered group coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 11:37:06,564 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Revoking previously assigned partitions []
2020-04-08 11:37:06,564 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 11:37:07,326 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 11:37:10,733 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Successfully joined group with generation 15
2020-04-08 11:37:10,737 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting newly assigned partitions: PABUSI2-2, PABUSI2-0, PABUSI2-1
2020-04-08 11:37:10,924 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-2 to the committed offset FetchPosition{offset=150, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-2.vpc.cloudera.com:9092 (id: 52 rack: null), epoch=0}}
2020-04-08 11:37:10,926 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-0 to the committed offset FetchPosition{offset=151, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-4.vpc.cloudera.com:9092 (id: 53 rack: null), epoch=2}}
2020-04-08 11:37:10,926 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-1 to the committed offset FetchPosition{offset=148, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-3.vpc.cloudera.com:9092 (id: 54 rack: null), epoch=2}}
2020-04-08 11:37:10,935 INFO org.apache.spark.streaming.util.RecurringTimer: Started timer for JobGenerator at time 1586317030000
2020-04-08 11:37:10,935 INFO org.apache.spark.streaming.scheduler.JobGenerator: Started JobGenerator at 1586317030000 ms
2020-04-08 11:37:10,936 INFO org.apache.spark.streaming.scheduler.JobScheduler: Started JobScheduler
2020-04-08 11:37:10,941 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@33a3c44a{/streaming,null,AVAILABLE,@Spark}
2020-04-08 11:37:10,942 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fcfca62{/streaming/json,null,AVAILABLE,@Spark}
2020-04-08 11:37:10,944 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@288214b1{/streaming/batch,null,AVAILABLE,@Spark}
2020-04-08 11:37:10,945 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@16eedaa6{/streaming/batch/json,null,AVAILABLE,@Spark}
2020-04-08 11:37:10,947 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@639aba11{/static/streaming,null,AVAILABLE,@Spark}
2020-04-08 11:37:10,947 INFO org.apache.spark.streaming.StreamingContext: StreamingContext started
2020-04-08 11:37:10,988 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 11:37:10,989 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 11:37:10,989 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 11:37:11,484 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 149.
2020-04-08 11:37:11,991 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 151.
2020-04-08 11:37:11,991 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 150.
2020-04-08 11:37:12,016 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586317030000 ms
2020-04-08 11:37:12,019 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586317030000 ms.0 from job set of time 1586317030000 ms
2020-04-08 11:37:12,060 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:135
2020-04-08 11:37:12,075 INFO org.apache.spark.scheduler.DAGScheduler: Got job 0 (collect at Kafka2SparkStreaming2Kudu.scala:135) with 3 output partitions
2020-04-08 11:37:12,075 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 0 (collect at Kafka2SparkStreaming2Kudu.scala:135)
2020-04-08 11:37:12,075 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 11:37:12,076 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 11:37:12,079 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at Kafka2SparkStreaming2Kudu.scala:122), which has no missing parents
2020-04-08 11:37:12,115 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 11:37:12,377 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 11:37:12,378 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:61123 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 11:37:12,380 INFO org.apache.spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2020-04-08 11:37:12,392 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at Kafka2SparkStreaming2Kudu.scala:122) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 11:37:12,392 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 0.0 with 3 tasks
2020-04-08 11:37:12,425 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:37:12,432 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
2020-04-08 11:37:12,456 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 150 is the same as ending offset skipping PABUSI2 2
2020-04-08 11:37:12,467 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 712 bytes result sent to driver
2020-04-08 11:37:12,468 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:37:12,469 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 0.0 (TID 1)
2020-04-08 11:37:12,472 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 55 ms on localhost (executor driver) (1/3)
2020-04-08 11:37:12,474 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Computing topic PABUSI2, partition 1 offsets 148 -> 149
2020-04-08 11:37:12,478 INFO org.apache.spark.streaming.kafka010.KafkaDataConsumer: Initializing cache 16 64 0.75
2020-04-08 11:37:12,481 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-testgroup1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-08 11:37:12,487 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-08 11:37:12,487 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-08 11:37:12,487 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586317032486
2020-04-08 11:37:12,487 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Subscribed to partition(s): PABUSI2-1
2020-04-08 11:37:12,491 INFO org.apache.spark.streaming.kafka010.InternalKafkaConsumer: Initial fetch for spark-executor-testgroup1 PABUSI2-1 148
2020-04-08 11:37:12,491 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Seeking to offset 148 for partition PABUSI2-1
2020-04-08 11:37:13,018 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-08 11:37:13,694 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 0.0 (TID 1). 2489 bytes result sent to driver
2020-04-08 11:37:13,695 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:37:13,695 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 0.0 (TID 2)
2020-04-08 11:37:13,698 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1230 ms on localhost (executor driver) (2/3)
2020-04-08 11:37:13,700 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 0
2020-04-08 11:37:13,701 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 0.0 (TID 2). 712 bytes result sent to driver
2020-04-08 11:37:13,703 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 9 ms on localhost (executor driver) (3/3)
2020-04-08 11:37:13,704 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-04-08 11:37:13,705 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 0 (collect at Kafka2SparkStreaming2Kudu.scala:135) finished in 1.611 s
2020-04-08 11:37:13,708 INFO org.apache.spark.scheduler.DAGScheduler: Job 0 finished: collect at Kafka2SparkStreaming2Kudu.scala:135, took 1.648191 s
2020-04-08 11:37:13,734 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586317030000 ms.0 from job set of time 1586317030000 ms
2020-04-08 11:37:13,734 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 3.733 s for time 1586317030000 ms (execution: 1.715 s)
2020-04-08 11:37:13,738 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 11:37:13,740 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 
2020-04-08 11:37:20,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 11:37:20,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 11:37:20,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 11:37:20,182 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 149.
2020-04-08 11:37:20,183 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 151.
2020-04-08 11:37:20,185 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 150.
2020-04-08 11:37:20,186 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586317040000 ms
2020-04-08 11:37:20,187 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586317040000 ms.0 from job set of time 1586317040000 ms
2020-04-08 11:37:20,193 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:135
2020-04-08 11:37:20,194 INFO org.apache.spark.scheduler.DAGScheduler: Got job 1 (collect at Kafka2SparkStreaming2Kudu.scala:135) with 3 output partitions
2020-04-08 11:37:20,194 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 1 (collect at Kafka2SparkStreaming2Kudu.scala:135)
2020-04-08 11:37:20,194 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 11:37:20,194 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 11:37:20,195 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at map at Kafka2SparkStreaming2Kudu.scala:122), which has no missing parents
2020-04-08 11:37:20,198 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 11:37:20,199 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 11:37:20,200 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.102:61123 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 11:37:20,200 INFO org.apache.spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2020-04-08 11:37:20,201 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at map at Kafka2SparkStreaming2Kudu.scala:122) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 11:37:20,202 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 1.0 with 3 tasks
2020-04-08 11:37:20,203 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:37:20,203 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 1.0 (TID 3)
2020-04-08 11:37:20,206 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 150 is the same as ending offset skipping PABUSI2 2
2020-04-08 11:37:20,207 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 1.0 (TID 3). 712 bytes result sent to driver
2020-04-08 11:37:20,208 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 4, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:37:20,208 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 1.0 (TID 4)
2020-04-08 11:37:20,208 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 3) in 6 ms on localhost (executor driver) (1/3)
2020-04-08 11:37:20,212 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 149 is the same as ending offset skipping PABUSI2 1
2020-04-08 11:37:20,214 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 1.0 (TID 4). 712 bytes result sent to driver
2020-04-08 11:37:20,217 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 1.0 (TID 5, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:37:20,219 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 1.0 (TID 5)
2020-04-08 11:37:20,219 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 4) in 11 ms on localhost (executor driver) (2/3)
2020-04-08 11:37:20,225 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 0
2020-04-08 11:37:20,227 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 1.0 (TID 5). 712 bytes result sent to driver
2020-04-08 11:37:20,228 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 1.0 (TID 5) in 12 ms on localhost (executor driver) (3/3)
2020-04-08 11:37:20,228 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
2020-04-08 11:37:20,228 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 1 (collect at Kafka2SparkStreaming2Kudu.scala:135) finished in 0.032 s
2020-04-08 11:37:20,229 INFO org.apache.spark.scheduler.DAGScheduler: Job 1 finished: collect at Kafka2SparkStreaming2Kudu.scala:135, took 0.035436 s
2020-04-08 11:37:20,235 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586317040000 ms.0 from job set of time 1586317040000 ms
2020-04-08 11:37:20,235 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.235 s for time 1586317040000 ms (execution: 0.048 s)
2020-04-08 11:37:20,236 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 0 from persistence list
2020-04-08 11:37:20,241 INFO org.apache.spark.storage.BlockManager: Removing RDD 0
2020-04-08 11:37:20,241 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 11:37:20,241 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 
2020-04-08 11:37:22,356 INFO org.apache.spark.streaming.StreamingContext: Invoking stop(stopGracefully=false) from shutdown hook
2020-04-08 11:37:22,358 INFO org.apache.spark.streaming.scheduler.ReceiverTracker: ReceiverTracker stopped
2020-04-08 11:37:22,359 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopping JobGenerator immediately
2020-04-08 11:37:22,360 INFO org.apache.spark.streaming.util.RecurringTimer: Stopped timer for JobGenerator after time 1586317040000
2020-04-08 11:37:22,539 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Member consumer-1-6f1a6856-54f0-410d-b8ec-e7c5b7138fa9 sending LeaveGroup request to coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 11:37:22,723 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopped JobGenerator
2020-04-08 11:37:22,726 INFO org.apache.spark.streaming.scheduler.JobScheduler: Stopped JobScheduler
2020-04-08 11:37:22,730 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@33a3c44a{/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 11:37:22,730 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@288214b1{/streaming/batch,null,UNAVAILABLE,@Spark}
2020-04-08 11:37:22,731 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@639aba11{/static/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 11:37:22,732 INFO org.apache.spark.streaming.StreamingContext: StreamingContext stopped successfully
2020-04-08 11:37:22,732 INFO org.apache.spark.SparkContext: Invoking stop() from shutdown hook
2020-04-08 11:37:22,737 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@71e5f61d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 11:37:22,739 INFO org.apache.spark.ui.SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
2020-04-08 11:37:22,746 INFO org.apache.spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2020-04-08 11:37:22,757 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2020-04-08 11:37:22,757 INFO org.apache.spark.storage.BlockManager: BlockManager stopped
2020-04-08 11:37:22,758 INFO org.apache.spark.storage.BlockManagerMaster: BlockManagerMaster stopped
2020-04-08 11:37:22,760 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2020-04-08 11:37:22,768 INFO org.apache.spark.SparkContext: Successfully stopped SparkContext
2020-04-08 11:37:22,769 INFO org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2020-04-08 11:37:22,769 INFO org.apache.spark.util.ShutdownHookManager: Deleting directory /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/spark-7dd2673d-0a43-49bd-a851-48c18eae95ad
2020-04-08 11:38:13,546 WARN org.apache.spark.util.Utils: Your hostname, MacBook-Pro-CW.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)
2020-04-08 11:38:13,547 WARN org.apache.spark.util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
2020-04-08 11:38:13,583 INFO org.apache.spark.SparkContext: Running Spark version 2.4.0.cloudera2
2020-04-08 11:38:13,783 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-04-08 11:38:13,895 INFO org.apache.spark.SparkContext: Submitted application: Kafka2SparkStreaming2Kudu-kerberos
2020-04-08 11:38:13,933 INFO org.apache.spark.SecurityManager: Changing view acls to: godiswc
2020-04-08 11:38:13,934 INFO org.apache.spark.SecurityManager: Changing modify acls to: godiswc
2020-04-08 11:38:13,934 INFO org.apache.spark.SecurityManager: Changing view acls groups to: 
2020-04-08 11:38:13,934 INFO org.apache.spark.SecurityManager: Changing modify acls groups to: 
2020-04-08 11:38:13,935 INFO org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(godiswc); groups with view permissions: Set(); users  with modify permissions: Set(godiswc); groups with modify permissions: Set()
2020-04-08 11:38:14,058 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 11:38:14,129 INFO org.apache.spark.util.Utils: Successfully started service 'sparkDriver' on port 61396.
2020-04-08 11:38:14,146 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
2020-04-08 11:38:14,158 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
2020-04-08 11:38:14,160 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-04-08 11:38:14,161 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2020-04-08 11:38:14,192 INFO org.apache.spark.storage.DiskBlockManager: Created local directory at /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/blockmgr-6661c58a-04cf-41ed-9f44-e8d18d5c83ee
2020-04-08 11:38:14,206 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 4.1 GB
2020-04-08 11:38:14,215 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
2020-04-08 11:38:14,269 INFO org.spark_project.jetty.util.log: Logging initialized @1519ms
2020-04-08 11:38:14,329 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2019-04-30T04:42:08+08:00, git hash: e1bc35120a6617ee3df052294e433f3a25ce7097
2020-04-08 11:38:14,346 INFO org.spark_project.jetty.server.Server: Started @1597ms
2020-04-08 11:38:14,349 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 11:38:14,369 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@71e5f61d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 11:38:14,369 INFO org.apache.spark.util.Utils: Successfully started service 'SparkUI' on port 4040.
2020-04-08 11:38:14,391 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fd62371{/jobs,null,AVAILABLE,@Spark}
2020-04-08 11:38:14,392 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1d9bec4d{/jobs/json,null,AVAILABLE,@Spark}
2020-04-08 11:38:14,393 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5c48c0c0{/jobs/job,null,AVAILABLE,@Spark}
2020-04-08 11:38:14,395 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@25f7391e{/jobs/job/json,null,AVAILABLE,@Spark}
2020-04-08 11:38:14,398 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3f23a3a0{/stages,null,AVAILABLE,@Spark}
2020-04-08 11:38:14,398 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ab14cb9{/stages/json,null,AVAILABLE,@Spark}
2020-04-08 11:38:14,399 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fb97279{/stages/stage,null,AVAILABLE,@Spark}
2020-04-08 11:38:14,400 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@31024624{/stages/stage/json,null,AVAILABLE,@Spark}
2020-04-08 11:38:14,400 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@25bcd0c7{/stages/pool,null,AVAILABLE,@Spark}
2020-04-08 11:38:14,401 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@32cb636e{/stages/pool/json,null,AVAILABLE,@Spark}
2020-04-08 11:38:14,401 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@63cd604c{/storage,null,AVAILABLE,@Spark}
2020-04-08 11:38:14,402 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@40dd3977{/storage/json,null,AVAILABLE,@Spark}
2020-04-08 11:38:14,402 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3a4e343{/storage/rdd,null,AVAILABLE,@Spark}
2020-04-08 11:38:14,403 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a1d204a{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-04-08 11:38:14,403 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62dae245{/environment,null,AVAILABLE,@Spark}
2020-04-08 11:38:14,404 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4b6579e8{/environment/json,null,AVAILABLE,@Spark}
2020-04-08 11:38:14,404 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6fff253c{/executors,null,AVAILABLE,@Spark}
2020-04-08 11:38:14,405 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c6357f9{/executors/json,null,AVAILABLE,@Spark}
2020-04-08 11:38:14,405 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@591e58fa{/executors/threadDump,null,AVAILABLE,@Spark}
2020-04-08 11:38:14,406 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3954d008{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-04-08 11:38:14,411 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f94c4db{/static,null,AVAILABLE,@Spark}
2020-04-08 11:38:14,412 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@669513d8{/,null,AVAILABLE,@Spark}
2020-04-08 11:38:14,413 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3a1d593e{/api,null,AVAILABLE,@Spark}
2020-04-08 11:38:14,413 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@285d851a{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-04-08 11:38:14,414 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@314b8f2d{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-04-08 11:38:14,416 INFO org.apache.spark.ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
2020-04-08 11:38:14,536 INFO org.apache.spark.executor.Executor: Starting executor ID driver on host localhost
2020-04-08 11:38:14,629 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 11:38:14,630 INFO org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 61399.
2020-04-08 11:38:14,631 INFO org.apache.spark.network.netty.NettyBlockTransferService: Server created on 192.168.0.102:61399
2020-04-08 11:38:14,632 INFO org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-04-08 11:38:14,653 INFO org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 61399, None)
2020-04-08 11:38:14,656 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:61399 with 4.1 GB RAM, BlockManagerId(driver, 192.168.0.102, 61399, None)
2020-04-08 11:38:14,657 INFO org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 61399, None)
2020-04-08 11:38:14,658 INFO org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 61399, None)
2020-04-08 11:38:14,868 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@49b07ee3{/metrics/json,null,AVAILABLE,@Spark}
2020-04-08 11:38:14,939 WARN org.apache.spark.streaming.StreamingContext: spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
2020-04-08 11:38:15,178 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding enable.auto.commit to false for executor
2020-04-08 11:38:15,179 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding auto.offset.reset to none for executor
2020-04-08 11:38:15,180 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding executor group.id to spark-executor-testgroup1
2020-04-08 11:38:15,180 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
2020-04-08 11:38:16,930 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Slide time = 10000 ms
2020-04-08 11:38:16,931 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Storage level = Serialized 1x Replicated
2020-04-08 11:38:16,931 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Checkpoint interval = null
2020-04-08 11:38:16,932 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Remember interval = 10000 ms
2020-04-08 11:38:16,932 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@63a62273
2020-04-08 11:38:16,932 INFO org.apache.spark.streaming.dstream.ForEachDStream: Slide time = 10000 ms
2020-04-08 11:38:16,932 INFO org.apache.spark.streaming.dstream.ForEachDStream: Storage level = Serialized 1x Replicated
2020-04-08 11:38:16,932 INFO org.apache.spark.streaming.dstream.ForEachDStream: Checkpoint interval = null
2020-04-08 11:38:16,932 INFO org.apache.spark.streaming.dstream.ForEachDStream: Remember interval = 10000 ms
2020-04-08 11:38:16,933 INFO org.apache.spark.streaming.dstream.ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@21f110ae
2020-04-08 11:38:16,976 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = testgroup1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-08 11:38:17,038 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-08 11:38:17,038 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-08 11:38:17,038 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586317097036
2020-04-08 11:38:17,040 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-1, groupId=testgroup1] Subscribed to topic(s): PABUSI2
2020-04-08 11:38:17,737 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-1, groupId=testgroup1] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-08 11:38:17,738 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Discovered group coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 11:38:17,741 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Revoking previously assigned partitions []
2020-04-08 11:38:17,741 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 11:38:18,290 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 11:38:21,743 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Successfully joined group with generation 17
2020-04-08 11:38:21,747 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting newly assigned partitions: PABUSI2-2, PABUSI2-0, PABUSI2-1
2020-04-08 11:38:21,938 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-2 to the committed offset FetchPosition{offset=150, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-2.vpc.cloudera.com:9092 (id: 52 rack: null), epoch=0}}
2020-04-08 11:38:21,940 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-0 to the committed offset FetchPosition{offset=151, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-4.vpc.cloudera.com:9092 (id: 53 rack: null), epoch=2}}
2020-04-08 11:38:21,940 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-1 to the committed offset FetchPosition{offset=149, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-3.vpc.cloudera.com:9092 (id: 54 rack: null), epoch=2}}
2020-04-08 11:38:21,957 INFO org.apache.spark.streaming.util.RecurringTimer: Started timer for JobGenerator at time 1586317100000
2020-04-08 11:38:21,958 INFO org.apache.spark.streaming.scheduler.JobGenerator: Started JobGenerator at 1586317100000 ms
2020-04-08 11:38:21,959 INFO org.apache.spark.streaming.scheduler.JobScheduler: Started JobScheduler
2020-04-08 11:38:21,964 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@24934262{/streaming,null,AVAILABLE,@Spark}
2020-04-08 11:38:21,966 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@93f432e{/streaming/json,null,AVAILABLE,@Spark}
2020-04-08 11:38:21,969 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5b051a5c{/streaming/batch,null,AVAILABLE,@Spark}
2020-04-08 11:38:21,970 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@639aba11{/streaming/batch/json,null,AVAILABLE,@Spark}
2020-04-08 11:38:21,975 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@58cf8f94{/static/streaming,null,AVAILABLE,@Spark}
2020-04-08 11:38:21,977 INFO org.apache.spark.streaming.StreamingContext: StreamingContext started
2020-04-08 11:38:22,057 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 11:38:22,058 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 11:38:22,059 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 11:38:22,694 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 152.
2020-04-08 11:38:22,984 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 150.
2020-04-08 11:38:22,985 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 149.
2020-04-08 11:38:23,008 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586317100000 ms
2020-04-08 11:38:23,010 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586317100000 ms.0 from job set of time 1586317100000 ms
2020-04-08 11:38:23,103 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:136
2020-04-08 11:38:23,117 INFO org.apache.spark.scheduler.DAGScheduler: Got job 0 (collect at Kafka2SparkStreaming2Kudu.scala:136) with 3 output partitions
2020-04-08 11:38:23,117 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 0 (collect at Kafka2SparkStreaming2Kudu.scala:136)
2020-04-08 11:38:23,118 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 11:38:23,122 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 11:38:23,127 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at Kafka2SparkStreaming2Kudu.scala:122), which has no missing parents
2020-04-08 11:38:23,176 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.1 KB, free 4.1 GB)
2020-04-08 11:38:23,437 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.4 KB, free 4.1 GB)
2020-04-08 11:38:23,438 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:61399 (size: 2.4 KB, free: 4.1 GB)
2020-04-08 11:38:23,440 INFO org.apache.spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2020-04-08 11:38:23,454 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at Kafka2SparkStreaming2Kudu.scala:122) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 11:38:23,454 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 0.0 with 3 tasks
2020-04-08 11:38:23,487 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:38:23,495 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
2020-04-08 11:38:23,521 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 150 is the same as ending offset skipping PABUSI2 2
2020-04-08 11:38:23,532 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 763 bytes result sent to driver
2020-04-08 11:38:23,534 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:38:23,534 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 0.0 (TID 1)
2020-04-08 11:38:23,537 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 57 ms on localhost (executor driver) (1/3)
2020-04-08 11:38:23,538 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 149 is the same as ending offset skipping PABUSI2 1
2020-04-08 11:38:23,539 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 0.0 (TID 1). 720 bytes result sent to driver
2020-04-08 11:38:23,540 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:38:23,540 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 0.0 (TID 2)
2020-04-08 11:38:23,540 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 6 ms on localhost (executor driver) (2/3)
2020-04-08 11:38:23,544 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Computing topic PABUSI2, partition 0 offsets 151 -> 152
2020-04-08 11:38:23,547 INFO org.apache.spark.streaming.kafka010.KafkaDataConsumer: Initializing cache 16 64 0.75
2020-04-08 11:38:23,549 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-testgroup1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-08 11:38:23,567 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-08 11:38:23,567 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-08 11:38:23,567 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586317103567
2020-04-08 11:38:23,567 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Subscribed to partition(s): PABUSI2-0
2020-04-08 11:38:23,570 INFO org.apache.spark.streaming.kafka010.InternalKafkaConsumer: Initial fetch for spark-executor-testgroup1 PABUSI2-0 151
2020-04-08 11:38:23,570 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Seeking to offset 151 for partition PABUSI2-0
2020-04-08 11:38:24,103 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-08 11:38:24,802 ERROR org.apache.spark.executor.Executor: Exception in task 2.0 in stage 0.0 (TID 2)
java.lang.NullPointerException
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:130)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:122)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:363)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1174)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:296)
	at scala.collection.AbstractIterator.to(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:275)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1174)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 11:38:24,818 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 2.0 in stage 0.0 (TID 2, localhost, executor driver): java.lang.NullPointerException
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:130)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:122)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:363)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1174)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:296)
	at scala.collection.AbstractIterator.to(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:275)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1174)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2020-04-08 11:38:24,819 ERROR org.apache.spark.scheduler.TaskSetManager: Task 2 in stage 0.0 failed 1 times; aborting job
2020-04-08 11:38:24,820 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-04-08 11:38:24,822 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Cancelling stage 0
2020-04-08 11:38:24,823 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Killing all running tasks in stage 0: Stage cancelled
2020-04-08 11:38:24,824 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 0 (collect at Kafka2SparkStreaming2Kudu.scala:136) failed in 1.676 s due to Job aborted due to stage failure: Task 2 in stage 0.0 failed 1 times, most recent failure: Lost task 2.0 in stage 0.0 (TID 2, localhost, executor driver): java.lang.NullPointerException
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:130)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:122)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:363)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1174)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:296)
	at scala.collection.AbstractIterator.to(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:275)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1174)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
2020-04-08 11:38:24,828 INFO org.apache.spark.scheduler.DAGScheduler: Job 0 failed: collect at Kafka2SparkStreaming2Kudu.scala:136, took 1.724655 s
2020-04-08 11:38:24,830 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586317100000 ms.0 from job set of time 1586317100000 ms
2020-04-08 11:38:24,831 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1586317100000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 0.0 failed 1 times, most recent failure: Lost task 2.0 in stage 0.0 (TID 2, localhost, executor driver): java.lang.NullPointerException
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:130)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:122)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:363)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1174)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:296)
	at scala.collection.AbstractIterator.to(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:275)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1174)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:245)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:944)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1.apply(Kafka2SparkStreaming2Kudu.scala:120)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:191)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.NullPointerException
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:130)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:122)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:363)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1174)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:296)
	at scala.collection.AbstractIterator.to(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:275)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1174)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	... 3 more
2020-04-08 11:38:24,843 INFO org.apache.spark.streaming.StreamingContext: Invoking stop(stopGracefully=false) from shutdown hook
2020-04-08 11:38:24,844 INFO org.apache.spark.streaming.scheduler.ReceiverTracker: ReceiverTracker stopped
2020-04-08 11:38:24,845 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopping JobGenerator immediately
2020-04-08 11:38:24,845 INFO org.apache.spark.streaming.util.RecurringTimer: Stopped timer for JobGenerator after time 1586317100000
2020-04-08 11:38:25,024 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Member consumer-1-8e087ebb-b9bb-4ef2-ab08-5ebce7c2f5f2 sending LeaveGroup request to coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 11:38:25,208 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopped JobGenerator
2020-04-08 11:38:25,211 INFO org.apache.spark.streaming.scheduler.JobScheduler: Stopped JobScheduler
2020-04-08 11:38:25,215 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@24934262{/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 11:38:25,215 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@5b051a5c{/streaming/batch,null,UNAVAILABLE,@Spark}
2020-04-08 11:38:25,216 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@58cf8f94{/static/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 11:38:25,217 INFO org.apache.spark.streaming.StreamingContext: StreamingContext stopped successfully
2020-04-08 11:38:25,217 INFO org.apache.spark.SparkContext: Invoking stop() from shutdown hook
2020-04-08 11:38:25,222 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@71e5f61d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 11:38:25,223 INFO org.apache.spark.ui.SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
2020-04-08 11:38:25,230 INFO org.apache.spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2020-04-08 11:38:25,242 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2020-04-08 11:38:25,243 INFO org.apache.spark.storage.BlockManager: BlockManager stopped
2020-04-08 11:38:25,246 INFO org.apache.spark.storage.BlockManagerMaster: BlockManagerMaster stopped
2020-04-08 11:38:25,247 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2020-04-08 11:38:25,258 INFO org.apache.spark.SparkContext: Successfully stopped SparkContext
2020-04-08 11:38:25,258 INFO org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2020-04-08 11:38:25,259 INFO org.apache.spark.util.ShutdownHookManager: Deleting directory /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/spark-6045de5d-6a76-4860-9d7d-a18b13663cfe
2020-04-08 11:39:04,043 WARN org.apache.spark.util.Utils: Your hostname, MacBook-Pro-CW.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)
2020-04-08 11:39:04,043 WARN org.apache.spark.util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
2020-04-08 11:39:04,076 INFO org.apache.spark.SparkContext: Running Spark version 2.4.0.cloudera2
2020-04-08 11:39:04,306 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-04-08 11:39:04,436 INFO org.apache.spark.SparkContext: Submitted application: Kafka2SparkStreaming2Kudu-kerberos
2020-04-08 11:39:04,493 INFO org.apache.spark.SecurityManager: Changing view acls to: godiswc
2020-04-08 11:39:04,493 INFO org.apache.spark.SecurityManager: Changing modify acls to: godiswc
2020-04-08 11:39:04,494 INFO org.apache.spark.SecurityManager: Changing view acls groups to: 
2020-04-08 11:39:04,494 INFO org.apache.spark.SecurityManager: Changing modify acls groups to: 
2020-04-08 11:39:04,494 INFO org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(godiswc); groups with view permissions: Set(); users  with modify permissions: Set(godiswc); groups with modify permissions: Set()
2020-04-08 11:39:04,631 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 11:39:04,710 INFO org.apache.spark.util.Utils: Successfully started service 'sparkDriver' on port 61596.
2020-04-08 11:39:04,726 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
2020-04-08 11:39:04,736 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
2020-04-08 11:39:04,738 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-04-08 11:39:04,738 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2020-04-08 11:39:04,768 INFO org.apache.spark.storage.DiskBlockManager: Created local directory at /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/blockmgr-05aabfe3-8ec0-4e57-862d-46eb65b8dc77
2020-04-08 11:39:04,782 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 4.1 GB
2020-04-08 11:39:04,791 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
2020-04-08 11:39:04,840 INFO org.spark_project.jetty.util.log: Logging initialized @1450ms
2020-04-08 11:39:04,906 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2019-04-30T04:42:08+08:00, git hash: e1bc35120a6617ee3df052294e433f3a25ce7097
2020-04-08 11:39:04,937 INFO org.spark_project.jetty.server.Server: Started @1548ms
2020-04-08 11:39:04,941 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 11:39:04,957 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@71e5f61d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 11:39:04,958 INFO org.apache.spark.util.Utils: Successfully started service 'SparkUI' on port 4040.
2020-04-08 11:39:04,979 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fd62371{/jobs,null,AVAILABLE,@Spark}
2020-04-08 11:39:04,979 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1d9bec4d{/jobs/json,null,AVAILABLE,@Spark}
2020-04-08 11:39:04,980 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5c48c0c0{/jobs/job,null,AVAILABLE,@Spark}
2020-04-08 11:39:04,984 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@25f7391e{/jobs/job/json,null,AVAILABLE,@Spark}
2020-04-08 11:39:04,987 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3f23a3a0{/stages,null,AVAILABLE,@Spark}
2020-04-08 11:39:04,987 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ab14cb9{/stages/json,null,AVAILABLE,@Spark}
2020-04-08 11:39:04,988 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fb97279{/stages/stage,null,AVAILABLE,@Spark}
2020-04-08 11:39:04,990 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@31024624{/stages/stage/json,null,AVAILABLE,@Spark}
2020-04-08 11:39:04,991 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@25bcd0c7{/stages/pool,null,AVAILABLE,@Spark}
2020-04-08 11:39:04,991 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@32cb636e{/stages/pool/json,null,AVAILABLE,@Spark}
2020-04-08 11:39:04,992 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@63cd604c{/storage,null,AVAILABLE,@Spark}
2020-04-08 11:39:04,993 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@40dd3977{/storage/json,null,AVAILABLE,@Spark}
2020-04-08 11:39:04,994 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3a4e343{/storage/rdd,null,AVAILABLE,@Spark}
2020-04-08 11:39:04,995 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a1d204a{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-04-08 11:39:04,995 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62dae245{/environment,null,AVAILABLE,@Spark}
2020-04-08 11:39:04,996 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4b6579e8{/environment/json,null,AVAILABLE,@Spark}
2020-04-08 11:39:04,997 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6fff253c{/executors,null,AVAILABLE,@Spark}
2020-04-08 11:39:04,998 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c6357f9{/executors/json,null,AVAILABLE,@Spark}
2020-04-08 11:39:04,998 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@591e58fa{/executors/threadDump,null,AVAILABLE,@Spark}
2020-04-08 11:39:04,999 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3954d008{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-04-08 11:39:05,004 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f94c4db{/static,null,AVAILABLE,@Spark}
2020-04-08 11:39:05,005 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@669513d8{/,null,AVAILABLE,@Spark}
2020-04-08 11:39:05,006 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3a1d593e{/api,null,AVAILABLE,@Spark}
2020-04-08 11:39:05,007 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@285d851a{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-04-08 11:39:05,007 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@314b8f2d{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-04-08 11:39:05,009 INFO org.apache.spark.ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
2020-04-08 11:39:05,135 INFO org.apache.spark.executor.Executor: Starting executor ID driver on host localhost
2020-04-08 11:39:05,205 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 11:39:05,206 INFO org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 61598.
2020-04-08 11:39:05,206 INFO org.apache.spark.network.netty.NettyBlockTransferService: Server created on 192.168.0.102:61598
2020-04-08 11:39:05,207 INFO org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-04-08 11:39:05,228 INFO org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 61598, None)
2020-04-08 11:39:05,230 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:61598 with 4.1 GB RAM, BlockManagerId(driver, 192.168.0.102, 61598, None)
2020-04-08 11:39:05,232 INFO org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 61598, None)
2020-04-08 11:39:05,232 INFO org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 61598, None)
2020-04-08 11:39:05,381 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@49b07ee3{/metrics/json,null,AVAILABLE,@Spark}
2020-04-08 11:39:05,470 WARN org.apache.spark.streaming.StreamingContext: spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
2020-04-08 11:39:05,662 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding enable.auto.commit to false for executor
2020-04-08 11:39:05,663 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding auto.offset.reset to none for executor
2020-04-08 11:39:05,663 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding executor group.id to spark-executor-testgroup1
2020-04-08 11:39:05,664 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
2020-04-08 11:39:07,357 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Slide time = 10000 ms
2020-04-08 11:39:07,357 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Storage level = Serialized 1x Replicated
2020-04-08 11:39:07,357 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Checkpoint interval = null
2020-04-08 11:39:07,358 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Remember interval = 10000 ms
2020-04-08 11:39:07,358 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@a8c9435
2020-04-08 11:39:07,358 INFO org.apache.spark.streaming.dstream.ForEachDStream: Slide time = 10000 ms
2020-04-08 11:39:07,358 INFO org.apache.spark.streaming.dstream.ForEachDStream: Storage level = Serialized 1x Replicated
2020-04-08 11:39:07,358 INFO org.apache.spark.streaming.dstream.ForEachDStream: Checkpoint interval = null
2020-04-08 11:39:07,358 INFO org.apache.spark.streaming.dstream.ForEachDStream: Remember interval = 10000 ms
2020-04-08 11:39:07,359 INFO org.apache.spark.streaming.dstream.ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@dd71c78
2020-04-08 11:39:07,407 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = testgroup1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-08 11:39:07,457 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-08 11:39:07,457 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-08 11:39:07,457 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586317147456
2020-04-08 11:39:07,459 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-1, groupId=testgroup1] Subscribed to topic(s): PABUSI2
2020-04-08 11:39:08,343 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-1, groupId=testgroup1] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-08 11:39:08,344 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Discovered group coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 11:39:08,347 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Revoking previously assigned partitions []
2020-04-08 11:39:08,347 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 11:39:09,015 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 11:39:12,400 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Successfully joined group with generation 19
2020-04-08 11:39:12,406 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting newly assigned partitions: PABUSI2-2, PABUSI2-0, PABUSI2-1
2020-04-08 11:39:12,594 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-2 to the committed offset FetchPosition{offset=150, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-2.vpc.cloudera.com:9092 (id: 52 rack: null), epoch=0}}
2020-04-08 11:39:12,596 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-0 to the committed offset FetchPosition{offset=152, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-4.vpc.cloudera.com:9092 (id: 53 rack: null), epoch=2}}
2020-04-08 11:39:12,597 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-1 to the committed offset FetchPosition{offset=149, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-3.vpc.cloudera.com:9092 (id: 54 rack: null), epoch=2}}
2020-04-08 11:39:12,609 INFO org.apache.spark.streaming.util.RecurringTimer: Started timer for JobGenerator at time 1586317150000
2020-04-08 11:39:12,610 INFO org.apache.spark.streaming.scheduler.JobGenerator: Started JobGenerator at 1586317150000 ms
2020-04-08 11:39:12,612 INFO org.apache.spark.streaming.scheduler.JobScheduler: Started JobScheduler
2020-04-08 11:39:12,619 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@24934262{/streaming,null,AVAILABLE,@Spark}
2020-04-08 11:39:12,622 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@93f432e{/streaming/json,null,AVAILABLE,@Spark}
2020-04-08 11:39:12,623 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5b051a5c{/streaming/batch,null,AVAILABLE,@Spark}
2020-04-08 11:39:12,624 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@639aba11{/streaming/batch/json,null,AVAILABLE,@Spark}
2020-04-08 11:39:12,625 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@58cf8f94{/static/streaming,null,AVAILABLE,@Spark}
2020-04-08 11:39:12,626 INFO org.apache.spark.streaming.StreamingContext: StreamingContext started
2020-04-08 11:39:12,680 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 11:39:12,681 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 11:39:12,681 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 11:39:13,667 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 152.
2020-04-08 11:39:13,669 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 149.
2020-04-08 11:39:14,674 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 150.
2020-04-08 11:39:14,698 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586317150000 ms
2020-04-08 11:39:14,701 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586317150000 ms.0 from job set of time 1586317150000 ms
2020-04-08 11:39:14,763 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:136
2020-04-08 11:39:14,776 INFO org.apache.spark.scheduler.DAGScheduler: Got job 0 (collect at Kafka2SparkStreaming2Kudu.scala:136) with 3 output partitions
2020-04-08 11:39:14,776 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 0 (collect at Kafka2SparkStreaming2Kudu.scala:136)
2020-04-08 11:39:14,777 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 11:39:14,777 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 11:39:14,781 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at Kafka2SparkStreaming2Kudu.scala:122), which has no missing parents
2020-04-08 11:39:14,824 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 3.8 KB, free 4.1 GB)
2020-04-08 11:39:15,151 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.4 KB, free 4.1 GB)
2020-04-08 11:39:15,153 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:61598 (size: 2.4 KB, free: 4.1 GB)
2020-04-08 11:39:15,157 INFO org.apache.spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2020-04-08 11:39:15,170 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at Kafka2SparkStreaming2Kudu.scala:122) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 11:39:15,171 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 0.0 with 3 tasks
2020-04-08 11:39:15,203 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:39:15,210 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
2020-04-08 11:39:15,236 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 150 is the same as ending offset skipping PABUSI2 2
2020-04-08 11:39:15,246 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 712 bytes result sent to driver
2020-04-08 11:39:15,248 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:39:15,248 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 0.0 (TID 1)
2020-04-08 11:39:15,251 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 56 ms on localhost (executor driver) (1/3)
2020-04-08 11:39:15,252 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 149 is the same as ending offset skipping PABUSI2 1
2020-04-08 11:39:15,253 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 0.0 (TID 1). 712 bytes result sent to driver
2020-04-08 11:39:15,254 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:39:15,254 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 6 ms on localhost (executor driver) (2/3)
2020-04-08 11:39:15,254 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 0.0 (TID 2)
2020-04-08 11:39:15,258 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 152 is the same as ending offset skipping PABUSI2 0
2020-04-08 11:39:15,259 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 0.0 (TID 2). 712 bytes result sent to driver
2020-04-08 11:39:15,262 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 8 ms on localhost (executor driver) (3/3)
2020-04-08 11:39:15,263 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-04-08 11:39:15,264 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 0 (collect at Kafka2SparkStreaming2Kudu.scala:136) finished in 0.467 s
2020-04-08 11:39:15,268 INFO org.apache.spark.scheduler.DAGScheduler: Job 0 finished: collect at Kafka2SparkStreaming2Kudu.scala:136, took 0.505074 s
2020-04-08 11:39:15,286 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586317150000 ms.0 from job set of time 1586317150000 ms
2020-04-08 11:39:15,287 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 5.285 s for time 1586317150000 ms (execution: 0.585 s)
2020-04-08 11:39:15,305 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 11:39:15,305 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 5
2020-04-08 11:39:15,305 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 22
2020-04-08 11:39:15,305 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 11
2020-04-08 11:39:15,306 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 18
2020-04-08 11:39:15,308 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 
2020-04-08 11:39:15,315 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.102:61598 in memory (size: 2.4 KB, free: 4.1 GB)
2020-04-08 11:39:15,317 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 13
2020-04-08 11:39:15,317 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 16
2020-04-08 11:39:15,317 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 2
2020-04-08 11:39:15,318 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 12
2020-04-08 11:39:15,318 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 10
2020-04-08 11:39:15,318 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 9
2020-04-08 11:39:15,318 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 25
2020-04-08 11:39:15,318 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 1
2020-04-08 11:39:15,318 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 15
2020-04-08 11:39:15,318 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 23
2020-04-08 11:39:15,318 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 14
2020-04-08 11:39:15,318 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 8
2020-04-08 11:39:15,318 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 21
2020-04-08 11:39:15,318 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 17
2020-04-08 11:39:15,318 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 20
2020-04-08 11:39:15,318 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 7
2020-04-08 11:39:15,319 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 24
2020-04-08 11:39:15,319 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 6
2020-04-08 11:39:15,319 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 19
2020-04-08 11:39:15,319 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 3
2020-04-08 11:39:15,319 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 4
2020-04-08 11:39:20,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 11:39:20,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 11:39:20,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 11:39:20,188 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 150.
2020-04-08 11:39:20,189 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 11:39:20,189 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 149.
2020-04-08 11:39:20,191 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586317160000 ms
2020-04-08 11:39:20,191 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586317160000 ms.0 from job set of time 1586317160000 ms
2020-04-08 11:39:20,201 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:136
2020-04-08 11:39:20,202 INFO org.apache.spark.scheduler.DAGScheduler: Got job 1 (collect at Kafka2SparkStreaming2Kudu.scala:136) with 3 output partitions
2020-04-08 11:39:20,202 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 1 (collect at Kafka2SparkStreaming2Kudu.scala:136)
2020-04-08 11:39:20,202 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 11:39:20,202 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 11:39:20,203 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at map at Kafka2SparkStreaming2Kudu.scala:122), which has no missing parents
2020-04-08 11:39:20,206 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 3.8 KB, free 4.1 GB)
2020-04-08 11:39:20,207 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.4 KB, free 4.1 GB)
2020-04-08 11:39:20,208 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.102:61598 (size: 2.4 KB, free: 4.1 GB)
2020-04-08 11:39:20,208 INFO org.apache.spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2020-04-08 11:39:20,209 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at map at Kafka2SparkStreaming2Kudu.scala:122) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 11:39:20,209 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 1.0 with 3 tasks
2020-04-08 11:39:20,210 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:39:20,210 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 1.0 (TID 3)
2020-04-08 11:39:20,213 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 150 is the same as ending offset skipping PABUSI2 2
2020-04-08 11:39:20,214 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 1.0 (TID 3). 712 bytes result sent to driver
2020-04-08 11:39:20,216 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 4, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:39:20,216 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 1.0 (TID 4)
2020-04-08 11:39:20,216 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 3) in 6 ms on localhost (executor driver) (1/3)
2020-04-08 11:39:20,219 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 149 is the same as ending offset skipping PABUSI2 1
2020-04-08 11:39:20,221 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 1.0 (TID 4). 712 bytes result sent to driver
2020-04-08 11:39:20,222 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 1.0 (TID 5, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:39:20,223 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 1.0 (TID 5)
2020-04-08 11:39:20,223 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 4) in 7 ms on localhost (executor driver) (2/3)
2020-04-08 11:39:20,226 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Computing topic PABUSI2, partition 0 offsets 152 -> 153
2020-04-08 11:39:20,230 INFO org.apache.spark.streaming.kafka010.KafkaDataConsumer: Initializing cache 16 64 0.75
2020-04-08 11:39:20,233 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-testgroup1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-08 11:39:20,238 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-08 11:39:20,238 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-08 11:39:20,239 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586317160238
2020-04-08 11:39:20,239 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Subscribed to partition(s): PABUSI2-0
2020-04-08 11:39:20,242 INFO org.apache.spark.streaming.kafka010.InternalKafkaConsumer: Initial fetch for spark-executor-testgroup1 PABUSI2-0 152
2020-04-08 11:39:20,243 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Seeking to offset 152 for partition PABUSI2-0
2020-04-08 11:39:20,771 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-08 11:39:21,469 ERROR org.apache.spark.executor.Executor: Exception in task 2.0 in stage 1.0 (TID 5)
java.lang.NullPointerException
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:130)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:122)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:363)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1174)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:296)
	at scala.collection.AbstractIterator.to(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:275)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1174)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 11:39:21,483 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 2.0 in stage 1.0 (TID 5, localhost, executor driver): java.lang.NullPointerException
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:130)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:122)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:363)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1174)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:296)
	at scala.collection.AbstractIterator.to(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:275)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1174)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2020-04-08 11:39:21,484 ERROR org.apache.spark.scheduler.TaskSetManager: Task 2 in stage 1.0 failed 1 times; aborting job
2020-04-08 11:39:21,485 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
2020-04-08 11:39:21,486 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Cancelling stage 1
2020-04-08 11:39:21,487 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Killing all running tasks in stage 1: Stage cancelled
2020-04-08 11:39:21,488 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 1 (collect at Kafka2SparkStreaming2Kudu.scala:136) failed in 1.283 s due to Job aborted due to stage failure: Task 2 in stage 1.0 failed 1 times, most recent failure: Lost task 2.0 in stage 1.0 (TID 5, localhost, executor driver): java.lang.NullPointerException
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:130)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:122)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:363)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1174)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:296)
	at scala.collection.AbstractIterator.to(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:275)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1174)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
2020-04-08 11:39:21,489 INFO org.apache.spark.scheduler.DAGScheduler: Job 1 failed: collect at Kafka2SparkStreaming2Kudu.scala:136, took 1.287122 s
2020-04-08 11:39:21,489 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586317160000 ms.0 from job set of time 1586317160000 ms
2020-04-08 11:39:21,491 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1586317160000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 1.0 failed 1 times, most recent failure: Lost task 2.0 in stage 1.0 (TID 5, localhost, executor driver): java.lang.NullPointerException
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:130)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:122)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:363)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1174)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:296)
	at scala.collection.AbstractIterator.to(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:275)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1174)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:245)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:944)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1.apply(Kafka2SparkStreaming2Kudu.scala:120)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:191)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.NullPointerException
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:130)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:122)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:363)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1174)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:296)
	at scala.collection.AbstractIterator.to(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:275)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1174)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	... 3 more
2020-04-08 11:39:21,497 INFO org.apache.spark.streaming.StreamingContext: Invoking stop(stopGracefully=false) from shutdown hook
2020-04-08 11:39:21,499 INFO org.apache.spark.streaming.scheduler.ReceiverTracker: ReceiverTracker stopped
2020-04-08 11:39:21,500 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopping JobGenerator immediately
2020-04-08 11:39:21,500 INFO org.apache.spark.streaming.util.RecurringTimer: Stopped timer for JobGenerator after time 1586317160000
2020-04-08 11:39:21,682 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Member consumer-1-8bec810f-b847-4e8c-b6fa-20106650b7ab sending LeaveGroup request to coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 11:39:21,869 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopped JobGenerator
2020-04-08 11:39:21,872 INFO org.apache.spark.streaming.scheduler.JobScheduler: Stopped JobScheduler
2020-04-08 11:39:21,876 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@24934262{/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 11:39:21,876 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@5b051a5c{/streaming/batch,null,UNAVAILABLE,@Spark}
2020-04-08 11:39:21,877 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@58cf8f94{/static/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 11:39:21,878 INFO org.apache.spark.streaming.StreamingContext: StreamingContext stopped successfully
2020-04-08 11:39:21,878 INFO org.apache.spark.SparkContext: Invoking stop() from shutdown hook
2020-04-08 11:39:21,883 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@71e5f61d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 11:39:21,885 INFO org.apache.spark.ui.SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
2020-04-08 11:39:21,893 INFO org.apache.spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2020-04-08 11:39:21,904 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2020-04-08 11:39:21,905 INFO org.apache.spark.storage.BlockManager: BlockManager stopped
2020-04-08 11:39:21,905 INFO org.apache.spark.storage.BlockManagerMaster: BlockManagerMaster stopped
2020-04-08 11:39:21,907 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2020-04-08 11:39:21,917 INFO org.apache.spark.SparkContext: Successfully stopped SparkContext
2020-04-08 11:39:21,918 INFO org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2020-04-08 11:39:21,918 INFO org.apache.spark.util.ShutdownHookManager: Deleting directory /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/spark-9097cfef-0853-48ea-ac23-913d12ac75ba
2020-04-08 11:41:16,824 WARN org.apache.spark.util.Utils: Your hostname, MacBook-Pro-CW.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)
2020-04-08 11:41:16,825 WARN org.apache.spark.util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
2020-04-08 11:41:16,861 INFO org.apache.spark.SparkContext: Running Spark version 2.4.0.cloudera2
2020-04-08 11:41:17,095 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-04-08 11:41:17,224 INFO org.apache.spark.SparkContext: Submitted application: Kafka2SparkStreaming2Kudu-kerberos
2020-04-08 11:41:17,267 INFO org.apache.spark.SecurityManager: Changing view acls to: godiswc
2020-04-08 11:41:17,267 INFO org.apache.spark.SecurityManager: Changing modify acls to: godiswc
2020-04-08 11:41:17,267 INFO org.apache.spark.SecurityManager: Changing view acls groups to: 
2020-04-08 11:41:17,268 INFO org.apache.spark.SecurityManager: Changing modify acls groups to: 
2020-04-08 11:41:17,268 INFO org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(godiswc); groups with view permissions: Set(); users  with modify permissions: Set(godiswc); groups with modify permissions: Set()
2020-04-08 11:41:17,513 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 11:41:17,601 INFO org.apache.spark.util.Utils: Successfully started service 'sparkDriver' on port 62112.
2020-04-08 11:41:17,620 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
2020-04-08 11:41:17,633 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
2020-04-08 11:41:17,635 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-04-08 11:41:17,635 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2020-04-08 11:41:17,669 INFO org.apache.spark.storage.DiskBlockManager: Created local directory at /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/blockmgr-cbc78a80-2669-4a23-bf14-0dc97a0a34ed
2020-04-08 11:41:17,687 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 4.1 GB
2020-04-08 11:41:17,703 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
2020-04-08 11:41:17,831 INFO org.spark_project.jetty.util.log: Logging initialized @1748ms
2020-04-08 11:41:17,904 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2019-04-30T04:42:08+08:00, git hash: e1bc35120a6617ee3df052294e433f3a25ce7097
2020-04-08 11:41:17,917 INFO org.spark_project.jetty.server.Server: Started @1836ms
2020-04-08 11:41:17,919 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 11:41:17,930 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@71e5f61d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 11:41:17,930 INFO org.apache.spark.util.Utils: Successfully started service 'SparkUI' on port 4040.
2020-04-08 11:41:17,952 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fd62371{/jobs,null,AVAILABLE,@Spark}
2020-04-08 11:41:17,953 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1d9bec4d{/jobs/json,null,AVAILABLE,@Spark}
2020-04-08 11:41:17,954 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5c48c0c0{/jobs/job,null,AVAILABLE,@Spark}
2020-04-08 11:41:17,957 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@25f7391e{/jobs/job/json,null,AVAILABLE,@Spark}
2020-04-08 11:41:17,959 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3f23a3a0{/stages,null,AVAILABLE,@Spark}
2020-04-08 11:41:17,960 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ab14cb9{/stages/json,null,AVAILABLE,@Spark}
2020-04-08 11:41:17,960 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fb97279{/stages/stage,null,AVAILABLE,@Spark}
2020-04-08 11:41:17,961 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@31024624{/stages/stage/json,null,AVAILABLE,@Spark}
2020-04-08 11:41:17,962 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@25bcd0c7{/stages/pool,null,AVAILABLE,@Spark}
2020-04-08 11:41:17,962 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@32cb636e{/stages/pool/json,null,AVAILABLE,@Spark}
2020-04-08 11:41:17,963 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@63cd604c{/storage,null,AVAILABLE,@Spark}
2020-04-08 11:41:17,963 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@40dd3977{/storage/json,null,AVAILABLE,@Spark}
2020-04-08 11:41:17,964 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3a4e343{/storage/rdd,null,AVAILABLE,@Spark}
2020-04-08 11:41:17,964 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a1d204a{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-04-08 11:41:17,965 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62dae245{/environment,null,AVAILABLE,@Spark}
2020-04-08 11:41:17,965 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4b6579e8{/environment/json,null,AVAILABLE,@Spark}
2020-04-08 11:41:17,966 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6fff253c{/executors,null,AVAILABLE,@Spark}
2020-04-08 11:41:17,966 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c6357f9{/executors/json,null,AVAILABLE,@Spark}
2020-04-08 11:41:17,967 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@591e58fa{/executors/threadDump,null,AVAILABLE,@Spark}
2020-04-08 11:41:17,967 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3954d008{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-04-08 11:41:17,972 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f94c4db{/static,null,AVAILABLE,@Spark}
2020-04-08 11:41:17,973 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@669513d8{/,null,AVAILABLE,@Spark}
2020-04-08 11:41:17,974 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3a1d593e{/api,null,AVAILABLE,@Spark}
2020-04-08 11:41:17,974 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@285d851a{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-04-08 11:41:17,975 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@314b8f2d{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-04-08 11:41:17,976 INFO org.apache.spark.ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
2020-04-08 11:41:18,093 INFO org.apache.spark.executor.Executor: Starting executor ID driver on host localhost
2020-04-08 11:41:18,182 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 11:41:18,183 INFO org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 62115.
2020-04-08 11:41:18,183 INFO org.apache.spark.network.netty.NettyBlockTransferService: Server created on 192.168.0.102:62115
2020-04-08 11:41:18,184 INFO org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-04-08 11:41:18,206 INFO org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 62115, None)
2020-04-08 11:41:18,208 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:62115 with 4.1 GB RAM, BlockManagerId(driver, 192.168.0.102, 62115, None)
2020-04-08 11:41:18,210 INFO org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 62115, None)
2020-04-08 11:41:18,210 INFO org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 62115, None)
2020-04-08 11:41:18,422 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@49b07ee3{/metrics/json,null,AVAILABLE,@Spark}
2020-04-08 11:41:18,507 WARN org.apache.spark.streaming.StreamingContext: spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
2020-04-08 11:41:18,748 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding enable.auto.commit to false for executor
2020-04-08 11:41:18,749 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding auto.offset.reset to none for executor
2020-04-08 11:41:18,749 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding executor group.id to spark-executor-testgroup1
2020-04-08 11:41:18,750 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
2020-04-08 11:41:19,693 WARN org.apache.kudu.util.NetUtil: Slow DNS lookup! Resolved IP of `cw-instances-2.vpc.cloudera.com' to cw-instances-2.vpc.cloudera.com/10.65.1.47 in 538691904ns
2020-04-08 11:41:20,302 WARN org.apache.kudu.util.NetUtil: Slow DNS lookup! Resolved IP of `cw-instances-3.vpc.cloudera.com' to cw-instances-3.vpc.cloudera.com/10.65.9.108 in 524958726ns
2020-04-08 11:41:20,668 WARN org.apache.kudu.util.NetUtil: Slow DNS lookup! Resolved IP of `cw-instances-4.vpc.cloudera.com' to cw-instances-4.vpc.cloudera.com/10.65.12.223 in 364111288ns
2020-04-08 11:41:23,620 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Slide time = 10000 ms
2020-04-08 11:41:23,620 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Storage level = Serialized 1x Replicated
2020-04-08 11:41:23,621 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Checkpoint interval = null
2020-04-08 11:41:23,621 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Remember interval = 10000 ms
2020-04-08 11:41:23,621 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@5e9d026a
2020-04-08 11:41:23,621 INFO org.apache.spark.streaming.dstream.ForEachDStream: Slide time = 10000 ms
2020-04-08 11:41:23,622 INFO org.apache.spark.streaming.dstream.ForEachDStream: Storage level = Serialized 1x Replicated
2020-04-08 11:41:23,622 INFO org.apache.spark.streaming.dstream.ForEachDStream: Checkpoint interval = null
2020-04-08 11:41:23,622 INFO org.apache.spark.streaming.dstream.ForEachDStream: Remember interval = 10000 ms
2020-04-08 11:41:23,622 INFO org.apache.spark.streaming.dstream.ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@42c80389
2020-04-08 11:41:23,673 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = testgroup1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-08 11:41:23,740 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-08 11:41:23,740 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-08 11:41:23,740 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586317283738
2020-04-08 11:41:23,742 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-1, groupId=testgroup1] Subscribed to topic(s): PABUSI2
2020-04-08 11:41:23,859 INFO org.apache.kudu.client.ConnectToCluster: Unable to connect to master cw-instances-3.vpc.cloudera.com:7051: connection disconnected
2020-04-08 11:41:24,463 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-1, groupId=testgroup1] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-08 11:41:24,464 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Discovered group coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 11:41:24,467 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Revoking previously assigned partitions []
2020-04-08 11:41:24,468 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 11:41:25,017 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 11:41:28,414 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Successfully joined group with generation 21
2020-04-08 11:41:28,419 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting newly assigned partitions: PABUSI2-2, PABUSI2-0, PABUSI2-1
2020-04-08 11:41:28,615 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-2 to the committed offset FetchPosition{offset=150, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-2.vpc.cloudera.com:9092 (id: 52 rack: null), epoch=0}}
2020-04-08 11:41:28,617 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-0 to the committed offset FetchPosition{offset=153, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-4.vpc.cloudera.com:9092 (id: 53 rack: null), epoch=2}}
2020-04-08 11:41:28,617 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-1 to the committed offset FetchPosition{offset=149, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-3.vpc.cloudera.com:9092 (id: 54 rack: null), epoch=2}}
2020-04-08 11:41:28,627 INFO org.apache.spark.streaming.util.RecurringTimer: Started timer for JobGenerator at time 1586317290000
2020-04-08 11:41:28,627 INFO org.apache.spark.streaming.scheduler.JobGenerator: Started JobGenerator at 1586317290000 ms
2020-04-08 11:41:28,628 INFO org.apache.spark.streaming.scheduler.JobScheduler: Started JobScheduler
2020-04-08 11:41:28,632 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@24934262{/streaming,null,AVAILABLE,@Spark}
2020-04-08 11:41:28,633 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@93f432e{/streaming/json,null,AVAILABLE,@Spark}
2020-04-08 11:41:28,634 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5b051a5c{/streaming/batch,null,AVAILABLE,@Spark}
2020-04-08 11:41:28,634 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@639aba11{/streaming/batch/json,null,AVAILABLE,@Spark}
2020-04-08 11:41:28,635 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@58cf8f94{/static/streaming,null,AVAILABLE,@Spark}
2020-04-08 11:41:28,635 INFO org.apache.spark.streaming.StreamingContext: StreamingContext started
2020-04-08 11:41:30,054 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 11:41:30,055 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 11:41:30,055 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 11:41:30,298 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 150.
2020-04-08 11:41:30,298 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 149.
2020-04-08 11:41:30,298 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 11:41:30,322 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586317290000 ms
2020-04-08 11:41:30,324 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586317290000 ms.0 from job set of time 1586317290000 ms
2020-04-08 11:41:30,383 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:137
2020-04-08 11:41:30,393 INFO org.apache.spark.scheduler.DAGScheduler: Got job 0 (collect at Kafka2SparkStreaming2Kudu.scala:137) with 3 output partitions
2020-04-08 11:41:30,394 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 0 (collect at Kafka2SparkStreaming2Kudu.scala:137)
2020-04-08 11:41:30,394 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 11:41:30,395 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 11:41:30,398 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at Kafka2SparkStreaming2Kudu.scala:122), which has no missing parents
2020-04-08 11:41:30,433 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 3.8 KB, free 4.1 GB)
2020-04-08 11:41:30,678 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.4 KB, free 4.1 GB)
2020-04-08 11:41:30,679 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:62115 (size: 2.4 KB, free: 4.1 GB)
2020-04-08 11:41:30,681 INFO org.apache.spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2020-04-08 11:41:30,694 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at Kafka2SparkStreaming2Kudu.scala:122) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 11:41:30,694 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 0.0 with 3 tasks
2020-04-08 11:41:30,725 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:41:30,732 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
2020-04-08 11:41:30,751 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 150 is the same as ending offset skipping PABUSI2 2
2020-04-08 11:41:30,760 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 712 bytes result sent to driver
2020-04-08 11:41:30,762 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:41:30,762 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 0.0 (TID 1)
2020-04-08 11:41:30,765 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 47 ms on localhost (executor driver) (1/3)
2020-04-08 11:41:30,766 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 149 is the same as ending offset skipping PABUSI2 1
2020-04-08 11:41:30,767 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 0.0 (TID 1). 712 bytes result sent to driver
2020-04-08 11:41:30,767 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:41:30,768 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 0.0 (TID 2)
2020-04-08 11:41:30,768 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 6 ms on localhost (executor driver) (2/3)
2020-04-08 11:41:30,770 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 11:41:30,771 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 0.0 (TID 2). 712 bytes result sent to driver
2020-04-08 11:41:30,774 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 7 ms on localhost (executor driver) (3/3)
2020-04-08 11:41:30,775 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-04-08 11:41:30,776 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 0 (collect at Kafka2SparkStreaming2Kudu.scala:137) finished in 0.364 s
2020-04-08 11:41:30,780 INFO org.apache.spark.scheduler.DAGScheduler: Job 0 finished: collect at Kafka2SparkStreaming2Kudu.scala:137, took 0.396885 s
2020-04-08 11:41:30,793 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586317290000 ms.0 from job set of time 1586317290000 ms
2020-04-08 11:41:30,794 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.793 s for time 1586317290000 ms (execution: 0.470 s)
2020-04-08 11:41:30,798 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 11:41:30,801 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 
2020-04-08 11:41:40,008 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 11:41:40,008 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 11:41:40,008 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 11:41:40,478 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 150.
2020-04-08 11:41:40,479 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 11:41:40,479 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 149.
2020-04-08 11:41:40,480 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586317300000 ms
2020-04-08 11:41:40,480 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586317300000 ms.0 from job set of time 1586317300000 ms
2020-04-08 11:41:40,492 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:137
2020-04-08 11:41:40,493 INFO org.apache.spark.scheduler.DAGScheduler: Got job 1 (collect at Kafka2SparkStreaming2Kudu.scala:137) with 3 output partitions
2020-04-08 11:41:40,493 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 1 (collect at Kafka2SparkStreaming2Kudu.scala:137)
2020-04-08 11:41:40,493 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 11:41:40,493 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 11:41:40,494 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at map at Kafka2SparkStreaming2Kudu.scala:122), which has no missing parents
2020-04-08 11:41:40,496 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 3.8 KB, free 4.1 GB)
2020-04-08 11:41:40,511 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.4 KB, free 4.1 GB)
2020-04-08 11:41:40,511 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.102:62115 (size: 2.4 KB, free: 4.1 GB)
2020-04-08 11:41:40,512 INFO org.apache.spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2020-04-08 11:41:40,513 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at map at Kafka2SparkStreaming2Kudu.scala:122) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 11:41:40,513 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 1.0 with 3 tasks
2020-04-08 11:41:40,514 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 6
2020-04-08 11:41:40,514 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 11
2020-04-08 11:41:40,514 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 14
2020-04-08 11:41:40,514 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 10
2020-04-08 11:41:40,514 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 19
2020-04-08 11:41:40,514 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 23
2020-04-08 11:41:40,514 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 7
2020-04-08 11:41:40,514 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 25
2020-04-08 11:41:40,514 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 9
2020-04-08 11:41:40,514 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 15
2020-04-08 11:41:40,514 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 12
2020-04-08 11:41:40,514 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:41:40,514 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 21
2020-04-08 11:41:40,515 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 3
2020-04-08 11:41:40,515 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 13
2020-04-08 11:41:40,515 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 1.0 (TID 3)
2020-04-08 11:41:40,515 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 18
2020-04-08 11:41:40,515 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 16
2020-04-08 11:41:40,515 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 22
2020-04-08 11:41:40,515 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 8
2020-04-08 11:41:40,515 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 1
2020-04-08 11:41:40,515 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 17
2020-04-08 11:41:40,515 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 20
2020-04-08 11:41:40,517 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 150 is the same as ending offset skipping PABUSI2 2
2020-04-08 11:41:40,519 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 1.0 (TID 3). 712 bytes result sent to driver
2020-04-08 11:41:40,519 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 4, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:41:40,519 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 1.0 (TID 4)
2020-04-08 11:41:40,520 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 3) in 6 ms on localhost (executor driver) (1/3)
2020-04-08 11:41:40,522 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 149 is the same as ending offset skipping PABUSI2 1
2020-04-08 11:41:40,522 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.102:62115 in memory (size: 2.4 KB, free: 4.1 GB)
2020-04-08 11:41:40,523 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 1.0 (TID 4). 712 bytes result sent to driver
2020-04-08 11:41:40,523 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 1.0 (TID 5, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:41:40,523 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 1.0 (TID 5)
2020-04-08 11:41:40,524 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 4) in 5 ms on localhost (executor driver) (2/3)
2020-04-08 11:41:40,525 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 5
2020-04-08 11:41:40,525 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 4
2020-04-08 11:41:40,525 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 24
2020-04-08 11:41:40,525 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 2
2020-04-08 11:41:40,526 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 11:41:40,526 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 1.0 (TID 5). 712 bytes result sent to driver
2020-04-08 11:41:40,527 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 1.0 (TID 5) in 4 ms on localhost (executor driver) (3/3)
2020-04-08 11:41:40,527 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
2020-04-08 11:41:40,528 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 1 (collect at Kafka2SparkStreaming2Kudu.scala:137) finished in 0.032 s
2020-04-08 11:41:40,528 INFO org.apache.spark.scheduler.DAGScheduler: Job 1 finished: collect at Kafka2SparkStreaming2Kudu.scala:137, took 0.035954 s
2020-04-08 11:41:40,533 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586317300000 ms.0 from job set of time 1586317300000 ms
2020-04-08 11:41:40,533 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.533 s for time 1586317300000 ms (execution: 0.053 s)
2020-04-08 11:41:40,534 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 0 from persistence list
2020-04-08 11:41:40,536 INFO org.apache.spark.storage.BlockManager: Removing RDD 0
2020-04-08 11:41:40,536 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 11:41:40,537 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 
2020-04-08 11:41:50,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 11:41:50,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 11:41:50,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 11:41:50,413 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 149.
2020-04-08 11:41:50,420 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 150.
2020-04-08 11:41:50,421 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 11:41:50,421 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586317310000 ms
2020-04-08 11:41:50,422 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586317310000 ms.0 from job set of time 1586317310000 ms
2020-04-08 11:41:50,431 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:137
2020-04-08 11:41:50,432 INFO org.apache.spark.scheduler.DAGScheduler: Got job 2 (collect at Kafka2SparkStreaming2Kudu.scala:137) with 3 output partitions
2020-04-08 11:41:50,432 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 2 (collect at Kafka2SparkStreaming2Kudu.scala:137)
2020-04-08 11:41:50,432 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 11:41:50,432 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 11:41:50,432 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[7] at map at Kafka2SparkStreaming2Kudu.scala:122), which has no missing parents
2020-04-08 11:41:50,435 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 3.8 KB, free 4.1 GB)
2020-04-08 11:41:50,436 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.4 KB, free 4.1 GB)
2020-04-08 11:41:50,437 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.102:62115 (size: 2.4 KB, free: 4.1 GB)
2020-04-08 11:41:50,437 INFO org.apache.spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2020-04-08 11:41:50,438 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at map at Kafka2SparkStreaming2Kudu.scala:122) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 11:41:50,438 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 2.0 with 3 tasks
2020-04-08 11:41:50,439 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:41:50,439 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 2.0 (TID 6)
2020-04-08 11:41:50,441 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 150 is the same as ending offset skipping PABUSI2 2
2020-04-08 11:41:50,442 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 2.0 (TID 6). 712 bytes result sent to driver
2020-04-08 11:41:50,443 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 2.0 (TID 7, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:41:50,443 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 2.0 (TID 7)
2020-04-08 11:41:50,443 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 6) in 4 ms on localhost (executor driver) (1/3)
2020-04-08 11:41:50,445 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 149 is the same as ending offset skipping PABUSI2 1
2020-04-08 11:41:50,446 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 2.0 (TID 7). 712 bytes result sent to driver
2020-04-08 11:41:50,446 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 2.0 (TID 8, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:41:50,447 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 2.0 (TID 8)
2020-04-08 11:41:50,447 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 2.0 (TID 7) in 5 ms on localhost (executor driver) (2/3)
2020-04-08 11:41:50,448 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 11:41:50,449 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 2.0 (TID 8). 669 bytes result sent to driver
2020-04-08 11:41:50,449 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 2.0 (TID 8) in 3 ms on localhost (executor driver) (3/3)
2020-04-08 11:41:50,450 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
2020-04-08 11:41:50,450 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 2 (collect at Kafka2SparkStreaming2Kudu.scala:137) finished in 0.016 s
2020-04-08 11:41:50,450 INFO org.apache.spark.scheduler.DAGScheduler: Job 2 finished: collect at Kafka2SparkStreaming2Kudu.scala:137, took 0.019494 s
2020-04-08 11:41:50,455 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586317310000 ms.0 from job set of time 1586317310000 ms
2020-04-08 11:41:50,455 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.455 s for time 1586317310000 ms (execution: 0.033 s)
2020-04-08 11:41:50,455 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 3 from persistence list
2020-04-08 11:41:50,456 INFO org.apache.spark.storage.BlockManager: Removing RDD 3
2020-04-08 11:41:50,456 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 11:41:50,456 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586317290000 ms
2020-04-08 11:42:00,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 11:42:00,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 11:42:00,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 11:42:00,187 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 150.
2020-04-08 11:42:00,187 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 11:42:00,188 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 149.
2020-04-08 11:42:00,189 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586317320000 ms
2020-04-08 11:42:00,189 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586317320000 ms.0 from job set of time 1586317320000 ms
2020-04-08 11:42:00,197 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:137
2020-04-08 11:42:00,198 INFO org.apache.spark.scheduler.DAGScheduler: Got job 3 (collect at Kafka2SparkStreaming2Kudu.scala:137) with 3 output partitions
2020-04-08 11:42:00,198 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 3 (collect at Kafka2SparkStreaming2Kudu.scala:137)
2020-04-08 11:42:00,198 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 11:42:00,198 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 11:42:00,198 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[10] at map at Kafka2SparkStreaming2Kudu.scala:122), which has no missing parents
2020-04-08 11:42:00,201 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 3.8 KB, free 4.1 GB)
2020-04-08 11:42:00,202 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.4 KB, free 4.1 GB)
2020-04-08 11:42:00,203 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.102:62115 (size: 2.4 KB, free: 4.1 GB)
2020-04-08 11:42:00,203 INFO org.apache.spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2020-04-08 11:42:00,204 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 3 (MapPartitionsRDD[10] at map at Kafka2SparkStreaming2Kudu.scala:122) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 11:42:00,204 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 3.0 with 3 tasks
2020-04-08 11:42:00,205 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 9, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:42:00,205 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 3.0 (TID 9)
2020-04-08 11:42:00,207 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 150 is the same as ending offset skipping PABUSI2 2
2020-04-08 11:42:00,208 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 3.0 (TID 9). 669 bytes result sent to driver
2020-04-08 11:42:00,208 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 3.0 (TID 10, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:42:00,209 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 3.0 (TID 10)
2020-04-08 11:42:00,209 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 9) in 3 ms on localhost (executor driver) (1/3)
2020-04-08 11:42:00,210 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 149 is the same as ending offset skipping PABUSI2 1
2020-04-08 11:42:00,211 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 3.0 (TID 10). 669 bytes result sent to driver
2020-04-08 11:42:00,211 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 3.0 (TID 11, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:42:00,212 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 3.0 (TID 11)
2020-04-08 11:42:00,212 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 3.0 (TID 10) in 4 ms on localhost (executor driver) (2/3)
2020-04-08 11:42:00,213 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 11:42:00,214 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 3.0 (TID 11). 712 bytes result sent to driver
2020-04-08 11:42:00,215 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 3.0 (TID 11) in 4 ms on localhost (executor driver) (3/3)
2020-04-08 11:42:00,215 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
2020-04-08 11:42:00,215 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 3 (collect at Kafka2SparkStreaming2Kudu.scala:137) finished in 0.015 s
2020-04-08 11:42:00,216 INFO org.apache.spark.scheduler.DAGScheduler: Job 3 finished: collect at Kafka2SparkStreaming2Kudu.scala:137, took 0.018644 s
2020-04-08 11:42:00,221 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586317320000 ms.0 from job set of time 1586317320000 ms
2020-04-08 11:42:00,221 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 6 from persistence list
2020-04-08 11:42:00,221 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.221 s for time 1586317320000 ms (execution: 0.032 s)
2020-04-08 11:42:00,222 INFO org.apache.spark.storage.BlockManager: Removing RDD 6
2020-04-08 11:42:00,222 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 11:42:00,222 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586317300000 ms
2020-04-08 11:42:10,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 11:42:10,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 11:42:10,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 11:42:10,188 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 150.
2020-04-08 11:42:10,188 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 11:42:10,188 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 149.
2020-04-08 11:42:10,189 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586317330000 ms
2020-04-08 11:42:10,189 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586317330000 ms.0 from job set of time 1586317330000 ms
2020-04-08 11:42:10,198 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:137
2020-04-08 11:42:10,199 INFO org.apache.spark.scheduler.DAGScheduler: Got job 4 (collect at Kafka2SparkStreaming2Kudu.scala:137) with 3 output partitions
2020-04-08 11:42:10,199 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 4 (collect at Kafka2SparkStreaming2Kudu.scala:137)
2020-04-08 11:42:10,199 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 11:42:10,199 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 11:42:10,199 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[13] at map at Kafka2SparkStreaming2Kudu.scala:122), which has no missing parents
2020-04-08 11:42:10,202 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 3.8 KB, free 4.1 GB)
2020-04-08 11:42:10,203 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.4 KB, free 4.1 GB)
2020-04-08 11:42:10,204 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.102:62115 (size: 2.4 KB, free: 4.1 GB)
2020-04-08 11:42:10,204 INFO org.apache.spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1161
2020-04-08 11:42:10,204 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 4 (MapPartitionsRDD[13] at map at Kafka2SparkStreaming2Kudu.scala:122) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 11:42:10,205 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 4.0 with 3 tasks
2020-04-08 11:42:10,205 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 12, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:42:10,206 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 4.0 (TID 12)
2020-04-08 11:42:10,207 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 150 is the same as ending offset skipping PABUSI2 2
2020-04-08 11:42:10,208 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 4.0 (TID 12). 712 bytes result sent to driver
2020-04-08 11:42:10,209 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 4.0 (TID 13, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:42:10,210 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 4.0 (TID 13)
2020-04-08 11:42:10,210 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 12) in 5 ms on localhost (executor driver) (1/3)
2020-04-08 11:42:10,211 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 149 is the same as ending offset skipping PABUSI2 1
2020-04-08 11:42:10,212 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 4.0 (TID 13). 669 bytes result sent to driver
2020-04-08 11:42:10,212 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 4.0 (TID 14, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:42:10,212 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 4.0 (TID 13) in 3 ms on localhost (executor driver) (2/3)
2020-04-08 11:42:10,212 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 4.0 (TID 14)
2020-04-08 11:42:10,213 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 11:42:10,214 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 4.0 (TID 14). 712 bytes result sent to driver
2020-04-08 11:42:10,214 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 4.0 (TID 14) in 2 ms on localhost (executor driver) (3/3)
2020-04-08 11:42:10,214 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
2020-04-08 11:42:10,215 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 4 (collect at Kafka2SparkStreaming2Kudu.scala:137) finished in 0.014 s
2020-04-08 11:42:10,215 INFO org.apache.spark.scheduler.DAGScheduler: Job 4 finished: collect at Kafka2SparkStreaming2Kudu.scala:137, took 0.016970 s
2020-04-08 11:42:10,220 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586317330000 ms.0 from job set of time 1586317330000 ms
2020-04-08 11:42:10,220 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.220 s for time 1586317330000 ms (execution: 0.031 s)
2020-04-08 11:42:10,220 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 9 from persistence list
2020-04-08 11:42:10,221 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 11:42:10,221 INFO org.apache.spark.storage.BlockManager: Removing RDD 9
2020-04-08 11:42:10,221 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586317310000 ms
2020-04-08 11:42:20,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 11:42:20,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 11:42:20,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 11:42:20,187 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 150.
2020-04-08 11:42:20,189 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 11:42:20,191 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 149.
2020-04-08 11:42:20,191 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586317340000 ms
2020-04-08 11:42:20,192 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586317340000 ms.0 from job set of time 1586317340000 ms
2020-04-08 11:42:20,203 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:137
2020-04-08 11:42:20,204 INFO org.apache.spark.scheduler.DAGScheduler: Got job 5 (collect at Kafka2SparkStreaming2Kudu.scala:137) with 3 output partitions
2020-04-08 11:42:20,204 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 5 (collect at Kafka2SparkStreaming2Kudu.scala:137)
2020-04-08 11:42:20,204 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 11:42:20,204 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 11:42:20,205 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[16] at map at Kafka2SparkStreaming2Kudu.scala:122), which has no missing parents
2020-04-08 11:42:20,207 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 3.8 KB, free 4.1 GB)
2020-04-08 11:42:20,208 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.4 KB, free 4.1 GB)
2020-04-08 11:42:20,209 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.102:62115 (size: 2.4 KB, free: 4.1 GB)
2020-04-08 11:42:20,209 INFO org.apache.spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1161
2020-04-08 11:42:20,210 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 5 (MapPartitionsRDD[16] at map at Kafka2SparkStreaming2Kudu.scala:122) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 11:42:20,210 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 5.0 with 3 tasks
2020-04-08 11:42:20,211 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 15, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:42:20,211 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 5.0 (TID 15)
2020-04-08 11:42:20,213 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 150 is the same as ending offset skipping PABUSI2 2
2020-04-08 11:42:20,214 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 5.0 (TID 15). 712 bytes result sent to driver
2020-04-08 11:42:20,215 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 5.0 (TID 16, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:42:20,215 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 5.0 (TID 16)
2020-04-08 11:42:20,215 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 15) in 5 ms on localhost (executor driver) (1/3)
2020-04-08 11:42:20,216 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 149 is the same as ending offset skipping PABUSI2 1
2020-04-08 11:42:20,217 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 5.0 (TID 16). 669 bytes result sent to driver
2020-04-08 11:42:20,217 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 5.0 (TID 17, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:42:20,218 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 5.0 (TID 17)
2020-04-08 11:42:20,218 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 5.0 (TID 16) in 3 ms on localhost (executor driver) (2/3)
2020-04-08 11:42:20,219 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 11:42:20,220 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 5.0 (TID 17). 669 bytes result sent to driver
2020-04-08 11:42:20,220 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 5.0 (TID 17) in 3 ms on localhost (executor driver) (3/3)
2020-04-08 11:42:20,220 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
2020-04-08 11:42:20,221 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 5 (collect at Kafka2SparkStreaming2Kudu.scala:137) finished in 0.015 s
2020-04-08 11:42:20,221 INFO org.apache.spark.scheduler.DAGScheduler: Job 5 finished: collect at Kafka2SparkStreaming2Kudu.scala:137, took 0.018011 s
2020-04-08 11:42:20,226 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586317340000 ms.0 from job set of time 1586317340000 ms
2020-04-08 11:42:20,227 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.226 s for time 1586317340000 ms (execution: 0.034 s)
2020-04-08 11:42:20,227 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 12 from persistence list
2020-04-08 11:42:20,227 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 11:42:20,227 INFO org.apache.spark.storage.BlockManager: Removing RDD 12
2020-04-08 11:42:20,227 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586317320000 ms
2020-04-08 11:42:30,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 11:42:30,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 11:42:30,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 11:42:30,182 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 150.
2020-04-08 11:42:30,182 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 11:42:30,182 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 149.
2020-04-08 11:42:30,183 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586317350000 ms
2020-04-08 11:42:30,183 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586317350000 ms.0 from job set of time 1586317350000 ms
2020-04-08 11:42:30,190 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:137
2020-04-08 11:42:30,191 INFO org.apache.spark.scheduler.DAGScheduler: Got job 6 (collect at Kafka2SparkStreaming2Kudu.scala:137) with 3 output partitions
2020-04-08 11:42:30,191 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 6 (collect at Kafka2SparkStreaming2Kudu.scala:137)
2020-04-08 11:42:30,191 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 11:42:30,191 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 11:42:30,192 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[19] at map at Kafka2SparkStreaming2Kudu.scala:122), which has no missing parents
2020-04-08 11:42:30,194 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 3.8 KB, free 4.1 GB)
2020-04-08 11:42:30,195 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.4 KB, free 4.1 GB)
2020-04-08 11:42:30,195 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.0.102:62115 (size: 2.4 KB, free: 4.1 GB)
2020-04-08 11:42:30,196 INFO org.apache.spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1161
2020-04-08 11:42:30,196 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 6 (MapPartitionsRDD[19] at map at Kafka2SparkStreaming2Kudu.scala:122) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 11:42:30,196 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 6.0 with 3 tasks
2020-04-08 11:42:30,197 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 18, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:42:30,197 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 6.0 (TID 18)
2020-04-08 11:42:30,199 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 150 is the same as ending offset skipping PABUSI2 2
2020-04-08 11:42:30,199 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 6.0 (TID 18). 712 bytes result sent to driver
2020-04-08 11:42:30,200 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 6.0 (TID 19, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:42:30,200 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 18) in 3 ms on localhost (executor driver) (1/3)
2020-04-08 11:42:30,200 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 6.0 (TID 19)
2020-04-08 11:42:30,201 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 149 is the same as ending offset skipping PABUSI2 1
2020-04-08 11:42:30,202 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 6.0 (TID 19). 712 bytes result sent to driver
2020-04-08 11:42:30,203 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 6.0 (TID 20, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:42:30,203 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 6.0 (TID 20)
2020-04-08 11:42:30,203 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 6.0 (TID 19) in 3 ms on localhost (executor driver) (2/3)
2020-04-08 11:42:30,204 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 11:42:30,205 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 6.0 (TID 20). 712 bytes result sent to driver
2020-04-08 11:42:30,206 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 6.0 (TID 20) in 3 ms on localhost (executor driver) (3/3)
2020-04-08 11:42:30,206 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
2020-04-08 11:42:30,206 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 6 (collect at Kafka2SparkStreaming2Kudu.scala:137) finished in 0.013 s
2020-04-08 11:42:30,207 INFO org.apache.spark.scheduler.DAGScheduler: Job 6 finished: collect at Kafka2SparkStreaming2Kudu.scala:137, took 0.016006 s
2020-04-08 11:42:30,212 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586317350000 ms.0 from job set of time 1586317350000 ms
2020-04-08 11:42:30,212 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.212 s for time 1586317350000 ms (execution: 0.029 s)
2020-04-08 11:42:30,212 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 15 from persistence list
2020-04-08 11:42:30,213 INFO org.apache.spark.storage.BlockManager: Removing RDD 15
2020-04-08 11:42:30,213 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 11:42:30,214 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586317330000 ms
2020-04-08 11:42:40,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 11:42:40,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 11:42:40,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 11:42:40,189 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 150.
2020-04-08 11:42:40,190 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 11:42:40,190 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 150.
2020-04-08 11:42:40,191 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586317360000 ms
2020-04-08 11:42:40,192 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586317360000 ms.0 from job set of time 1586317360000 ms
2020-04-08 11:42:40,202 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:137
2020-04-08 11:42:40,203 INFO org.apache.spark.scheduler.DAGScheduler: Got job 7 (collect at Kafka2SparkStreaming2Kudu.scala:137) with 3 output partitions
2020-04-08 11:42:40,203 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 7 (collect at Kafka2SparkStreaming2Kudu.scala:137)
2020-04-08 11:42:40,203 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 11:42:40,203 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 11:42:40,204 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[22] at map at Kafka2SparkStreaming2Kudu.scala:122), which has no missing parents
2020-04-08 11:42:40,207 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 3.8 KB, free 4.1 GB)
2020-04-08 11:42:40,209 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.4 KB, free 4.1 GB)
2020-04-08 11:42:40,210 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.0.102:62115 (size: 2.4 KB, free: 4.1 GB)
2020-04-08 11:42:40,210 INFO org.apache.spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1161
2020-04-08 11:42:40,211 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 7 (MapPartitionsRDD[22] at map at Kafka2SparkStreaming2Kudu.scala:122) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 11:42:40,211 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 7.0 with 3 tasks
2020-04-08 11:42:40,212 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 7.0 (TID 21, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:42:40,212 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 7.0 (TID 21)
2020-04-08 11:42:40,215 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 150 is the same as ending offset skipping PABUSI2 2
2020-04-08 11:42:40,216 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 7.0 (TID 21). 669 bytes result sent to driver
2020-04-08 11:42:40,216 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 7.0 (TID 22, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:42:40,216 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 7.0 (TID 22)
2020-04-08 11:42:40,216 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 7.0 (TID 21) in 4 ms on localhost (executor driver) (1/3)
2020-04-08 11:42:40,218 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Computing topic PABUSI2, partition 1 offsets 149 -> 150
2020-04-08 11:42:40,222 INFO org.apache.spark.streaming.kafka010.KafkaDataConsumer: Initializing cache 16 64 0.75
2020-04-08 11:42:40,225 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-testgroup1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-08 11:42:40,233 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-08 11:42:40,233 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-08 11:42:40,233 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586317360232
2020-04-08 11:42:40,234 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Subscribed to partition(s): PABUSI2-1
2020-04-08 11:42:40,237 INFO org.apache.spark.streaming.kafka010.InternalKafkaConsumer: Initial fetch for spark-executor-testgroup1 PABUSI2-1 149
2020-04-08 11:42:40,237 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Seeking to offset 149 for partition PABUSI2-1
2020-04-08 11:42:40,768 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-08 11:42:41,552 ERROR org.apache.spark.executor.Executor: Exception in task 1.0 in stage 7.0 (TID 22)
java.lang.NullPointerException
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:131)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:122)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:363)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1174)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:296)
	at scala.collection.AbstractIterator.to(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:275)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1174)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 11:42:41,572 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 7.0 (TID 23, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 11:42:41,574 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 7.0 (TID 23)
2020-04-08 11:42:41,578 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 11:42:41,578 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 1.0 in stage 7.0 (TID 22, localhost, executor driver): java.lang.NullPointerException
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:131)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:122)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:363)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1174)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:296)
	at scala.collection.AbstractIterator.to(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:275)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1174)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2020-04-08 11:42:41,578 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 7.0 (TID 23). 712 bytes result sent to driver
2020-04-08 11:42:41,579 ERROR org.apache.spark.scheduler.TaskSetManager: Task 1 in stage 7.0 failed 1 times; aborting job
2020-04-08 11:42:41,581 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 7.0 (TID 23) in 9 ms on localhost (executor driver) (2/3)
2020-04-08 11:42:41,581 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
2020-04-08 11:42:41,582 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Cancelling stage 7
2020-04-08 11:42:41,583 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Killing all running tasks in stage 7: Stage cancelled
2020-04-08 11:42:41,584 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 7 (collect at Kafka2SparkStreaming2Kudu.scala:137) failed in 1.378 s due to Job aborted due to stage failure: Task 1 in stage 7.0 failed 1 times, most recent failure: Lost task 1.0 in stage 7.0 (TID 22, localhost, executor driver): java.lang.NullPointerException
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:131)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:122)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:363)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1174)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:296)
	at scala.collection.AbstractIterator.to(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:275)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1174)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
2020-04-08 11:42:41,587 INFO org.apache.spark.scheduler.DAGScheduler: Job 7 failed: collect at Kafka2SparkStreaming2Kudu.scala:137, took 1.383117 s
2020-04-08 11:42:41,589 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586317360000 ms.0 from job set of time 1586317360000 ms
2020-04-08 11:42:41,591 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1586317360000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 7.0 failed 1 times, most recent failure: Lost task 1.0 in stage 7.0 (TID 22, localhost, executor driver): java.lang.NullPointerException
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:131)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:122)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:363)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1174)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:296)
	at scala.collection.AbstractIterator.to(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:275)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1174)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:245)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:944)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1.apply(Kafka2SparkStreaming2Kudu.scala:137)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1.apply(Kafka2SparkStreaming2Kudu.scala:120)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:191)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.NullPointerException
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:131)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$1.apply(Kafka2SparkStreaming2Kudu.scala:122)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:363)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1174)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:296)
	at scala.collection.AbstractIterator.to(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:275)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1174)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	... 3 more
2020-04-08 11:42:41,604 INFO org.apache.spark.streaming.StreamingContext: Invoking stop(stopGracefully=false) from shutdown hook
2020-04-08 11:42:41,607 INFO org.apache.spark.streaming.scheduler.ReceiverTracker: ReceiverTracker stopped
2020-04-08 11:42:41,609 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopping JobGenerator immediately
2020-04-08 11:42:41,611 INFO org.apache.spark.streaming.util.RecurringTimer: Stopped timer for JobGenerator after time 1586317360000
2020-04-08 11:42:41,818 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Member consumer-1-c77f22c5-535e-41ad-9beb-88feb3efe4d3 sending LeaveGroup request to coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 11:42:42,010 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopped JobGenerator
2020-04-08 11:42:42,015 INFO org.apache.spark.streaming.scheduler.JobScheduler: Stopped JobScheduler
2020-04-08 11:42:42,020 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@24934262{/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 11:42:42,021 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@5b051a5c{/streaming/batch,null,UNAVAILABLE,@Spark}
2020-04-08 11:42:42,022 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@58cf8f94{/static/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 11:42:42,024 INFO org.apache.spark.streaming.StreamingContext: StreamingContext stopped successfully
2020-04-08 11:42:42,024 INFO org.apache.spark.SparkContext: Invoking stop() from shutdown hook
2020-04-08 11:42:42,032 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@71e5f61d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 11:42:42,036 INFO org.apache.spark.ui.SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
2020-04-08 11:42:42,045 INFO org.apache.spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2020-04-08 11:42:42,060 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2020-04-08 11:42:42,060 INFO org.apache.spark.storage.BlockManager: BlockManager stopped
2020-04-08 11:42:42,061 INFO org.apache.spark.storage.BlockManagerMaster: BlockManagerMaster stopped
2020-04-08 11:42:42,069 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2020-04-08 11:42:42,076 INFO org.apache.spark.SparkContext: Successfully stopped SparkContext
2020-04-08 11:42:42,076 INFO org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2020-04-08 11:42:42,077 INFO org.apache.spark.util.ShutdownHookManager: Deleting directory /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/spark-c588d894-ada2-4339-aba5-c37765dafecd
2020-04-08 12:07:18,233 WARN org.apache.spark.util.Utils: Your hostname, MacBook-Pro-CW.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)
2020-04-08 12:07:18,234 WARN org.apache.spark.util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
2020-04-08 12:07:18,269 INFO org.apache.spark.SparkContext: Running Spark version 2.4.0.cloudera2
2020-04-08 12:07:18,512 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-04-08 12:07:18,645 INFO org.apache.spark.SparkContext: Submitted application: Kafka2-kerberos
2020-04-08 12:07:18,694 INFO org.apache.spark.SecurityManager: Changing view acls to: godiswc
2020-04-08 12:07:18,695 INFO org.apache.spark.SecurityManager: Changing modify acls to: godiswc
2020-04-08 12:07:18,695 INFO org.apache.spark.SecurityManager: Changing view acls groups to: 
2020-04-08 12:07:18,696 INFO org.apache.spark.SecurityManager: Changing modify acls groups to: 
2020-04-08 12:07:18,696 INFO org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(godiswc); groups with view permissions: Set(); users  with modify permissions: Set(godiswc); groups with modify permissions: Set()
2020-04-08 12:07:18,839 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 12:07:18,913 INFO org.apache.spark.util.Utils: Successfully started service 'sparkDriver' on port 51901.
2020-04-08 12:07:18,930 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
2020-04-08 12:07:18,940 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
2020-04-08 12:07:18,941 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-04-08 12:07:18,942 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2020-04-08 12:07:18,951 INFO org.apache.spark.storage.DiskBlockManager: Created local directory at /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/blockmgr-565760a1-f263-4811-a161-976d99d2eed7
2020-04-08 12:07:18,985 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 4.1 GB
2020-04-08 12:07:18,995 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
2020-04-08 12:07:19,043 INFO org.spark_project.jetty.util.log: Logging initialized @1573ms
2020-04-08 12:07:19,096 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2019-04-30T04:42:08+08:00, git hash: e1bc35120a6617ee3df052294e433f3a25ce7097
2020-04-08 12:07:19,108 INFO org.spark_project.jetty.server.Server: Started @1640ms
2020-04-08 12:07:19,110 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 12:07:19,125 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@743cb8e0{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 12:07:19,125 INFO org.apache.spark.util.Utils: Successfully started service 'SparkUI' on port 4040.
2020-04-08 12:07:19,147 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@41a90fa8{/jobs,null,AVAILABLE,@Spark}
2020-04-08 12:07:19,147 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c67e137{/jobs/json,null,AVAILABLE,@Spark}
2020-04-08 12:07:19,148 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2c9399a4{/jobs/job,null,AVAILABLE,@Spark}
2020-04-08 12:07:19,150 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@53ab0286{/jobs/job/json,null,AVAILABLE,@Spark}
2020-04-08 12:07:19,151 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@63c5efee{/stages,null,AVAILABLE,@Spark}
2020-04-08 12:07:19,152 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2d10e0b1{/stages/json,null,AVAILABLE,@Spark}
2020-04-08 12:07:19,152 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1c98290c{/stages/stage,null,AVAILABLE,@Spark}
2020-04-08 12:07:19,154 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@71e5f61d{/stages/stage/json,null,AVAILABLE,@Spark}
2020-04-08 12:07:19,154 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2ce86164{/stages/pool,null,AVAILABLE,@Spark}
2020-04-08 12:07:19,154 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5e8f9e2d{/stages/pool/json,null,AVAILABLE,@Spark}
2020-04-08 12:07:19,155 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@51df223b{/storage,null,AVAILABLE,@Spark}
2020-04-08 12:07:19,155 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@fd46303{/storage/json,null,AVAILABLE,@Spark}
2020-04-08 12:07:19,156 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60d8c0dc{/storage/rdd,null,AVAILABLE,@Spark}
2020-04-08 12:07:19,156 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4204541c{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-04-08 12:07:19,157 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a62689d{/environment,null,AVAILABLE,@Spark}
2020-04-08 12:07:19,158 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4602c2a9{/environment/json,null,AVAILABLE,@Spark}
2020-04-08 12:07:19,159 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60fa3495{/executors,null,AVAILABLE,@Spark}
2020-04-08 12:07:19,159 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3e2822{/executors/json,null,AVAILABLE,@Spark}
2020-04-08 12:07:19,160 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@79e18e38{/executors/threadDump,null,AVAILABLE,@Spark}
2020-04-08 12:07:19,161 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@29a60c27{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-04-08 12:07:19,167 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1849db1a{/static,null,AVAILABLE,@Spark}
2020-04-08 12:07:19,168 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2cf23c81{/,null,AVAILABLE,@Spark}
2020-04-08 12:07:19,168 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3624da92{/api,null,AVAILABLE,@Spark}
2020-04-08 12:07:19,169 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2484f433{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-04-08 12:07:19,170 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60b71e8f{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-04-08 12:07:19,171 INFO org.apache.spark.ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
2020-04-08 12:07:19,262 INFO org.apache.spark.executor.Executor: Starting executor ID driver on host localhost
2020-04-08 12:07:19,340 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 12:07:19,341 INFO org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51905.
2020-04-08 12:07:19,341 INFO org.apache.spark.network.netty.NettyBlockTransferService: Server created on 192.168.0.102:51905
2020-04-08 12:07:19,343 INFO org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-04-08 12:07:19,376 INFO org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 51905, None)
2020-04-08 12:07:19,379 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:51905 with 4.1 GB RAM, BlockManagerId(driver, 192.168.0.102, 51905, None)
2020-04-08 12:07:19,381 INFO org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 51905, None)
2020-04-08 12:07:19,382 INFO org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 51905, None)
2020-04-08 12:07:19,588 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@502f8b57{/metrics/json,null,AVAILABLE,@Spark}
2020-04-08 12:07:19,897 INFO org.apache.spark.SparkContext: Starting job: collect at test.scala:15
2020-04-08 12:07:19,918 INFO org.apache.spark.scheduler.DAGScheduler: Got job 0 (collect at test.scala:15) with 1 output partitions
2020-04-08 12:07:19,919 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 0 (collect at test.scala:15)
2020-04-08 12:07:19,920 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 12:07:19,921 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 12:07:19,925 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at test.scala:13), which has no missing parents
2020-04-08 12:07:19,980 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 1448.0 B, free 4.1 GB)
2020-04-08 12:07:20,421 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 975.0 B, free 4.1 GB)
2020-04-08 12:07:20,424 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:51905 (size: 975.0 B, free: 4.1 GB)
2020-04-08 12:07:20,426 INFO org.apache.spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2020-04-08 12:07:20,447 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at test.scala:13) (first 15 tasks are for partitions Vector(0))
2020-04-08 12:07:20,448 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
2020-04-08 12:07:20,550 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 8011 bytes)
2020-04-08 12:07:20,560 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
2020-04-08 12:07:20,599 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 868 bytes result sent to driver
2020-04-08 12:07:20,609 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 81 ms on localhost (executor driver) (1/1)
2020-04-08 12:07:20,615 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-04-08 12:07:20,622 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 0 (collect at test.scala:15) finished in 0.683 s
2020-04-08 12:07:20,631 INFO org.apache.spark.scheduler.DAGScheduler: Job 0 finished: collect at test.scala:15, took 0.733730 s
2020-04-08 12:07:20,642 INFO org.apache.spark.SparkContext: Invoking stop() from shutdown hook
2020-04-08 12:07:20,668 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@743cb8e0{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 12:07:20,672 INFO org.apache.spark.ui.SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
2020-04-08 12:07:20,681 INFO org.apache.spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2020-04-08 12:07:20,708 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2020-04-08 12:07:20,709 INFO org.apache.spark.storage.BlockManager: BlockManager stopped
2020-04-08 12:07:20,727 INFO org.apache.spark.storage.BlockManagerMaster: BlockManagerMaster stopped
2020-04-08 12:07:20,731 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2020-04-08 12:07:20,755 INFO org.apache.spark.SparkContext: Successfully stopped SparkContext
2020-04-08 12:07:20,756 INFO org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2020-04-08 12:07:20,758 INFO org.apache.spark.util.ShutdownHookManager: Deleting directory /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/spark-0b254255-6838-458b-bdf3-029d8067f883
2020-04-08 12:08:14,216 WARN org.apache.spark.util.Utils: Your hostname, MacBook-Pro-CW.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)
2020-04-08 12:08:14,217 WARN org.apache.spark.util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
2020-04-08 12:08:14,244 INFO org.apache.spark.SparkContext: Running Spark version 2.4.0.cloudera2
2020-04-08 12:08:14,422 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-04-08 12:08:14,552 INFO org.apache.spark.SparkContext: Submitted application: Kafka2-kerberos
2020-04-08 12:08:14,593 INFO org.apache.spark.SecurityManager: Changing view acls to: godiswc
2020-04-08 12:08:14,593 INFO org.apache.spark.SecurityManager: Changing modify acls to: godiswc
2020-04-08 12:08:14,594 INFO org.apache.spark.SecurityManager: Changing view acls groups to: 
2020-04-08 12:08:14,594 INFO org.apache.spark.SecurityManager: Changing modify acls groups to: 
2020-04-08 12:08:14,595 INFO org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(godiswc); groups with view permissions: Set(); users  with modify permissions: Set(godiswc); groups with modify permissions: Set()
2020-04-08 12:08:14,736 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 12:08:14,815 INFO org.apache.spark.util.Utils: Successfully started service 'sparkDriver' on port 52109.
2020-04-08 12:08:14,832 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
2020-04-08 12:08:14,843 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
2020-04-08 12:08:14,845 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-04-08 12:08:14,845 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2020-04-08 12:08:14,856 INFO org.apache.spark.storage.DiskBlockManager: Created local directory at /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/blockmgr-c83ab838-0507-4f54-80ed-76728a56d3a4
2020-04-08 12:08:14,888 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 4.1 GB
2020-04-08 12:08:14,896 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
2020-04-08 12:08:14,943 INFO org.spark_project.jetty.util.log: Logging initialized @1311ms
2020-04-08 12:08:15,004 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2019-04-30T04:42:08+08:00, git hash: e1bc35120a6617ee3df052294e433f3a25ce7097
2020-04-08 12:08:15,018 INFO org.spark_project.jetty.server.Server: Started @1387ms
2020-04-08 12:08:15,021 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 12:08:15,040 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@743cb8e0{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 12:08:15,040 INFO org.apache.spark.util.Utils: Successfully started service 'SparkUI' on port 4040.
2020-04-08 12:08:15,062 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@41a90fa8{/jobs,null,AVAILABLE,@Spark}
2020-04-08 12:08:15,063 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c67e137{/jobs/json,null,AVAILABLE,@Spark}
2020-04-08 12:08:15,063 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2c9399a4{/jobs/job,null,AVAILABLE,@Spark}
2020-04-08 12:08:15,066 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@53ab0286{/jobs/job/json,null,AVAILABLE,@Spark}
2020-04-08 12:08:15,066 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@63c5efee{/stages,null,AVAILABLE,@Spark}
2020-04-08 12:08:15,066 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2d10e0b1{/stages/json,null,AVAILABLE,@Spark}
2020-04-08 12:08:15,067 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1c98290c{/stages/stage,null,AVAILABLE,@Spark}
2020-04-08 12:08:15,068 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@71e5f61d{/stages/stage/json,null,AVAILABLE,@Spark}
2020-04-08 12:08:15,069 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2ce86164{/stages/pool,null,AVAILABLE,@Spark}
2020-04-08 12:08:15,069 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5e8f9e2d{/stages/pool/json,null,AVAILABLE,@Spark}
2020-04-08 12:08:15,069 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@51df223b{/storage,null,AVAILABLE,@Spark}
2020-04-08 12:08:15,070 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@fd46303{/storage/json,null,AVAILABLE,@Spark}
2020-04-08 12:08:15,070 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60d8c0dc{/storage/rdd,null,AVAILABLE,@Spark}
2020-04-08 12:08:15,071 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4204541c{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-04-08 12:08:15,071 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a62689d{/environment,null,AVAILABLE,@Spark}
2020-04-08 12:08:15,072 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4602c2a9{/environment/json,null,AVAILABLE,@Spark}
2020-04-08 12:08:15,072 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60fa3495{/executors,null,AVAILABLE,@Spark}
2020-04-08 12:08:15,073 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3e2822{/executors/json,null,AVAILABLE,@Spark}
2020-04-08 12:08:15,073 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@79e18e38{/executors/threadDump,null,AVAILABLE,@Spark}
2020-04-08 12:08:15,074 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@29a60c27{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-04-08 12:08:15,079 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1849db1a{/static,null,AVAILABLE,@Spark}
2020-04-08 12:08:15,080 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2cf23c81{/,null,AVAILABLE,@Spark}
2020-04-08 12:08:15,080 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3624da92{/api,null,AVAILABLE,@Spark}
2020-04-08 12:08:15,081 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2484f433{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-04-08 12:08:15,081 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60b71e8f{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-04-08 12:08:15,083 INFO org.apache.spark.ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
2020-04-08 12:08:15,189 INFO org.apache.spark.executor.Executor: Starting executor ID driver on host localhost
2020-04-08 12:08:15,264 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 12:08:15,266 INFO org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52110.
2020-04-08 12:08:15,267 INFO org.apache.spark.network.netty.NettyBlockTransferService: Server created on 192.168.0.102:52110
2020-04-08 12:08:15,269 INFO org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-04-08 12:08:15,299 INFO org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 52110, None)
2020-04-08 12:08:15,301 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:52110 with 4.1 GB RAM, BlockManagerId(driver, 192.168.0.102, 52110, None)
2020-04-08 12:08:15,304 INFO org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 52110, None)
2020-04-08 12:08:15,304 INFO org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 52110, None)
2020-04-08 12:08:15,459 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@502f8b57{/metrics/json,null,AVAILABLE,@Spark}
2020-04-08 12:08:15,749 INFO org.apache.spark.SparkContext: Starting job: collect at test.scala:15
2020-04-08 12:08:15,765 INFO org.apache.spark.scheduler.DAGScheduler: Got job 0 (collect at test.scala:15) with 1 output partitions
2020-04-08 12:08:15,765 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 0 (collect at test.scala:15)
2020-04-08 12:08:15,766 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 12:08:15,767 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 12:08:15,770 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at test.scala:13), which has no missing parents
2020-04-08 12:08:15,811 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 1448.0 B, free 4.1 GB)
2020-04-08 12:08:16,211 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 975.0 B, free 4.1 GB)
2020-04-08 12:08:16,213 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:52110 (size: 975.0 B, free: 4.1 GB)
2020-04-08 12:08:16,214 INFO org.apache.spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2020-04-08 12:08:16,227 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at test.scala:13) (first 15 tasks are for partitions Vector(0))
2020-04-08 12:08:16,227 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
2020-04-08 12:08:16,298 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 8011 bytes)
2020-04-08 12:08:16,304 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
2020-04-08 12:08:16,332 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 868 bytes result sent to driver
2020-04-08 12:08:16,336 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 52 ms on localhost (executor driver) (1/1)
2020-04-08 12:08:16,338 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-04-08 12:08:16,342 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 0 (collect at test.scala:15) finished in 0.561 s
2020-04-08 12:08:16,346 INFO org.apache.spark.scheduler.DAGScheduler: Job 0 finished: collect at test.scala:15, took 0.597181 s
2020-04-08 12:08:16,351 INFO org.apache.spark.SparkContext: Invoking stop() from shutdown hook
2020-04-08 12:08:16,360 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@743cb8e0{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 12:08:16,361 INFO org.apache.spark.ui.SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
2020-04-08 12:08:16,367 INFO org.apache.spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2020-04-08 12:08:16,376 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2020-04-08 12:08:16,377 INFO org.apache.spark.storage.BlockManager: BlockManager stopped
2020-04-08 12:08:16,380 INFO org.apache.spark.storage.BlockManagerMaster: BlockManagerMaster stopped
2020-04-08 12:08:16,381 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2020-04-08 12:08:16,388 INFO org.apache.spark.SparkContext: Successfully stopped SparkContext
2020-04-08 12:08:16,389 INFO org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2020-04-08 12:08:16,389 INFO org.apache.spark.util.ShutdownHookManager: Deleting directory /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/spark-4cdd61d9-7a60-45b3-831a-8ab2b92a1a25
2020-04-08 12:09:12,845 WARN org.apache.spark.util.Utils: Your hostname, MacBook-Pro-CW.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)
2020-04-08 12:09:12,846 WARN org.apache.spark.util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
2020-04-08 12:09:12,873 INFO org.apache.spark.SparkContext: Running Spark version 2.4.0.cloudera2
2020-04-08 12:09:13,057 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-04-08 12:09:13,180 INFO org.apache.spark.SparkContext: Submitted application: Kafka2-kerberos
2020-04-08 12:09:13,223 INFO org.apache.spark.SecurityManager: Changing view acls to: godiswc
2020-04-08 12:09:13,223 INFO org.apache.spark.SecurityManager: Changing modify acls to: godiswc
2020-04-08 12:09:13,224 INFO org.apache.spark.SecurityManager: Changing view acls groups to: 
2020-04-08 12:09:13,224 INFO org.apache.spark.SecurityManager: Changing modify acls groups to: 
2020-04-08 12:09:13,225 INFO org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(godiswc); groups with view permissions: Set(); users  with modify permissions: Set(godiswc); groups with modify permissions: Set()
2020-04-08 12:09:13,373 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 12:09:13,462 INFO org.apache.spark.util.Utils: Successfully started service 'sparkDriver' on port 52331.
2020-04-08 12:09:13,480 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
2020-04-08 12:09:13,491 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
2020-04-08 12:09:13,493 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-04-08 12:09:13,493 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2020-04-08 12:09:13,504 INFO org.apache.spark.storage.DiskBlockManager: Created local directory at /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/blockmgr-fad6753e-8b61-4a1b-92b0-06cb712e611e
2020-04-08 12:09:13,535 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 4.1 GB
2020-04-08 12:09:13,545 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
2020-04-08 12:09:13,607 INFO org.spark_project.jetty.util.log: Logging initialized @1399ms
2020-04-08 12:09:13,656 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2019-04-30T04:42:08+08:00, git hash: e1bc35120a6617ee3df052294e433f3a25ce7097
2020-04-08 12:09:13,668 INFO org.spark_project.jetty.server.Server: Started @1461ms
2020-04-08 12:09:13,670 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 12:09:13,682 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@743cb8e0{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 12:09:13,685 INFO org.apache.spark.util.Utils: Successfully started service 'SparkUI' on port 4040.
2020-04-08 12:09:13,706 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@41a90fa8{/jobs,null,AVAILABLE,@Spark}
2020-04-08 12:09:13,707 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c67e137{/jobs/json,null,AVAILABLE,@Spark}
2020-04-08 12:09:13,708 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2c9399a4{/jobs/job,null,AVAILABLE,@Spark}
2020-04-08 12:09:13,711 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@53ab0286{/jobs/job/json,null,AVAILABLE,@Spark}
2020-04-08 12:09:13,712 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@63c5efee{/stages,null,AVAILABLE,@Spark}
2020-04-08 12:09:13,712 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2d10e0b1{/stages/json,null,AVAILABLE,@Spark}
2020-04-08 12:09:13,713 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1c98290c{/stages/stage,null,AVAILABLE,@Spark}
2020-04-08 12:09:13,714 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@71e5f61d{/stages/stage/json,null,AVAILABLE,@Spark}
2020-04-08 12:09:13,714 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2ce86164{/stages/pool,null,AVAILABLE,@Spark}
2020-04-08 12:09:13,715 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5e8f9e2d{/stages/pool/json,null,AVAILABLE,@Spark}
2020-04-08 12:09:13,715 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@51df223b{/storage,null,AVAILABLE,@Spark}
2020-04-08 12:09:13,716 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@fd46303{/storage/json,null,AVAILABLE,@Spark}
2020-04-08 12:09:13,716 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60d8c0dc{/storage/rdd,null,AVAILABLE,@Spark}
2020-04-08 12:09:13,717 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4204541c{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-04-08 12:09:13,717 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a62689d{/environment,null,AVAILABLE,@Spark}
2020-04-08 12:09:13,718 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4602c2a9{/environment/json,null,AVAILABLE,@Spark}
2020-04-08 12:09:13,718 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60fa3495{/executors,null,AVAILABLE,@Spark}
2020-04-08 12:09:13,719 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3e2822{/executors/json,null,AVAILABLE,@Spark}
2020-04-08 12:09:13,720 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@79e18e38{/executors/threadDump,null,AVAILABLE,@Spark}
2020-04-08 12:09:13,720 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@29a60c27{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-04-08 12:09:13,725 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1849db1a{/static,null,AVAILABLE,@Spark}
2020-04-08 12:09:13,726 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2cf23c81{/,null,AVAILABLE,@Spark}
2020-04-08 12:09:13,727 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3624da92{/api,null,AVAILABLE,@Spark}
2020-04-08 12:09:13,727 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2484f433{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-04-08 12:09:13,728 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60b71e8f{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-04-08 12:09:13,729 INFO org.apache.spark.ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
2020-04-08 12:09:13,802 INFO org.apache.spark.executor.Executor: Starting executor ID driver on host localhost
2020-04-08 12:09:13,864 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 12:09:13,867 INFO org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52332.
2020-04-08 12:09:13,868 INFO org.apache.spark.network.netty.NettyBlockTransferService: Server created on 192.168.0.102:52332
2020-04-08 12:09:13,869 INFO org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-04-08 12:09:13,899 INFO org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 52332, None)
2020-04-08 12:09:13,902 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:52332 with 4.1 GB RAM, BlockManagerId(driver, 192.168.0.102, 52332, None)
2020-04-08 12:09:13,904 INFO org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 52332, None)
2020-04-08 12:09:13,905 INFO org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 52332, None)
2020-04-08 12:09:14,057 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@502f8b57{/metrics/json,null,AVAILABLE,@Spark}
2020-04-08 12:09:14,412 INFO org.apache.spark.SparkContext: Starting job: collect at test.scala:15
2020-04-08 12:09:14,434 INFO org.apache.spark.scheduler.DAGScheduler: Got job 0 (collect at test.scala:15) with 1 output partitions
2020-04-08 12:09:14,434 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 0 (collect at test.scala:15)
2020-04-08 12:09:14,435 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 12:09:14,436 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 12:09:14,441 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at test.scala:13), which has no missing parents
2020-04-08 12:09:14,498 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 1448.0 B, free 4.1 GB)
2020-04-08 12:09:14,877 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 975.0 B, free 4.1 GB)
2020-04-08 12:09:14,878 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:52332 (size: 975.0 B, free: 4.1 GB)
2020-04-08 12:09:14,880 INFO org.apache.spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2020-04-08 12:09:14,892 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at test.scala:13) (first 15 tasks are for partitions Vector(0))
2020-04-08 12:09:14,892 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
2020-04-08 12:09:14,968 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 8011 bytes)
2020-04-08 12:09:14,977 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
2020-04-08 12:09:15,012 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 868 bytes result sent to driver
2020-04-08 12:09:15,017 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 66 ms on localhost (executor driver) (1/1)
2020-04-08 12:09:15,019 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-04-08 12:09:15,023 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 0 (collect at test.scala:15) finished in 0.566 s
2020-04-08 12:09:15,028 INFO org.apache.spark.scheduler.DAGScheduler: Job 0 finished: collect at test.scala:15, took 0.615914 s
2020-04-08 12:09:15,033 INFO org.apache.spark.SparkContext: Invoking stop() from shutdown hook
2020-04-08 12:09:15,044 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@743cb8e0{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 12:09:15,046 INFO org.apache.spark.ui.SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
2020-04-08 12:09:15,052 INFO org.apache.spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2020-04-08 12:09:15,063 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2020-04-08 12:09:15,064 INFO org.apache.spark.storage.BlockManager: BlockManager stopped
2020-04-08 12:09:15,067 INFO org.apache.spark.storage.BlockManagerMaster: BlockManagerMaster stopped
2020-04-08 12:09:15,069 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2020-04-08 12:09:15,077 INFO org.apache.spark.SparkContext: Successfully stopped SparkContext
2020-04-08 12:09:15,078 INFO org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2020-04-08 12:09:15,079 INFO org.apache.spark.util.ShutdownHookManager: Deleting directory /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/spark-5e088ec9-484c-4ea0-b52d-465a0589e6cf
2020-04-08 12:09:42,281 WARN org.apache.spark.util.Utils: Your hostname, MacBook-Pro-CW.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)
2020-04-08 12:09:42,282 WARN org.apache.spark.util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
2020-04-08 12:09:42,316 INFO org.apache.spark.SparkContext: Running Spark version 2.4.0.cloudera2
2020-04-08 12:09:42,527 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-04-08 12:09:42,663 INFO org.apache.spark.SparkContext: Submitted application: Kafka2SparkStreaming2Kudu-kerberos
2020-04-08 12:09:42,714 INFO org.apache.spark.SecurityManager: Changing view acls to: godiswc
2020-04-08 12:09:42,714 INFO org.apache.spark.SecurityManager: Changing modify acls to: godiswc
2020-04-08 12:09:42,714 INFO org.apache.spark.SecurityManager: Changing view acls groups to: 
2020-04-08 12:09:42,715 INFO org.apache.spark.SecurityManager: Changing modify acls groups to: 
2020-04-08 12:09:42,715 INFO org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(godiswc); groups with view permissions: Set(); users  with modify permissions: Set(godiswc); groups with modify permissions: Set()
2020-04-08 12:09:42,858 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 12:09:42,936 INFO org.apache.spark.util.Utils: Successfully started service 'sparkDriver' on port 52443.
2020-04-08 12:09:42,953 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
2020-04-08 12:09:42,964 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
2020-04-08 12:09:42,966 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-04-08 12:09:42,967 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2020-04-08 12:09:42,998 INFO org.apache.spark.storage.DiskBlockManager: Created local directory at /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/blockmgr-737da082-9674-4f22-835d-c6b7e16960a8
2020-04-08 12:09:43,011 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 4.1 GB
2020-04-08 12:09:43,020 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
2020-04-08 12:09:43,079 INFO org.spark_project.jetty.util.log: Logging initialized @1502ms
2020-04-08 12:09:43,142 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2019-04-30T04:42:08+08:00, git hash: e1bc35120a6617ee3df052294e433f3a25ce7097
2020-04-08 12:09:43,160 INFO org.spark_project.jetty.server.Server: Started @1584ms
2020-04-08 12:09:43,162 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 12:09:43,178 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@71e5f61d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 12:09:43,178 INFO org.apache.spark.util.Utils: Successfully started service 'SparkUI' on port 4040.
2020-04-08 12:09:43,200 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fd62371{/jobs,null,AVAILABLE,@Spark}
2020-04-08 12:09:43,201 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1d9bec4d{/jobs/json,null,AVAILABLE,@Spark}
2020-04-08 12:09:43,201 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5c48c0c0{/jobs/job,null,AVAILABLE,@Spark}
2020-04-08 12:09:43,204 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@25f7391e{/jobs/job/json,null,AVAILABLE,@Spark}
2020-04-08 12:09:43,208 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3f23a3a0{/stages,null,AVAILABLE,@Spark}
2020-04-08 12:09:43,209 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ab14cb9{/stages/json,null,AVAILABLE,@Spark}
2020-04-08 12:09:43,209 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fb97279{/stages/stage,null,AVAILABLE,@Spark}
2020-04-08 12:09:43,211 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@31024624{/stages/stage/json,null,AVAILABLE,@Spark}
2020-04-08 12:09:43,211 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@25bcd0c7{/stages/pool,null,AVAILABLE,@Spark}
2020-04-08 12:09:43,212 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@32cb636e{/stages/pool/json,null,AVAILABLE,@Spark}
2020-04-08 12:09:43,212 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@63cd604c{/storage,null,AVAILABLE,@Spark}
2020-04-08 12:09:43,213 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@40dd3977{/storage/json,null,AVAILABLE,@Spark}
2020-04-08 12:09:43,213 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3a4e343{/storage/rdd,null,AVAILABLE,@Spark}
2020-04-08 12:09:43,214 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a1d204a{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-04-08 12:09:43,214 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62dae245{/environment,null,AVAILABLE,@Spark}
2020-04-08 12:09:43,215 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4b6579e8{/environment/json,null,AVAILABLE,@Spark}
2020-04-08 12:09:43,215 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6fff253c{/executors,null,AVAILABLE,@Spark}
2020-04-08 12:09:43,216 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c6357f9{/executors/json,null,AVAILABLE,@Spark}
2020-04-08 12:09:43,216 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@591e58fa{/executors/threadDump,null,AVAILABLE,@Spark}
2020-04-08 12:09:43,217 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3954d008{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-04-08 12:09:43,223 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f94c4db{/static,null,AVAILABLE,@Spark}
2020-04-08 12:09:43,225 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@669513d8{/,null,AVAILABLE,@Spark}
2020-04-08 12:09:43,226 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3a1d593e{/api,null,AVAILABLE,@Spark}
2020-04-08 12:09:43,226 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@285d851a{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-04-08 12:09:43,227 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@314b8f2d{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-04-08 12:09:43,229 INFO org.apache.spark.ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
2020-04-08 12:09:43,389 INFO org.apache.spark.executor.Executor: Starting executor ID driver on host localhost
2020-04-08 12:09:43,452 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 12:09:43,453 INFO org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52447.
2020-04-08 12:09:43,453 INFO org.apache.spark.network.netty.NettyBlockTransferService: Server created on 192.168.0.102:52447
2020-04-08 12:09:43,454 INFO org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-04-08 12:09:43,475 INFO org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 52447, None)
2020-04-08 12:09:43,477 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:52447 with 4.1 GB RAM, BlockManagerId(driver, 192.168.0.102, 52447, None)
2020-04-08 12:09:43,479 INFO org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 52447, None)
2020-04-08 12:09:43,480 INFO org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 52447, None)
2020-04-08 12:09:43,626 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@49b07ee3{/metrics/json,null,AVAILABLE,@Spark}
2020-04-08 12:09:43,725 WARN org.apache.spark.streaming.StreamingContext: spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
2020-04-08 12:09:43,924 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding enable.auto.commit to false for executor
2020-04-08 12:09:43,925 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding auto.offset.reset to none for executor
2020-04-08 12:09:43,925 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding executor group.id to spark-executor-testgroup1
2020-04-08 12:09:43,926 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
2020-04-08 12:09:44,470 WARN org.apache.kudu.util.NetUtil: Slow DNS lookup! Resolved IP of `cw-instances-2.vpc.cloudera.com' to cw-instances-2.vpc.cloudera.com/10.65.1.47 in 157061786ns
2020-04-08 12:09:44,703 WARN org.apache.kudu.util.NetUtil: Slow DNS lookup! Resolved IP of `cw-instances-3.vpc.cloudera.com' to cw-instances-3.vpc.cloudera.com/10.65.9.108 in 156723537ns
2020-04-08 12:09:44,862 WARN org.apache.kudu.util.NetUtil: Slow DNS lookup! Resolved IP of `cw-instances-4.vpc.cloudera.com' to cw-instances-4.vpc.cloudera.com/10.65.12.223 in 156147627ns
2020-04-08 12:09:46,009 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Slide time = 10000 ms
2020-04-08 12:09:46,009 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Storage level = Serialized 1x Replicated
2020-04-08 12:09:46,010 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Checkpoint interval = null
2020-04-08 12:09:46,010 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Remember interval = 10000 ms
2020-04-08 12:09:46,010 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@6ba9b66d
2020-04-08 12:09:46,011 INFO org.apache.spark.streaming.dstream.ForEachDStream: Slide time = 10000 ms
2020-04-08 12:09:46,011 INFO org.apache.spark.streaming.dstream.ForEachDStream: Storage level = Serialized 1x Replicated
2020-04-08 12:09:46,011 INFO org.apache.spark.streaming.dstream.ForEachDStream: Checkpoint interval = null
2020-04-08 12:09:46,011 INFO org.apache.spark.streaming.dstream.ForEachDStream: Remember interval = 10000 ms
2020-04-08 12:09:46,011 INFO org.apache.spark.streaming.dstream.ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@73c9d3d7
2020-04-08 12:09:46,066 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = testgroup1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-08 12:09:46,129 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-08 12:09:46,129 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-08 12:09:46,129 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586318986128
2020-04-08 12:09:46,131 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-1, groupId=testgroup1] Subscribed to topic(s): PABUSI2
2020-04-08 12:09:46,834 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-1, groupId=testgroup1] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-08 12:09:46,835 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Discovered group coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 12:09:46,839 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Revoking previously assigned partitions []
2020-04-08 12:09:46,839 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 12:09:47,360 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 12:09:50,715 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Successfully joined group with generation 23
2020-04-08 12:09:50,719 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting newly assigned partitions: PABUSI2-2, PABUSI2-0, PABUSI2-1
2020-04-08 12:09:50,903 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-2 to the committed offset FetchPosition{offset=150, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-2.vpc.cloudera.com:9092 (id: 52 rack: null), epoch=0}}
2020-04-08 12:09:50,904 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-0 to the committed offset FetchPosition{offset=153, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-4.vpc.cloudera.com:9092 (id: 53 rack: null), epoch=2}}
2020-04-08 12:09:50,905 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-1 to the committed offset FetchPosition{offset=150, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-3.vpc.cloudera.com:9092 (id: 54 rack: null), epoch=2}}
2020-04-08 12:09:50,916 INFO org.apache.spark.streaming.util.RecurringTimer: Started timer for JobGenerator at time 1586318990000
2020-04-08 12:09:50,917 INFO org.apache.spark.streaming.scheduler.JobGenerator: Started JobGenerator at 1586318990000 ms
2020-04-08 12:09:50,919 INFO org.apache.spark.streaming.scheduler.JobScheduler: Started JobScheduler
2020-04-08 12:09:50,930 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@33a3c44a{/streaming,null,AVAILABLE,@Spark}
2020-04-08 12:09:50,932 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fcfca62{/streaming/json,null,AVAILABLE,@Spark}
2020-04-08 12:09:50,934 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@288214b1{/streaming/batch,null,AVAILABLE,@Spark}
2020-04-08 12:09:50,935 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@16eedaa6{/streaming/batch/json,null,AVAILABLE,@Spark}
2020-04-08 12:09:50,936 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@639aba11{/static/streaming,null,AVAILABLE,@Spark}
2020-04-08 12:09:50,937 INFO org.apache.spark.streaming.StreamingContext: StreamingContext started
2020-04-08 12:09:50,981 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 12:09:50,981 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 12:09:50,981 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 12:09:51,946 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 150.
2020-04-08 12:09:51,947 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 12:09:51,947 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 150.
2020-04-08 12:09:51,970 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586318990000 ms
2020-04-08 12:09:51,973 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586318990000 ms.0 from job set of time 1586318990000 ms
2020-04-08 12:09:52,011 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:141
2020-04-08 12:09:52,026 INFO org.apache.spark.scheduler.DAGScheduler: Got job 0 (collect at Kafka2SparkStreaming2Kudu.scala:141) with 3 output partitions
2020-04-08 12:09:52,026 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 0 (collect at Kafka2SparkStreaming2Kudu.scala:141)
2020-04-08 12:09:52,026 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 12:09:52,027 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 12:09:52,030 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at Kafka2SparkStreaming2Kudu.scala:126), which has no missing parents
2020-04-08 12:09:52,069 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 12:09:52,428 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 12:09:52,429 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:52447 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:09:52,431 INFO org.apache.spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2020-04-08 12:09:52,441 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at Kafka2SparkStreaming2Kudu.scala:126) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 12:09:52,441 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 0.0 with 3 tasks
2020-04-08 12:09:52,466 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:09:52,472 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
2020-04-08 12:09:52,491 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 150 is the same as ending offset skipping PABUSI2 2
2020-04-08 12:09:52,499 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 712 bytes result sent to driver
2020-04-08 12:09:52,500 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:09:52,501 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 0.0 (TID 1)
2020-04-08 12:09:52,503 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 43 ms on localhost (executor driver) (1/3)
2020-04-08 12:09:52,504 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 150 is the same as ending offset skipping PABUSI2 1
2020-04-08 12:09:52,505 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 0.0 (TID 1). 712 bytes result sent to driver
2020-04-08 12:09:52,506 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:09:52,506 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 0.0 (TID 2)
2020-04-08 12:09:52,506 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 6 ms on localhost (executor driver) (2/3)
2020-04-08 12:09:52,509 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 12:09:52,510 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 0.0 (TID 2). 712 bytes result sent to driver
2020-04-08 12:09:52,512 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 6 ms on localhost (executor driver) (3/3)
2020-04-08 12:09:52,513 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-04-08 12:09:52,514 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 0 (collect at Kafka2SparkStreaming2Kudu.scala:141) finished in 0.468 s
2020-04-08 12:09:52,518 INFO org.apache.spark.scheduler.DAGScheduler: Job 0 finished: collect at Kafka2SparkStreaming2Kudu.scala:141, took 0.506504 s
2020-04-08 12:09:52,546 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586318990000 ms.0 from job set of time 1586318990000 ms
2020-04-08 12:09:52,546 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 2.545 s for time 1586318990000 ms (execution: 0.573 s)
2020-04-08 12:09:52,551 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 12:09:52,554 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 
2020-04-08 12:10:00,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 12:10:00,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 12:10:00,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 12:10:00,183 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 151.
2020-04-08 12:10:00,184 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 12:10:00,184 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 150.
2020-04-08 12:10:00,185 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586319000000 ms
2020-04-08 12:10:00,186 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586319000000 ms.0 from job set of time 1586319000000 ms
2020-04-08 12:10:00,193 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:141
2020-04-08 12:10:00,193 INFO org.apache.spark.scheduler.DAGScheduler: Got job 1 (collect at Kafka2SparkStreaming2Kudu.scala:141) with 3 output partitions
2020-04-08 12:10:00,194 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 1 (collect at Kafka2SparkStreaming2Kudu.scala:141)
2020-04-08 12:10:00,194 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 12:10:00,194 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 12:10:00,194 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at map at Kafka2SparkStreaming2Kudu.scala:126), which has no missing parents
2020-04-08 12:10:00,197 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 12:10:00,212 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 12:10:00,213 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.102:52447 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:10:00,214 INFO org.apache.spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2020-04-08 12:10:00,215 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 13
2020-04-08 12:10:00,215 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at map at Kafka2SparkStreaming2Kudu.scala:126) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 12:10:00,215 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 3
2020-04-08 12:10:00,216 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 1.0 with 3 tasks
2020-04-08 12:10:00,216 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 21
2020-04-08 12:10:00,216 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 17
2020-04-08 12:10:00,216 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 9
2020-04-08 12:10:00,216 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 14
2020-04-08 12:10:00,216 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 25
2020-04-08 12:10:00,216 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 18
2020-04-08 12:10:00,216 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 7
2020-04-08 12:10:00,216 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 20
2020-04-08 12:10:00,216 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 4
2020-04-08 12:10:00,216 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 12
2020-04-08 12:10:00,217 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 8
2020-04-08 12:10:00,217 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 15
2020-04-08 12:10:00,217 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 22
2020-04-08 12:10:00,217 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 16
2020-04-08 12:10:00,217 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 23
2020-04-08 12:10:00,217 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 2
2020-04-08 12:10:00,217 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 6
2020-04-08 12:10:00,217 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 10
2020-04-08 12:10:00,217 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 19
2020-04-08 12:10:00,217 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:10:00,217 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 5
2020-04-08 12:10:00,217 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 1
2020-04-08 12:10:00,218 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 1.0 (TID 3)
2020-04-08 12:10:00,221 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 150 is the same as ending offset skipping PABUSI2 1
2020-04-08 12:10:00,223 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 1.0 (TID 3). 712 bytes result sent to driver
2020-04-08 12:10:00,224 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 4, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:10:00,225 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 1.0 (TID 4)
2020-04-08 12:10:00,225 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 3) in 8 ms on localhost (executor driver) (1/3)
2020-04-08 12:10:00,229 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.102:52447 in memory (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:10:00,229 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Computing topic PABUSI2, partition 2 offsets 150 -> 151
2020-04-08 12:10:00,233 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 11
2020-04-08 12:10:00,234 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 24
2020-04-08 12:10:00,234 INFO org.apache.spark.streaming.kafka010.KafkaDataConsumer: Initializing cache 16 64 0.75
2020-04-08 12:10:00,238 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-testgroup1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-08 12:10:00,244 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-08 12:10:00,245 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-08 12:10:00,245 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586319000244
2020-04-08 12:10:00,246 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Subscribed to partition(s): PABUSI2-2
2020-04-08 12:10:00,249 INFO org.apache.spark.streaming.kafka010.InternalKafkaConsumer: Initial fetch for spark-executor-testgroup1 PABUSI2-2 150
2020-04-08 12:10:00,250 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Seeking to offset 150 for partition PABUSI2-2
2020-04-08 12:10:00,778 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-08 12:10:01,466 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 1.0 (TID 4). 2446 bytes result sent to driver
2020-04-08 12:10:01,467 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 1.0 (TID 5, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:10:01,468 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 1.0 (TID 5)
2020-04-08 12:10:01,468 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 4) in 1244 ms on localhost (executor driver) (2/3)
2020-04-08 12:10:01,470 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 12:10:01,470 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 1.0 (TID 5). 712 bytes result sent to driver
2020-04-08 12:10:01,471 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 1.0 (TID 5) in 4 ms on localhost (executor driver) (3/3)
2020-04-08 12:10:01,471 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
2020-04-08 12:10:01,472 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 1 (collect at Kafka2SparkStreaming2Kudu.scala:141) finished in 1.276 s
2020-04-08 12:10:01,472 INFO org.apache.spark.scheduler.DAGScheduler: Job 1 finished: collect at Kafka2SparkStreaming2Kudu.scala:141, took 1.279192 s
2020-04-08 12:10:01,478 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586319000000 ms.0 from job set of time 1586319000000 ms
2020-04-08 12:10:01,478 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 1.478 s for time 1586319000000 ms (execution: 1.292 s)
2020-04-08 12:10:01,479 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 0 from persistence list
2020-04-08 12:10:01,481 INFO org.apache.spark.storage.BlockManager: Removing RDD 0
2020-04-08 12:10:01,482 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 12:10:01,482 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 
2020-04-08 12:10:10,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 12:10:10,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 12:10:10,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 12:10:10,187 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 151.
2020-04-08 12:10:10,188 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 150.
2020-04-08 12:10:10,188 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 12:10:10,189 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586319010000 ms
2020-04-08 12:10:10,189 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586319010000 ms.0 from job set of time 1586319010000 ms
2020-04-08 12:10:10,195 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:141
2020-04-08 12:10:10,196 INFO org.apache.spark.scheduler.DAGScheduler: Got job 2 (collect at Kafka2SparkStreaming2Kudu.scala:141) with 3 output partitions
2020-04-08 12:10:10,196 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 2 (collect at Kafka2SparkStreaming2Kudu.scala:141)
2020-04-08 12:10:10,196 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 12:10:10,196 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 12:10:10,197 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[7] at map at Kafka2SparkStreaming2Kudu.scala:126), which has no missing parents
2020-04-08 12:10:10,199 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 12:10:10,201 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 12:10:10,202 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.102:52447 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:10:10,202 INFO org.apache.spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2020-04-08 12:10:10,203 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at map at Kafka2SparkStreaming2Kudu.scala:126) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 12:10:10,203 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 2.0 with 3 tasks
2020-04-08 12:10:10,204 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:10:10,204 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 2.0 (TID 6)
2020-04-08 12:10:10,206 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 150 is the same as ending offset skipping PABUSI2 1
2020-04-08 12:10:10,208 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 2.0 (TID 6). 712 bytes result sent to driver
2020-04-08 12:10:10,209 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 2.0 (TID 7, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:10:10,209 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 2.0 (TID 7)
2020-04-08 12:10:10,209 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 6) in 5 ms on localhost (executor driver) (1/3)
2020-04-08 12:10:10,213 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 2
2020-04-08 12:10:10,215 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 2.0 (TID 7). 712 bytes result sent to driver
2020-04-08 12:10:10,216 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 2.0 (TID 8, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:10:10,216 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 2.0 (TID 8)
2020-04-08 12:10:10,216 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 2.0 (TID 7) in 8 ms on localhost (executor driver) (2/3)
2020-04-08 12:10:10,219 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 12:10:10,221 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 2.0 (TID 8). 712 bytes result sent to driver
2020-04-08 12:10:10,222 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 2.0 (TID 8) in 6 ms on localhost (executor driver) (3/3)
2020-04-08 12:10:10,223 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
2020-04-08 12:10:10,224 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 2 (collect at Kafka2SparkStreaming2Kudu.scala:141) finished in 0.026 s
2020-04-08 12:10:10,225 INFO org.apache.spark.scheduler.DAGScheduler: Job 2 finished: collect at Kafka2SparkStreaming2Kudu.scala:141, took 0.029621 s
2020-04-08 12:10:10,234 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586319010000 ms.0 from job set of time 1586319010000 ms
2020-04-08 12:10:10,234 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 3 from persistence list
2020-04-08 12:10:10,234 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.234 s for time 1586319010000 ms (execution: 0.045 s)
2020-04-08 12:10:10,235 INFO org.apache.spark.storage.BlockManager: Removing RDD 3
2020-04-08 12:10:10,235 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 12:10:10,235 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586318990000 ms
2020-04-08 12:10:20,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 12:10:20,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 12:10:20,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 12:10:20,186 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 12:10:20,186 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 150.
2020-04-08 12:10:20,186 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 151.
2020-04-08 12:10:20,187 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586319020000 ms
2020-04-08 12:10:20,188 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586319020000 ms.0 from job set of time 1586319020000 ms
2020-04-08 12:10:20,193 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:141
2020-04-08 12:10:20,194 INFO org.apache.spark.scheduler.DAGScheduler: Got job 3 (collect at Kafka2SparkStreaming2Kudu.scala:141) with 3 output partitions
2020-04-08 12:10:20,194 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 3 (collect at Kafka2SparkStreaming2Kudu.scala:141)
2020-04-08 12:10:20,194 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 12:10:20,194 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 12:10:20,195 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[10] at map at Kafka2SparkStreaming2Kudu.scala:126), which has no missing parents
2020-04-08 12:10:20,198 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 12:10:20,199 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 12:10:20,200 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.102:52447 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:10:20,200 INFO org.apache.spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2020-04-08 12:10:20,201 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 3 (MapPartitionsRDD[10] at map at Kafka2SparkStreaming2Kudu.scala:126) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 12:10:20,201 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 3.0 with 3 tasks
2020-04-08 12:10:20,202 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 9, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:10:20,202 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 3.0 (TID 9)
2020-04-08 12:10:20,204 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 150 is the same as ending offset skipping PABUSI2 1
2020-04-08 12:10:20,205 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 3.0 (TID 9). 669 bytes result sent to driver
2020-04-08 12:10:20,205 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 3.0 (TID 10, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:10:20,206 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 3.0 (TID 10)
2020-04-08 12:10:20,206 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 9) in 4 ms on localhost (executor driver) (1/3)
2020-04-08 12:10:20,207 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 2
2020-04-08 12:10:20,208 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 3.0 (TID 10). 669 bytes result sent to driver
2020-04-08 12:10:20,208 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 3.0 (TID 11, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:10:20,209 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 3.0 (TID 11)
2020-04-08 12:10:20,209 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 3.0 (TID 10) in 4 ms on localhost (executor driver) (2/3)
2020-04-08 12:10:20,211 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 12:10:20,212 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 3.0 (TID 11). 712 bytes result sent to driver
2020-04-08 12:10:20,213 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 3.0 (TID 11) in 5 ms on localhost (executor driver) (3/3)
2020-04-08 12:10:20,213 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
2020-04-08 12:10:20,214 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 3 (collect at Kafka2SparkStreaming2Kudu.scala:141) finished in 0.018 s
2020-04-08 12:10:20,215 INFO org.apache.spark.scheduler.DAGScheduler: Job 3 finished: collect at Kafka2SparkStreaming2Kudu.scala:141, took 0.021298 s
2020-04-08 12:10:20,223 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586319020000 ms.0 from job set of time 1586319020000 ms
2020-04-08 12:10:20,224 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.223 s for time 1586319020000 ms (execution: 0.035 s)
2020-04-08 12:10:20,224 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 6 from persistence list
2020-04-08 12:10:20,226 INFO org.apache.spark.storage.BlockManager: Removing RDD 6
2020-04-08 12:10:20,227 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 12:10:20,227 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586319000000 ms
2020-04-08 12:10:30,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 12:10:30,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 12:10:30,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 12:10:30,196 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 151.
2020-04-08 12:10:30,197 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 150.
2020-04-08 12:10:30,197 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 12:10:30,198 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586319030000 ms
2020-04-08 12:10:30,198 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586319030000 ms.0 from job set of time 1586319030000 ms
2020-04-08 12:10:30,203 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:141
2020-04-08 12:10:30,204 INFO org.apache.spark.scheduler.DAGScheduler: Got job 4 (collect at Kafka2SparkStreaming2Kudu.scala:141) with 3 output partitions
2020-04-08 12:10:30,204 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 4 (collect at Kafka2SparkStreaming2Kudu.scala:141)
2020-04-08 12:10:30,204 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 12:10:30,204 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 12:10:30,205 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[13] at map at Kafka2SparkStreaming2Kudu.scala:126), which has no missing parents
2020-04-08 12:10:30,207 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 12:10:30,212 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 12:10:30,213 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.102:52447 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:10:30,215 INFO org.apache.spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1161
2020-04-08 12:10:30,217 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 4 (MapPartitionsRDD[13] at map at Kafka2SparkStreaming2Kudu.scala:126) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 12:10:30,217 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 4.0 with 3 tasks
2020-04-08 12:10:30,219 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 12, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:10:30,219 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 4.0 (TID 12)
2020-04-08 12:10:30,222 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 150 is the same as ending offset skipping PABUSI2 1
2020-04-08 12:10:30,223 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 4.0 (TID 12). 712 bytes result sent to driver
2020-04-08 12:10:30,223 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 4.0 (TID 13, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:10:30,223 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 4.0 (TID 13)
2020-04-08 12:10:30,223 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 12) in 4 ms on localhost (executor driver) (1/3)
2020-04-08 12:10:30,225 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 2
2020-04-08 12:10:30,226 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 4.0 (TID 13). 669 bytes result sent to driver
2020-04-08 12:10:30,226 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 4.0 (TID 14, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:10:30,226 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 4.0 (TID 13) in 3 ms on localhost (executor driver) (2/3)
2020-04-08 12:10:30,226 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 4.0 (TID 14)
2020-04-08 12:10:30,228 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 12:10:30,229 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 4.0 (TID 14). 669 bytes result sent to driver
2020-04-08 12:10:30,229 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 4.0 (TID 14) in 3 ms on localhost (executor driver) (3/3)
2020-04-08 12:10:30,229 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
2020-04-08 12:10:30,230 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 4 (collect at Kafka2SparkStreaming2Kudu.scala:141) finished in 0.023 s
2020-04-08 12:10:30,230 INFO org.apache.spark.scheduler.DAGScheduler: Job 4 finished: collect at Kafka2SparkStreaming2Kudu.scala:141, took 0.026465 s
2020-04-08 12:10:30,235 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586319030000 ms.0 from job set of time 1586319030000 ms
2020-04-08 12:10:30,235 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.235 s for time 1586319030000 ms (execution: 0.037 s)
2020-04-08 12:10:30,235 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 9 from persistence list
2020-04-08 12:10:30,236 INFO org.apache.spark.storage.BlockManager: Removing RDD 9
2020-04-08 12:10:30,236 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 12:10:30,236 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586319010000 ms
2020-04-08 12:10:40,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 12:10:40,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 12:10:40,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 12:10:40,182 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 150.
2020-04-08 12:10:40,182 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 151.
2020-04-08 12:10:40,182 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 12:10:40,183 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586319040000 ms
2020-04-08 12:10:40,183 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586319040000 ms.0 from job set of time 1586319040000 ms
2020-04-08 12:10:40,189 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:141
2020-04-08 12:10:40,190 INFO org.apache.spark.scheduler.DAGScheduler: Got job 5 (collect at Kafka2SparkStreaming2Kudu.scala:141) with 3 output partitions
2020-04-08 12:10:40,190 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 5 (collect at Kafka2SparkStreaming2Kudu.scala:141)
2020-04-08 12:10:40,190 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 12:10:40,190 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 12:10:40,191 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[16] at map at Kafka2SparkStreaming2Kudu.scala:126), which has no missing parents
2020-04-08 12:10:40,193 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 12:10:40,195 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 12:10:40,196 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.102:52447 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:10:40,197 INFO org.apache.spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1161
2020-04-08 12:10:40,198 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 5 (MapPartitionsRDD[16] at map at Kafka2SparkStreaming2Kudu.scala:126) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 12:10:40,198 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 5.0 with 3 tasks
2020-04-08 12:10:40,199 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 15, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:10:40,199 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 5.0 (TID 15)
2020-04-08 12:10:40,202 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 150 is the same as ending offset skipping PABUSI2 1
2020-04-08 12:10:40,203 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 5.0 (TID 15). 712 bytes result sent to driver
2020-04-08 12:10:40,203 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 5.0 (TID 16, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:10:40,204 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 5.0 (TID 16)
2020-04-08 12:10:40,204 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 15) in 5 ms on localhost (executor driver) (1/3)
2020-04-08 12:10:40,205 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 2
2020-04-08 12:10:40,206 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 5.0 (TID 16). 669 bytes result sent to driver
2020-04-08 12:10:40,207 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 5.0 (TID 17, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:10:40,207 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 5.0 (TID 17)
2020-04-08 12:10:40,207 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 5.0 (TID 16) in 4 ms on localhost (executor driver) (2/3)
2020-04-08 12:10:40,209 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 12:10:40,210 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 5.0 (TID 17). 669 bytes result sent to driver
2020-04-08 12:10:40,211 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 5.0 (TID 17) in 4 ms on localhost (executor driver) (3/3)
2020-04-08 12:10:40,211 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
2020-04-08 12:10:40,212 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 5 (collect at Kafka2SparkStreaming2Kudu.scala:141) finished in 0.020 s
2020-04-08 12:10:40,213 INFO org.apache.spark.scheduler.DAGScheduler: Job 5 finished: collect at Kafka2SparkStreaming2Kudu.scala:141, took 0.023855 s
2020-04-08 12:10:40,224 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586319040000 ms.0 from job set of time 1586319040000 ms
2020-04-08 12:10:40,225 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 12 from persistence list
2020-04-08 12:10:40,225 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.224 s for time 1586319040000 ms (execution: 0.041 s)
2020-04-08 12:10:40,225 INFO org.apache.spark.storage.BlockManager: Removing RDD 12
2020-04-08 12:10:40,225 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 12:10:40,226 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586319020000 ms
2020-04-08 12:10:47,934 INFO org.apache.spark.streaming.StreamingContext: Invoking stop(stopGracefully=false) from shutdown hook
2020-04-08 12:10:47,939 INFO org.apache.spark.streaming.scheduler.ReceiverTracker: ReceiverTracker stopped
2020-04-08 12:10:47,941 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopping JobGenerator immediately
2020-04-08 12:10:47,942 INFO org.apache.spark.streaming.util.RecurringTimer: Stopped timer for JobGenerator after time 1586319040000
2020-04-08 12:10:48,128 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Member consumer-1-240ea950-ee2b-4ccf-a387-1076b6b5aae2 sending LeaveGroup request to coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 12:10:48,311 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopped JobGenerator
2020-04-08 12:10:48,314 INFO org.apache.spark.streaming.scheduler.JobScheduler: Stopped JobScheduler
2020-04-08 12:10:48,317 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@33a3c44a{/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 12:10:48,317 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@288214b1{/streaming/batch,null,UNAVAILABLE,@Spark}
2020-04-08 12:10:48,318 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@639aba11{/static/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 12:10:48,318 INFO org.apache.spark.streaming.StreamingContext: StreamingContext stopped successfully
2020-04-08 12:10:48,318 INFO org.apache.spark.SparkContext: Invoking stop() from shutdown hook
2020-04-08 12:10:48,322 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@71e5f61d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 12:10:48,323 INFO org.apache.spark.ui.SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
2020-04-08 12:10:48,328 INFO org.apache.spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2020-04-08 12:10:48,346 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2020-04-08 12:10:48,348 INFO org.apache.spark.storage.BlockManager: BlockManager stopped
2020-04-08 12:10:48,349 INFO org.apache.spark.storage.BlockManagerMaster: BlockManagerMaster stopped
2020-04-08 12:10:48,351 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2020-04-08 12:10:48,362 INFO org.apache.spark.SparkContext: Successfully stopped SparkContext
2020-04-08 12:10:48,362 INFO org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2020-04-08 12:10:48,363 INFO org.apache.spark.util.ShutdownHookManager: Deleting directory /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/spark-9e7f5599-1320-4705-847f-508832ac5e26
2020-04-08 12:10:59,948 WARN org.apache.spark.util.Utils: Your hostname, MacBook-Pro-CW.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)
2020-04-08 12:10:59,949 WARN org.apache.spark.util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
2020-04-08 12:11:00,153 INFO org.apache.spark.SparkContext: Running Spark version 2.4.0.cloudera2
2020-04-08 12:11:00,334 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-04-08 12:11:00,467 INFO org.apache.spark.SparkContext: Submitted application: Kafka2SparkStreaming2Kudu-kerberos
2020-04-08 12:11:00,512 INFO org.apache.spark.SecurityManager: Changing view acls to: godiswc
2020-04-08 12:11:00,512 INFO org.apache.spark.SecurityManager: Changing modify acls to: godiswc
2020-04-08 12:11:00,513 INFO org.apache.spark.SecurityManager: Changing view acls groups to: 
2020-04-08 12:11:00,513 INFO org.apache.spark.SecurityManager: Changing modify acls groups to: 
2020-04-08 12:11:00,513 INFO org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(godiswc); groups with view permissions: Set(); users  with modify permissions: Set(godiswc); groups with modify permissions: Set()
2020-04-08 12:11:00,649 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 12:11:00,747 INFO org.apache.spark.util.Utils: Successfully started service 'sparkDriver' on port 52740.
2020-04-08 12:11:00,766 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
2020-04-08 12:11:00,779 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
2020-04-08 12:11:00,780 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-04-08 12:11:00,781 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2020-04-08 12:11:00,812 INFO org.apache.spark.storage.DiskBlockManager: Created local directory at /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/blockmgr-32beabe6-22f8-4299-bea3-31cad32a9033
2020-04-08 12:11:00,826 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 4.1 GB
2020-04-08 12:11:00,836 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
2020-04-08 12:11:00,889 INFO org.spark_project.jetty.util.log: Logging initialized @1541ms
2020-04-08 12:11:00,923 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2019-04-30T04:42:08+08:00, git hash: e1bc35120a6617ee3df052294e433f3a25ce7097
2020-04-08 12:11:00,934 INFO org.spark_project.jetty.server.Server: Started @1588ms
2020-04-08 12:11:00,936 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 12:11:00,946 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@71e5f61d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 12:11:00,946 INFO org.apache.spark.util.Utils: Successfully started service 'SparkUI' on port 4040.
2020-04-08 12:11:00,968 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fd62371{/jobs,null,AVAILABLE,@Spark}
2020-04-08 12:11:00,969 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1d9bec4d{/jobs/json,null,AVAILABLE,@Spark}
2020-04-08 12:11:00,969 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5c48c0c0{/jobs/job,null,AVAILABLE,@Spark}
2020-04-08 12:11:00,972 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@25f7391e{/jobs/job/json,null,AVAILABLE,@Spark}
2020-04-08 12:11:00,976 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3f23a3a0{/stages,null,AVAILABLE,@Spark}
2020-04-08 12:11:00,976 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ab14cb9{/stages/json,null,AVAILABLE,@Spark}
2020-04-08 12:11:00,977 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fb97279{/stages/stage,null,AVAILABLE,@Spark}
2020-04-08 12:11:00,979 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@31024624{/stages/stage/json,null,AVAILABLE,@Spark}
2020-04-08 12:11:00,980 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@25bcd0c7{/stages/pool,null,AVAILABLE,@Spark}
2020-04-08 12:11:00,980 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@32cb636e{/stages/pool/json,null,AVAILABLE,@Spark}
2020-04-08 12:11:00,981 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@63cd604c{/storage,null,AVAILABLE,@Spark}
2020-04-08 12:11:00,982 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@40dd3977{/storage/json,null,AVAILABLE,@Spark}
2020-04-08 12:11:00,983 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3a4e343{/storage/rdd,null,AVAILABLE,@Spark}
2020-04-08 12:11:00,984 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a1d204a{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-04-08 12:11:00,985 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62dae245{/environment,null,AVAILABLE,@Spark}
2020-04-08 12:11:00,986 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4b6579e8{/environment/json,null,AVAILABLE,@Spark}
2020-04-08 12:11:00,986 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6fff253c{/executors,null,AVAILABLE,@Spark}
2020-04-08 12:11:00,987 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c6357f9{/executors/json,null,AVAILABLE,@Spark}
2020-04-08 12:11:00,987 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@591e58fa{/executors/threadDump,null,AVAILABLE,@Spark}
2020-04-08 12:11:00,988 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3954d008{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-04-08 12:11:00,995 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f94c4db{/static,null,AVAILABLE,@Spark}
2020-04-08 12:11:00,995 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@669513d8{/,null,AVAILABLE,@Spark}
2020-04-08 12:11:00,996 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3a1d593e{/api,null,AVAILABLE,@Spark}
2020-04-08 12:11:00,998 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@285d851a{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-04-08 12:11:00,998 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@314b8f2d{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-04-08 12:11:01,001 INFO org.apache.spark.ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
2020-04-08 12:11:01,130 INFO org.apache.spark.executor.Executor: Starting executor ID driver on host localhost
2020-04-08 12:11:01,185 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 12:11:01,186 INFO org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52741.
2020-04-08 12:11:01,187 INFO org.apache.spark.network.netty.NettyBlockTransferService: Server created on 192.168.0.102:52741
2020-04-08 12:11:01,188 INFO org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-04-08 12:11:01,215 INFO org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 52741, None)
2020-04-08 12:11:01,218 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:52741 with 4.1 GB RAM, BlockManagerId(driver, 192.168.0.102, 52741, None)
2020-04-08 12:11:01,220 INFO org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 52741, None)
2020-04-08 12:11:01,221 INFO org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 52741, None)
2020-04-08 12:11:01,407 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@49b07ee3{/metrics/json,null,AVAILABLE,@Spark}
2020-04-08 12:11:01,503 WARN org.apache.spark.streaming.StreamingContext: spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
2020-04-08 12:11:01,693 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding enable.auto.commit to false for executor
2020-04-08 12:11:01,694 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding auto.offset.reset to none for executor
2020-04-08 12:11:01,695 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding executor group.id to spark-executor-testgroup1
2020-04-08 12:11:01,695 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
2020-04-08 12:11:03,502 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Slide time = 10000 ms
2020-04-08 12:11:03,503 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Storage level = Serialized 1x Replicated
2020-04-08 12:11:03,503 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Checkpoint interval = null
2020-04-08 12:11:03,503 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Remember interval = 10000 ms
2020-04-08 12:11:03,504 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@3e99f3de
2020-04-08 12:11:03,504 INFO org.apache.spark.streaming.dstream.ForEachDStream: Slide time = 10000 ms
2020-04-08 12:11:03,504 INFO org.apache.spark.streaming.dstream.ForEachDStream: Storage level = Serialized 1x Replicated
2020-04-08 12:11:03,504 INFO org.apache.spark.streaming.dstream.ForEachDStream: Checkpoint interval = null
2020-04-08 12:11:03,504 INFO org.apache.spark.streaming.dstream.ForEachDStream: Remember interval = 10000 ms
2020-04-08 12:11:03,504 INFO org.apache.spark.streaming.dstream.ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@740573fb
2020-04-08 12:11:03,567 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = testgroup1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-08 12:11:03,637 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-08 12:11:03,637 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-08 12:11:03,637 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586319063636
2020-04-08 12:11:03,639 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-1, groupId=testgroup1] Subscribed to topic(s): PABUSI2
2020-04-08 12:11:04,468 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-1, groupId=testgroup1] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-08 12:11:04,469 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Discovered group coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 12:11:04,472 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Revoking previously assigned partitions []
2020-04-08 12:11:04,473 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 12:11:04,997 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 12:11:08,393 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Successfully joined group with generation 25
2020-04-08 12:11:08,396 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting newly assigned partitions: PABUSI2-2, PABUSI2-0, PABUSI2-1
2020-04-08 12:11:08,586 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-2 to the committed offset FetchPosition{offset=151, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-2.vpc.cloudera.com:9092 (id: 52 rack: null), epoch=0}}
2020-04-08 12:11:08,587 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-0 to the committed offset FetchPosition{offset=153, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-4.vpc.cloudera.com:9092 (id: 53 rack: null), epoch=2}}
2020-04-08 12:11:08,587 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-1 to the committed offset FetchPosition{offset=150, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-3.vpc.cloudera.com:9092 (id: 54 rack: null), epoch=2}}
2020-04-08 12:11:08,596 INFO org.apache.spark.streaming.util.RecurringTimer: Started timer for JobGenerator at time 1586319070000
2020-04-08 12:11:08,597 INFO org.apache.spark.streaming.scheduler.JobGenerator: Started JobGenerator at 1586319070000 ms
2020-04-08 12:11:08,598 INFO org.apache.spark.streaming.scheduler.JobScheduler: Started JobScheduler
2020-04-08 12:11:08,602 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@33a3c44a{/streaming,null,AVAILABLE,@Spark}
2020-04-08 12:11:08,603 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fcfca62{/streaming/json,null,AVAILABLE,@Spark}
2020-04-08 12:11:08,605 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@288214b1{/streaming/batch,null,AVAILABLE,@Spark}
2020-04-08 12:11:08,605 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@16eedaa6{/streaming/batch/json,null,AVAILABLE,@Spark}
2020-04-08 12:11:08,607 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@639aba11{/static/streaming,null,AVAILABLE,@Spark}
2020-04-08 12:11:08,607 INFO org.apache.spark.streaming.StreamingContext: StreamingContext started
2020-04-08 12:11:10,045 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 12:11:10,046 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 12:11:10,046 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 12:11:10,234 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 151.
2020-04-08 12:11:10,234 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 151.
2020-04-08 12:11:10,234 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 12:11:10,259 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586319070000 ms
2020-04-08 12:11:10,262 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586319070000 ms.0 from job set of time 1586319070000 ms
2020-04-08 12:11:10,309 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:141
2020-04-08 12:11:10,326 INFO org.apache.spark.scheduler.DAGScheduler: Got job 0 (collect at Kafka2SparkStreaming2Kudu.scala:141) with 3 output partitions
2020-04-08 12:11:10,327 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 0 (collect at Kafka2SparkStreaming2Kudu.scala:141)
2020-04-08 12:11:10,327 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 12:11:10,328 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 12:11:10,333 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at Kafka2SparkStreaming2Kudu.scala:126), which has no missing parents
2020-04-08 12:11:10,380 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 12:11:10,805 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 12:11:10,806 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:52741 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:11:10,808 INFO org.apache.spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2020-04-08 12:11:10,820 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at Kafka2SparkStreaming2Kudu.scala:126) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 12:11:10,821 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 0.0 with 3 tasks
2020-04-08 12:11:10,856 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:11:10,863 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
2020-04-08 12:11:10,886 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Computing topic PABUSI2, partition 1 offsets 150 -> 151
2020-04-08 12:11:10,890 INFO org.apache.spark.streaming.kafka010.KafkaDataConsumer: Initializing cache 16 64 0.75
2020-04-08 12:11:10,892 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-testgroup1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-08 12:11:10,897 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-08 12:11:10,897 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-08 12:11:10,897 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586319070897
2020-04-08 12:11:10,898 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Subscribed to partition(s): PABUSI2-1
2020-04-08 12:11:10,900 INFO org.apache.spark.streaming.kafka010.InternalKafkaConsumer: Initial fetch for spark-executor-testgroup1 PABUSI2-1 150
2020-04-08 12:11:10,901 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Seeking to offset 150 for partition PABUSI2-1
2020-04-08 12:11:11,429 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-08 12:11:12,164 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 5665 bytes result sent to driver
2020-04-08 12:11:12,166 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:11:12,167 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 0.0 (TID 1)
2020-04-08 12:11:12,170 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1322 ms on localhost (executor driver) (1/3)
2020-04-08 12:11:12,170 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 2
2020-04-08 12:11:12,171 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 0.0 (TID 1). 765 bytes result sent to driver
2020-04-08 12:11:12,172 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:11:12,172 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 0.0 (TID 2)
2020-04-08 12:11:12,172 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 6 ms on localhost (executor driver) (2/3)
2020-04-08 12:11:12,175 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 12:11:12,176 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 0.0 (TID 2). 722 bytes result sent to driver
2020-04-08 12:11:12,178 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 6 ms on localhost (executor driver) (3/3)
2020-04-08 12:11:12,179 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-04-08 12:11:12,180 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 0 (collect at Kafka2SparkStreaming2Kudu.scala:141) finished in 1.827 s
2020-04-08 12:11:12,184 INFO org.apache.spark.scheduler.DAGScheduler: Job 0 finished: collect at Kafka2SparkStreaming2Kudu.scala:141, took 1.874245 s
2020-04-08 12:11:12,206 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586319070000 ms.0 from job set of time 1586319070000 ms
2020-04-08 12:11:12,207 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 2.206 s for time 1586319070000 ms (execution: 1.945 s)
2020-04-08 12:11:12,211 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 12:11:12,213 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 
2020-04-08 12:11:20,009 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 12:11:20,009 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 12:11:20,010 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 12:11:20,192 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 151.
2020-04-08 12:11:20,193 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 151.
2020-04-08 12:11:20,194 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 12:11:20,199 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586319080000 ms
2020-04-08 12:11:20,201 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586319080000 ms.0 from job set of time 1586319080000 ms
2020-04-08 12:11:20,237 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:141
2020-04-08 12:11:20,241 INFO org.apache.spark.scheduler.DAGScheduler: Got job 1 (collect at Kafka2SparkStreaming2Kudu.scala:141) with 3 output partitions
2020-04-08 12:11:20,241 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 1 (collect at Kafka2SparkStreaming2Kudu.scala:141)
2020-04-08 12:11:20,242 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 12:11:20,242 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 12:11:20,245 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at map at Kafka2SparkStreaming2Kudu.scala:126), which has no missing parents
2020-04-08 12:11:20,258 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 12:11:20,266 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 12:11:20,270 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.102:52741 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:11:20,272 INFO org.apache.spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2020-04-08 12:11:20,274 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at map at Kafka2SparkStreaming2Kudu.scala:126) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 12:11:20,275 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 1.0 with 3 tasks
2020-04-08 12:11:20,282 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:11:20,284 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 1.0 (TID 3)
2020-04-08 12:11:20,296 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 1
2020-04-08 12:11:20,301 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 1.0 (TID 3). 722 bytes result sent to driver
2020-04-08 12:11:20,304 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 4, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:11:20,306 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 1.0 (TID 4)
2020-04-08 12:11:20,306 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 3) in 26 ms on localhost (executor driver) (1/3)
2020-04-08 12:11:20,321 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 2
2020-04-08 12:11:20,326 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 1.0 (TID 4). 722 bytes result sent to driver
2020-04-08 12:11:20,330 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 1.0 (TID 5, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:11:20,332 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 1.0 (TID 5)
2020-04-08 12:11:20,332 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 4) in 29 ms on localhost (executor driver) (2/3)
2020-04-08 12:11:20,341 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 12:11:20,343 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 1.0 (TID 5). 722 bytes result sent to driver
2020-04-08 12:11:20,345 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 1.0 (TID 5) in 15 ms on localhost (executor driver) (3/3)
2020-04-08 12:11:20,347 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
2020-04-08 12:11:20,349 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 1 (collect at Kafka2SparkStreaming2Kudu.scala:141) finished in 0.097 s
2020-04-08 12:11:20,350 INFO org.apache.spark.scheduler.DAGScheduler: Job 1 finished: collect at Kafka2SparkStreaming2Kudu.scala:141, took 0.111580 s
2020-04-08 12:11:20,373 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586319080000 ms.0 from job set of time 1586319080000 ms
2020-04-08 12:11:20,374 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.373 s for time 1586319080000 ms (execution: 0.172 s)
2020-04-08 12:11:20,374 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 0 from persistence list
2020-04-08 12:11:20,379 INFO org.apache.spark.storage.BlockManager: Removing RDD 0
2020-04-08 12:11:20,380 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 12:11:20,380 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 
2020-04-08 12:11:30,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 12:11:30,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 12:11:30,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 12:11:30,191 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 151.
2020-04-08 12:11:30,192 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 12:11:30,193 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 151.
2020-04-08 12:11:30,194 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586319090000 ms
2020-04-08 12:11:30,194 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586319090000 ms.0 from job set of time 1586319090000 ms
2020-04-08 12:11:30,201 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:141
2020-04-08 12:11:30,202 INFO org.apache.spark.scheduler.DAGScheduler: Got job 2 (collect at Kafka2SparkStreaming2Kudu.scala:141) with 3 output partitions
2020-04-08 12:11:30,202 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 2 (collect at Kafka2SparkStreaming2Kudu.scala:141)
2020-04-08 12:11:30,202 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 12:11:30,202 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 12:11:30,203 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[7] at map at Kafka2SparkStreaming2Kudu.scala:126), which has no missing parents
2020-04-08 12:11:30,205 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 12:11:30,207 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 12:11:30,210 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.102:52741 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:11:30,211 INFO org.apache.spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2020-04-08 12:11:30,212 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at map at Kafka2SparkStreaming2Kudu.scala:126) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 12:11:30,212 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 2.0 with 3 tasks
2020-04-08 12:11:30,213 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:11:30,214 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 2.0 (TID 6)
2020-04-08 12:11:30,216 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 1
2020-04-08 12:11:30,218 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 2.0 (TID 6). 722 bytes result sent to driver
2020-04-08 12:11:30,220 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 2.0 (TID 7, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:11:30,221 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 2.0 (TID 7)
2020-04-08 12:11:30,221 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 6) in 8 ms on localhost (executor driver) (1/3)
2020-04-08 12:11:30,224 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 2
2020-04-08 12:11:30,225 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 2.0 (TID 7). 722 bytes result sent to driver
2020-04-08 12:11:30,226 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 2.0 (TID 8, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:11:30,227 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 2.0 (TID 8)
2020-04-08 12:11:30,227 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 2.0 (TID 7) in 7 ms on localhost (executor driver) (2/3)
2020-04-08 12:11:30,228 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 12:11:30,229 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 2.0 (TID 8). 679 bytes result sent to driver
2020-04-08 12:11:30,230 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 2.0 (TID 8) in 4 ms on localhost (executor driver) (3/3)
2020-04-08 12:11:30,230 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
2020-04-08 12:11:30,230 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 2 (collect at Kafka2SparkStreaming2Kudu.scala:141) finished in 0.026 s
2020-04-08 12:11:30,231 INFO org.apache.spark.scheduler.DAGScheduler: Job 2 finished: collect at Kafka2SparkStreaming2Kudu.scala:141, took 0.029632 s
2020-04-08 12:11:30,236 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586319090000 ms.0 from job set of time 1586319090000 ms
2020-04-08 12:11:30,237 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.236 s for time 1586319090000 ms (execution: 0.042 s)
2020-04-08 12:11:30,237 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 3 from persistence list
2020-04-08 12:11:30,237 INFO org.apache.spark.storage.BlockManager: Removing RDD 3
2020-04-08 12:11:30,237 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 12:11:30,238 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586319070000 ms
2020-04-08 12:11:40,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 12:11:40,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 12:11:40,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 12:11:40,185 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 12:11:40,187 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 151.
2020-04-08 12:11:40,187 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 151.
2020-04-08 12:11:40,188 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586319100000 ms
2020-04-08 12:11:40,188 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586319100000 ms.0 from job set of time 1586319100000 ms
2020-04-08 12:11:40,194 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:141
2020-04-08 12:11:40,195 INFO org.apache.spark.scheduler.DAGScheduler: Got job 3 (collect at Kafka2SparkStreaming2Kudu.scala:141) with 3 output partitions
2020-04-08 12:11:40,195 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 3 (collect at Kafka2SparkStreaming2Kudu.scala:141)
2020-04-08 12:11:40,195 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 12:11:40,195 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 12:11:40,196 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[10] at map at Kafka2SparkStreaming2Kudu.scala:126), which has no missing parents
2020-04-08 12:11:40,198 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 12:11:40,200 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 12:11:40,200 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.102:52741 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:11:40,201 INFO org.apache.spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2020-04-08 12:11:40,202 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 3 (MapPartitionsRDD[10] at map at Kafka2SparkStreaming2Kudu.scala:126) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 12:11:40,202 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 3.0 with 3 tasks
2020-04-08 12:11:40,203 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 9, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:11:40,203 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 3.0 (TID 9)
2020-04-08 12:11:40,204 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 1
2020-04-08 12:11:40,205 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 3.0 (TID 9). 679 bytes result sent to driver
2020-04-08 12:11:40,206 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 3.0 (TID 10, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:11:40,206 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 3.0 (TID 10)
2020-04-08 12:11:40,206 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 9) in 4 ms on localhost (executor driver) (1/3)
2020-04-08 12:11:40,208 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 2
2020-04-08 12:11:40,209 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 3.0 (TID 10). 722 bytes result sent to driver
2020-04-08 12:11:40,210 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 3.0 (TID 11, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:11:40,210 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 3.0 (TID 11)
2020-04-08 12:11:40,210 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 3.0 (TID 10) in 4 ms on localhost (executor driver) (2/3)
2020-04-08 12:11:40,212 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 12:11:40,213 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 3.0 (TID 11). 679 bytes result sent to driver
2020-04-08 12:11:40,213 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 3.0 (TID 11) in 4 ms on localhost (executor driver) (3/3)
2020-04-08 12:11:40,213 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
2020-04-08 12:11:40,214 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 3 (collect at Kafka2SparkStreaming2Kudu.scala:141) finished in 0.017 s
2020-04-08 12:11:40,215 INFO org.apache.spark.scheduler.DAGScheduler: Job 3 finished: collect at Kafka2SparkStreaming2Kudu.scala:141, took 0.020656 s
2020-04-08 12:11:40,221 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586319100000 ms.0 from job set of time 1586319100000 ms
2020-04-08 12:11:40,221 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.221 s for time 1586319100000 ms (execution: 0.033 s)
2020-04-08 12:11:40,221 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 6 from persistence list
2020-04-08 12:11:40,222 INFO org.apache.spark.storage.BlockManager: Removing RDD 6
2020-04-08 12:11:40,222 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 12:11:40,222 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586319080000 ms
2020-04-08 12:11:50,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 12:11:50,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 12:11:50,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 12:11:50,180 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 151.
2020-04-08 12:11:50,181 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 151.
2020-04-08 12:11:50,181 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 12:11:50,182 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586319110000 ms
2020-04-08 12:11:50,182 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586319110000 ms.0 from job set of time 1586319110000 ms
2020-04-08 12:11:50,187 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:141
2020-04-08 12:11:50,187 INFO org.apache.spark.scheduler.DAGScheduler: Got job 4 (collect at Kafka2SparkStreaming2Kudu.scala:141) with 3 output partitions
2020-04-08 12:11:50,187 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 4 (collect at Kafka2SparkStreaming2Kudu.scala:141)
2020-04-08 12:11:50,187 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 12:11:50,188 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 12:11:50,188 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[13] at map at Kafka2SparkStreaming2Kudu.scala:126), which has no missing parents
2020-04-08 12:11:50,190 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 12:11:50,192 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 12:11:50,193 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.102:52741 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:11:50,193 INFO org.apache.spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1161
2020-04-08 12:11:50,194 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 4 (MapPartitionsRDD[13] at map at Kafka2SparkStreaming2Kudu.scala:126) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 12:11:50,194 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 4.0 with 3 tasks
2020-04-08 12:11:50,194 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 12, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:11:50,195 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 4.0 (TID 12)
2020-04-08 12:11:50,196 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 1
2020-04-08 12:11:50,197 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 4.0 (TID 12). 679 bytes result sent to driver
2020-04-08 12:11:50,198 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 4.0 (TID 13, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:11:50,198 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 4.0 (TID 13)
2020-04-08 12:11:50,198 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 12) in 4 ms on localhost (executor driver) (1/3)
2020-04-08 12:11:50,199 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 2
2020-04-08 12:11:50,199 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 4.0 (TID 13). 679 bytes result sent to driver
2020-04-08 12:11:50,200 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 4.0 (TID 14, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:11:50,200 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 4.0 (TID 13) in 3 ms on localhost (executor driver) (2/3)
2020-04-08 12:11:50,200 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 4.0 (TID 14)
2020-04-08 12:11:50,201 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 12:11:50,202 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 4.0 (TID 14). 722 bytes result sent to driver
2020-04-08 12:11:50,203 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 4.0 (TID 14) in 3 ms on localhost (executor driver) (3/3)
2020-04-08 12:11:50,203 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
2020-04-08 12:11:50,203 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 4 (collect at Kafka2SparkStreaming2Kudu.scala:141) finished in 0.014 s
2020-04-08 12:11:50,204 INFO org.apache.spark.scheduler.DAGScheduler: Job 4 finished: collect at Kafka2SparkStreaming2Kudu.scala:141, took 0.016686 s
2020-04-08 12:11:50,210 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586319110000 ms.0 from job set of time 1586319110000 ms
2020-04-08 12:11:50,210 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 9 from persistence list
2020-04-08 12:11:50,210 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.210 s for time 1586319110000 ms (execution: 0.028 s)
2020-04-08 12:11:50,211 INFO org.apache.spark.storage.BlockManager: Removing RDD 9
2020-04-08 12:11:50,212 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 12:11:50,212 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586319090000 ms
2020-04-08 12:12:00,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 12:12:00,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 12:12:00,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 12:12:00,183 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 151.
2020-04-08 12:12:00,184 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 151.
2020-04-08 12:12:00,185 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 12:12:00,185 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586319120000 ms
2020-04-08 12:12:00,186 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586319120000 ms.0 from job set of time 1586319120000 ms
2020-04-08 12:12:00,190 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:141
2020-04-08 12:12:00,191 INFO org.apache.spark.scheduler.DAGScheduler: Got job 5 (collect at Kafka2SparkStreaming2Kudu.scala:141) with 3 output partitions
2020-04-08 12:12:00,191 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 5 (collect at Kafka2SparkStreaming2Kudu.scala:141)
2020-04-08 12:12:00,191 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 12:12:00,191 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 12:12:00,192 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[16] at map at Kafka2SparkStreaming2Kudu.scala:126), which has no missing parents
2020-04-08 12:12:00,194 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 12:12:00,195 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 12:12:00,196 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.102:52741 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:12:00,196 INFO org.apache.spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1161
2020-04-08 12:12:00,197 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 5 (MapPartitionsRDD[16] at map at Kafka2SparkStreaming2Kudu.scala:126) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 12:12:00,197 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 5.0 with 3 tasks
2020-04-08 12:12:00,198 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 15, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:12:00,198 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 5.0 (TID 15)
2020-04-08 12:12:00,200 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 1
2020-04-08 12:12:00,201 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 5.0 (TID 15). 722 bytes result sent to driver
2020-04-08 12:12:00,201 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 5.0 (TID 16, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:12:00,202 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 15) in 5 ms on localhost (executor driver) (1/3)
2020-04-08 12:12:00,202 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 5.0 (TID 16)
2020-04-08 12:12:00,203 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 2
2020-04-08 12:12:00,203 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 5.0 (TID 16). 679 bytes result sent to driver
2020-04-08 12:12:00,204 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 5.0 (TID 17, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:12:00,204 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 5.0 (TID 17)
2020-04-08 12:12:00,204 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 5.0 (TID 16) in 3 ms on localhost (executor driver) (2/3)
2020-04-08 12:12:00,205 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 12:12:00,206 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 5.0 (TID 17). 679 bytes result sent to driver
2020-04-08 12:12:00,206 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 5.0 (TID 17) in 2 ms on localhost (executor driver) (3/3)
2020-04-08 12:12:00,206 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
2020-04-08 12:12:00,207 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 5 (collect at Kafka2SparkStreaming2Kudu.scala:141) finished in 0.014 s
2020-04-08 12:12:00,207 INFO org.apache.spark.scheduler.DAGScheduler: Job 5 finished: collect at Kafka2SparkStreaming2Kudu.scala:141, took 0.016604 s
2020-04-08 12:12:00,213 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586319120000 ms.0 from job set of time 1586319120000 ms
2020-04-08 12:12:00,214 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.213 s for time 1586319120000 ms (execution: 0.027 s)
2020-04-08 12:12:00,214 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 12 from persistence list
2020-04-08 12:12:00,214 INFO org.apache.spark.storage.BlockManager: Removing RDD 12
2020-04-08 12:12:00,214 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 12:12:00,214 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586319100000 ms
2020-04-08 12:12:10,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 12:12:10,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 12:12:10,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 12:12:10,179 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 12:12:10,180 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 151.
2020-04-08 12:12:10,180 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 151.
2020-04-08 12:12:10,180 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586319130000 ms
2020-04-08 12:12:10,181 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586319130000 ms.0 from job set of time 1586319130000 ms
2020-04-08 12:12:10,186 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:141
2020-04-08 12:12:10,186 INFO org.apache.spark.scheduler.DAGScheduler: Got job 6 (collect at Kafka2SparkStreaming2Kudu.scala:141) with 3 output partitions
2020-04-08 12:12:10,186 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 6 (collect at Kafka2SparkStreaming2Kudu.scala:141)
2020-04-08 12:12:10,186 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 12:12:10,187 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 12:12:10,187 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[19] at map at Kafka2SparkStreaming2Kudu.scala:126), which has no missing parents
2020-04-08 12:12:10,189 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 12:12:10,191 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 12:12:10,191 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.0.102:52741 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:12:10,192 INFO org.apache.spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1161
2020-04-08 12:12:10,192 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 6 (MapPartitionsRDD[19] at map at Kafka2SparkStreaming2Kudu.scala:126) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 12:12:10,192 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 6.0 with 3 tasks
2020-04-08 12:12:10,193 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 18, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:12:10,193 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 6.0 (TID 18)
2020-04-08 12:12:10,194 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 1
2020-04-08 12:12:10,195 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 6.0 (TID 18). 679 bytes result sent to driver
2020-04-08 12:12:10,196 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 6.0 (TID 19, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:12:10,196 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 18) in 3 ms on localhost (executor driver) (1/3)
2020-04-08 12:12:10,196 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 6.0 (TID 19)
2020-04-08 12:12:10,197 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 2
2020-04-08 12:12:10,198 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 6.0 (TID 19). 679 bytes result sent to driver
2020-04-08 12:12:10,199 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 6.0 (TID 20, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:12:10,199 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 6.0 (TID 20)
2020-04-08 12:12:10,199 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 6.0 (TID 19) in 3 ms on localhost (executor driver) (2/3)
2020-04-08 12:12:10,200 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 12:12:10,201 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 6.0 (TID 20). 679 bytes result sent to driver
2020-04-08 12:12:10,201 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 6.0 (TID 20) in 2 ms on localhost (executor driver) (3/3)
2020-04-08 12:12:10,202 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
2020-04-08 12:12:10,202 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 6 (collect at Kafka2SparkStreaming2Kudu.scala:141) finished in 0.014 s
2020-04-08 12:12:10,203 INFO org.apache.spark.scheduler.DAGScheduler: Job 6 finished: collect at Kafka2SparkStreaming2Kudu.scala:141, took 0.016589 s
2020-04-08 12:12:10,207 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586319130000 ms.0 from job set of time 1586319130000 ms
2020-04-08 12:12:10,208 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.207 s for time 1586319130000 ms (execution: 0.026 s)
2020-04-08 12:12:10,208 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 15 from persistence list
2020-04-08 12:12:10,208 INFO org.apache.spark.storage.BlockManager: Removing RDD 15
2020-04-08 12:12:10,208 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 12:12:10,208 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586319110000 ms
2020-04-08 12:12:20,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 12:12:20,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 12:12:20,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 12:12:20,196 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 151.
2020-04-08 12:12:20,197 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 151.
2020-04-08 12:12:20,197 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 12:12:20,198 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586319140000 ms
2020-04-08 12:12:20,198 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586319140000 ms.0 from job set of time 1586319140000 ms
2020-04-08 12:12:20,203 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:141
2020-04-08 12:12:20,204 INFO org.apache.spark.scheduler.DAGScheduler: Got job 7 (collect at Kafka2SparkStreaming2Kudu.scala:141) with 3 output partitions
2020-04-08 12:12:20,204 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 7 (collect at Kafka2SparkStreaming2Kudu.scala:141)
2020-04-08 12:12:20,204 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 12:12:20,204 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 12:12:20,205 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[22] at map at Kafka2SparkStreaming2Kudu.scala:126), which has no missing parents
2020-04-08 12:12:20,207 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 12:12:20,208 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 12:12:20,209 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.0.102:52741 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:12:20,209 INFO org.apache.spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1161
2020-04-08 12:12:20,209 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 7 (MapPartitionsRDD[22] at map at Kafka2SparkStreaming2Kudu.scala:126) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 12:12:20,210 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 7.0 with 3 tasks
2020-04-08 12:12:20,211 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 7.0 (TID 21, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:12:20,211 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 7.0 (TID 21)
2020-04-08 12:12:20,212 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 1
2020-04-08 12:12:20,213 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 7.0 (TID 21). 679 bytes result sent to driver
2020-04-08 12:12:20,213 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 7.0 (TID 22, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:12:20,214 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 7.0 (TID 22)
2020-04-08 12:12:20,214 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 7.0 (TID 21) in 3 ms on localhost (executor driver) (1/3)
2020-04-08 12:12:20,215 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 2
2020-04-08 12:12:20,215 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 7.0 (TID 22). 679 bytes result sent to driver
2020-04-08 12:12:20,216 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 7.0 (TID 23, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:12:20,216 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 7.0 (TID 22) in 3 ms on localhost (executor driver) (2/3)
2020-04-08 12:12:20,216 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 7.0 (TID 23)
2020-04-08 12:12:20,217 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 12:12:20,218 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 7.0 (TID 23). 679 bytes result sent to driver
2020-04-08 12:12:20,219 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 7.0 (TID 23) in 3 ms on localhost (executor driver) (3/3)
2020-04-08 12:12:20,219 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
2020-04-08 12:12:20,219 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 7 (collect at Kafka2SparkStreaming2Kudu.scala:141) finished in 0.013 s
2020-04-08 12:12:20,221 INFO org.apache.spark.scheduler.DAGScheduler: Job 7 finished: collect at Kafka2SparkStreaming2Kudu.scala:141, took 0.016432 s
2020-04-08 12:12:20,225 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586319140000 ms.0 from job set of time 1586319140000 ms
2020-04-08 12:12:20,226 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.225 s for time 1586319140000 ms (execution: 0.027 s)
2020-04-08 12:12:20,226 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 18 from persistence list
2020-04-08 12:12:20,226 INFO org.apache.spark.storage.BlockManager: Removing RDD 18
2020-04-08 12:12:20,226 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 12:12:20,226 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586319120000 ms
2020-04-08 12:12:30,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 12:12:30,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 12:12:30,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 12:12:30,186 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 151.
2020-04-08 12:12:30,187 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 151.
2020-04-08 12:12:30,187 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 12:12:30,188 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586319150000 ms
2020-04-08 12:12:30,188 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586319150000 ms.0 from job set of time 1586319150000 ms
2020-04-08 12:12:30,193 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:141
2020-04-08 12:12:30,194 INFO org.apache.spark.scheduler.DAGScheduler: Got job 8 (collect at Kafka2SparkStreaming2Kudu.scala:141) with 3 output partitions
2020-04-08 12:12:30,194 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 8 (collect at Kafka2SparkStreaming2Kudu.scala:141)
2020-04-08 12:12:30,194 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 12:12:30,194 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 12:12:30,195 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[25] at map at Kafka2SparkStreaming2Kudu.scala:126), which has no missing parents
2020-04-08 12:12:30,197 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 12:12:30,198 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 12:12:30,199 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.0.102:52741 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:12:30,199 INFO org.apache.spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1161
2020-04-08 12:12:30,199 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 8 (MapPartitionsRDD[25] at map at Kafka2SparkStreaming2Kudu.scala:126) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 12:12:30,199 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 8.0 with 3 tasks
2020-04-08 12:12:30,200 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 24, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:12:30,200 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 8.0 (TID 24)
2020-04-08 12:12:30,201 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 1
2020-04-08 12:12:30,202 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 8.0 (TID 24). 722 bytes result sent to driver
2020-04-08 12:12:30,202 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 8.0 (TID 25, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:12:30,202 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 8.0 (TID 25)
2020-04-08 12:12:30,202 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 24) in 2 ms on localhost (executor driver) (1/3)
2020-04-08 12:12:30,203 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 2
2020-04-08 12:12:30,204 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 8.0 (TID 25). 679 bytes result sent to driver
2020-04-08 12:12:30,204 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 8.0 (TID 26, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:12:30,204 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 8.0 (TID 26)
2020-04-08 12:12:30,204 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 8.0 (TID 25) in 2 ms on localhost (executor driver) (2/3)
2020-04-08 12:12:30,205 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 12:12:30,205 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 8.0 (TID 26). 679 bytes result sent to driver
2020-04-08 12:12:30,206 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 8.0 (TID 26) in 2 ms on localhost (executor driver) (3/3)
2020-04-08 12:12:30,206 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
2020-04-08 12:12:30,206 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 8 (collect at Kafka2SparkStreaming2Kudu.scala:141) finished in 0.010 s
2020-04-08 12:12:30,206 INFO org.apache.spark.scheduler.DAGScheduler: Job 8 finished: collect at Kafka2SparkStreaming2Kudu.scala:141, took 0.012759 s
2020-04-08 12:12:30,211 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586319150000 ms.0 from job set of time 1586319150000 ms
2020-04-08 12:12:30,211 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 21 from persistence list
2020-04-08 12:12:30,211 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.211 s for time 1586319150000 ms (execution: 0.023 s)
2020-04-08 12:12:30,212 INFO org.apache.spark.storage.BlockManager: Removing RDD 21
2020-04-08 12:12:30,212 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 12:12:30,212 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586319130000 ms
2020-04-08 12:12:40,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 12:12:40,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 12:12:40,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 12:12:40,181 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 151.
2020-04-08 12:12:40,182 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 12:12:40,184 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 151.
2020-04-08 12:12:40,184 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586319160000 ms
2020-04-08 12:12:40,185 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586319160000 ms.0 from job set of time 1586319160000 ms
2020-04-08 12:12:40,189 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:141
2020-04-08 12:12:40,190 INFO org.apache.spark.scheduler.DAGScheduler: Got job 9 (collect at Kafka2SparkStreaming2Kudu.scala:141) with 3 output partitions
2020-04-08 12:12:40,190 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 9 (collect at Kafka2SparkStreaming2Kudu.scala:141)
2020-04-08 12:12:40,190 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 12:12:40,190 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 12:12:40,191 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[28] at map at Kafka2SparkStreaming2Kudu.scala:126), which has no missing parents
2020-04-08 12:12:40,193 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 12:12:40,194 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 12:12:40,195 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.0.102:52741 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:12:40,195 INFO org.apache.spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1161
2020-04-08 12:12:40,195 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 9 (MapPartitionsRDD[28] at map at Kafka2SparkStreaming2Kudu.scala:126) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 12:12:40,195 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 9.0 with 3 tasks
2020-04-08 12:12:40,196 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 27, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:12:40,196 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 9.0 (TID 27)
2020-04-08 12:12:40,197 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 1
2020-04-08 12:12:40,198 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 9.0 (TID 27). 679 bytes result sent to driver
2020-04-08 12:12:40,198 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 9.0 (TID 28, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:12:40,198 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 9.0 (TID 28)
2020-04-08 12:12:40,198 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 27) in 2 ms on localhost (executor driver) (1/3)
2020-04-08 12:12:40,199 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 2
2020-04-08 12:12:40,199 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 9.0 (TID 28). 679 bytes result sent to driver
2020-04-08 12:12:40,200 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 9.0 (TID 29, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:12:40,200 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 9.0 (TID 29)
2020-04-08 12:12:40,200 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 9.0 (TID 28) in 2 ms on localhost (executor driver) (2/3)
2020-04-08 12:12:40,201 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 12:12:40,201 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 9.0 (TID 29). 679 bytes result sent to driver
2020-04-08 12:12:40,202 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 9.0 (TID 29) in 2 ms on localhost (executor driver) (3/3)
2020-04-08 12:12:40,202 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
2020-04-08 12:12:40,202 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 9 (collect at Kafka2SparkStreaming2Kudu.scala:141) finished in 0.010 s
2020-04-08 12:12:40,202 INFO org.apache.spark.scheduler.DAGScheduler: Job 9 finished: collect at Kafka2SparkStreaming2Kudu.scala:141, took 0.012872 s
2020-04-08 12:12:40,207 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586319160000 ms.0 from job set of time 1586319160000 ms
2020-04-08 12:12:40,207 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.207 s for time 1586319160000 ms (execution: 0.022 s)
2020-04-08 12:12:40,207 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 24 from persistence list
2020-04-08 12:12:40,208 INFO org.apache.spark.storage.BlockManager: Removing RDD 24
2020-04-08 12:12:40,208 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 12:12:40,208 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586319140000 ms
2020-04-08 12:12:50,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 12:12:50,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 12:12:50,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 12:12:50,180 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 151.
2020-04-08 12:12:50,180 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 12:12:50,180 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 151.
2020-04-08 12:12:50,181 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586319170000 ms
2020-04-08 12:12:50,182 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586319170000 ms.0 from job set of time 1586319170000 ms
2020-04-08 12:12:50,186 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:141
2020-04-08 12:12:50,186 INFO org.apache.spark.scheduler.DAGScheduler: Got job 10 (collect at Kafka2SparkStreaming2Kudu.scala:141) with 3 output partitions
2020-04-08 12:12:50,186 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 10 (collect at Kafka2SparkStreaming2Kudu.scala:141)
2020-04-08 12:12:50,186 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 12:12:50,186 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 12:12:50,187 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[31] at map at Kafka2SparkStreaming2Kudu.scala:126), which has no missing parents
2020-04-08 12:12:50,188 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 12:12:50,190 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 12:12:50,190 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 192.168.0.102:52741 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:12:50,191 INFO org.apache.spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1161
2020-04-08 12:12:50,191 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 10 (MapPartitionsRDD[31] at map at Kafka2SparkStreaming2Kudu.scala:126) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 12:12:50,191 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 10.0 with 3 tasks
2020-04-08 12:12:50,192 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 30, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:12:50,192 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 10.0 (TID 30)
2020-04-08 12:12:50,193 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 1
2020-04-08 12:12:50,194 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 10.0 (TID 30). 679 bytes result sent to driver
2020-04-08 12:12:50,194 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 10.0 (TID 31, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:12:50,194 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 10.0 (TID 31)
2020-04-08 12:12:50,194 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 30) in 2 ms on localhost (executor driver) (1/3)
2020-04-08 12:12:50,195 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 2
2020-04-08 12:12:50,196 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 10.0 (TID 31). 679 bytes result sent to driver
2020-04-08 12:12:50,196 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 10.0 (TID 32, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:12:50,196 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 10.0 (TID 32)
2020-04-08 12:12:50,196 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 10.0 (TID 31) in 2 ms on localhost (executor driver) (2/3)
2020-04-08 12:12:50,197 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 12:12:50,198 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 10.0 (TID 32). 679 bytes result sent to driver
2020-04-08 12:12:50,198 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 10.0 (TID 32) in 2 ms on localhost (executor driver) (3/3)
2020-04-08 12:12:50,198 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
2020-04-08 12:12:50,199 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 10 (collect at Kafka2SparkStreaming2Kudu.scala:141) finished in 0.010 s
2020-04-08 12:12:50,199 INFO org.apache.spark.scheduler.DAGScheduler: Job 10 finished: collect at Kafka2SparkStreaming2Kudu.scala:141, took 0.013161 s
2020-04-08 12:12:50,204 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586319170000 ms.0 from job set of time 1586319170000 ms
2020-04-08 12:12:50,204 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 27 from persistence list
2020-04-08 12:12:50,204 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.204 s for time 1586319170000 ms (execution: 0.022 s)
2020-04-08 12:12:50,205 INFO org.apache.spark.storage.BlockManager: Removing RDD 27
2020-04-08 12:12:50,205 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 12:12:50,205 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586319150000 ms
2020-04-08 12:13:00,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 12:13:00,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 12:13:00,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 12:13:00,187 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 151.
2020-04-08 12:13:00,188 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 151.
2020-04-08 12:13:00,188 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 12:13:00,189 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586319180000 ms
2020-04-08 12:13:00,190 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586319180000 ms.0 from job set of time 1586319180000 ms
2020-04-08 12:13:00,194 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:141
2020-04-08 12:13:00,195 INFO org.apache.spark.scheduler.DAGScheduler: Got job 11 (collect at Kafka2SparkStreaming2Kudu.scala:141) with 3 output partitions
2020-04-08 12:13:00,195 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 11 (collect at Kafka2SparkStreaming2Kudu.scala:141)
2020-04-08 12:13:00,195 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 12:13:00,195 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 12:13:00,195 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[34] at map at Kafka2SparkStreaming2Kudu.scala:126), which has no missing parents
2020-04-08 12:13:00,197 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 12:13:00,198 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 12:13:00,199 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 192.168.0.102:52741 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:13:00,199 INFO org.apache.spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1161
2020-04-08 12:13:00,200 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 11 (MapPartitionsRDD[34] at map at Kafka2SparkStreaming2Kudu.scala:126) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 12:13:00,200 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 11.0 with 3 tasks
2020-04-08 12:13:00,200 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 33, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:13:00,201 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 11.0 (TID 33)
2020-04-08 12:13:00,201 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 1
2020-04-08 12:13:00,202 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 11.0 (TID 33). 722 bytes result sent to driver
2020-04-08 12:13:00,202 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 11.0 (TID 34, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:13:00,202 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 11.0 (TID 34)
2020-04-08 12:13:00,202 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 33) in 2 ms on localhost (executor driver) (1/3)
2020-04-08 12:13:00,203 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 2
2020-04-08 12:13:00,204 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 11.0 (TID 34). 679 bytes result sent to driver
2020-04-08 12:13:00,204 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 11.0 (TID 35, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:13:00,204 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 11.0 (TID 35)
2020-04-08 12:13:00,204 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 11.0 (TID 34) in 2 ms on localhost (executor driver) (2/3)
2020-04-08 12:13:00,205 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 12:13:00,205 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 11.0 (TID 35). 679 bytes result sent to driver
2020-04-08 12:13:00,206 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 11.0 (TID 35) in 2 ms on localhost (executor driver) (3/3)
2020-04-08 12:13:00,206 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
2020-04-08 12:13:00,206 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 11 (collect at Kafka2SparkStreaming2Kudu.scala:141) finished in 0.010 s
2020-04-08 12:13:00,206 INFO org.apache.spark.scheduler.DAGScheduler: Job 11 finished: collect at Kafka2SparkStreaming2Kudu.scala:141, took 0.012051 s
2020-04-08 12:13:00,210 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586319180000 ms.0 from job set of time 1586319180000 ms
2020-04-08 12:13:00,211 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.210 s for time 1586319180000 ms (execution: 0.020 s)
2020-04-08 12:13:00,211 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 30 from persistence list
2020-04-08 12:13:00,211 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 12:13:00,211 INFO org.apache.spark.storage.BlockManager: Removing RDD 30
2020-04-08 12:13:00,211 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586319160000 ms
2020-04-08 12:13:10,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 12:13:10,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 12:13:10,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 12:13:10,185 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 151.
2020-04-08 12:13:10,186 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 151.
2020-04-08 12:13:10,187 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 12:13:10,187 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586319190000 ms
2020-04-08 12:13:10,187 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586319190000 ms.0 from job set of time 1586319190000 ms
2020-04-08 12:13:10,192 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:141
2020-04-08 12:13:10,192 INFO org.apache.spark.scheduler.DAGScheduler: Got job 12 (collect at Kafka2SparkStreaming2Kudu.scala:141) with 3 output partitions
2020-04-08 12:13:10,192 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 12 (collect at Kafka2SparkStreaming2Kudu.scala:141)
2020-04-08 12:13:10,192 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 12:13:10,193 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 12:13:10,193 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[37] at map at Kafka2SparkStreaming2Kudu.scala:126), which has no missing parents
2020-04-08 12:13:10,195 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 12:13:10,196 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 12:13:10,197 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 192.168.0.102:52741 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:13:10,197 INFO org.apache.spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1161
2020-04-08 12:13:10,198 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 12 (MapPartitionsRDD[37] at map at Kafka2SparkStreaming2Kudu.scala:126) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 12:13:10,198 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 12.0 with 3 tasks
2020-04-08 12:13:10,198 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 12.0 (TID 36, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:13:10,199 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 12.0 (TID 36)
2020-04-08 12:13:10,200 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 1
2020-04-08 12:13:10,200 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 12.0 (TID 36). 679 bytes result sent to driver
2020-04-08 12:13:10,201 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 12.0 (TID 37, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:13:10,201 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 12.0 (TID 37)
2020-04-08 12:13:10,201 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 12.0 (TID 36) in 3 ms on localhost (executor driver) (1/3)
2020-04-08 12:13:10,202 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 2
2020-04-08 12:13:10,202 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 12.0 (TID 37). 679 bytes result sent to driver
2020-04-08 12:13:10,202 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 12.0 (TID 38, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:13:10,203 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 12.0 (TID 38)
2020-04-08 12:13:10,203 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 12.0 (TID 37) in 2 ms on localhost (executor driver) (2/3)
2020-04-08 12:13:10,203 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 12:13:10,204 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 12.0 (TID 38). 679 bytes result sent to driver
2020-04-08 12:13:10,204 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 12.0 (TID 38) in 2 ms on localhost (executor driver) (3/3)
2020-04-08 12:13:10,205 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
2020-04-08 12:13:10,205 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 12 (collect at Kafka2SparkStreaming2Kudu.scala:141) finished in 0.011 s
2020-04-08 12:13:10,205 INFO org.apache.spark.scheduler.DAGScheduler: Job 12 finished: collect at Kafka2SparkStreaming2Kudu.scala:141, took 0.013391 s
2020-04-08 12:13:10,210 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586319190000 ms.0 from job set of time 1586319190000 ms
2020-04-08 12:13:10,210 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 33 from persistence list
2020-04-08 12:13:10,210 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.210 s for time 1586319190000 ms (execution: 0.023 s)
2020-04-08 12:13:10,211 INFO org.apache.spark.storage.BlockManager: Removing RDD 33
2020-04-08 12:13:10,211 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 12:13:10,211 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586319170000 ms
2020-04-08 12:13:20,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 12:13:20,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 12:13:20,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 12:13:20,185 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 12:13:20,186 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 151.
2020-04-08 12:13:20,186 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 151.
2020-04-08 12:13:20,188 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586319200000 ms
2020-04-08 12:13:20,189 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586319200000 ms.0 from job set of time 1586319200000 ms
2020-04-08 12:13:20,201 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:141
2020-04-08 12:13:20,203 INFO org.apache.spark.scheduler.DAGScheduler: Got job 13 (collect at Kafka2SparkStreaming2Kudu.scala:141) with 3 output partitions
2020-04-08 12:13:20,203 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 13 (collect at Kafka2SparkStreaming2Kudu.scala:141)
2020-04-08 12:13:20,204 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 12:13:20,204 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 12:13:20,206 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[40] at map at Kafka2SparkStreaming2Kudu.scala:126), which has no missing parents
2020-04-08 12:13:20,212 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 12:13:20,216 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 12:13:20,219 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 192.168.0.102:52741 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:13:20,219 INFO org.apache.spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1161
2020-04-08 12:13:20,220 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 13 (MapPartitionsRDD[40] at map at Kafka2SparkStreaming2Kudu.scala:126) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 12:13:20,221 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 13.0 with 3 tasks
2020-04-08 12:13:20,224 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 13.0 (TID 39, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:13:20,225 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 13.0 (TID 39)
2020-04-08 12:13:20,227 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 1
2020-04-08 12:13:20,228 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 13.0 (TID 39). 722 bytes result sent to driver
2020-04-08 12:13:20,229 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 13.0 (TID 40, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:13:20,229 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 13.0 (TID 40)
2020-04-08 12:13:20,229 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 13.0 (TID 39) in 5 ms on localhost (executor driver) (1/3)
2020-04-08 12:13:20,231 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 2
2020-04-08 12:13:20,232 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 13.0 (TID 40). 679 bytes result sent to driver
2020-04-08 12:13:20,235 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 13.0 (TID 41, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:13:20,236 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 13.0 (TID 40) in 7 ms on localhost (executor driver) (2/3)
2020-04-08 12:13:20,236 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 13.0 (TID 41)
2020-04-08 12:13:20,238 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 12:13:20,239 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 13.0 (TID 41). 722 bytes result sent to driver
2020-04-08 12:13:20,240 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 13.0 (TID 41) in 5 ms on localhost (executor driver) (3/3)
2020-04-08 12:13:20,240 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
2020-04-08 12:13:20,241 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 13 (collect at Kafka2SparkStreaming2Kudu.scala:141) finished in 0.030 s
2020-04-08 12:13:20,241 INFO org.apache.spark.scheduler.DAGScheduler: Job 13 finished: collect at Kafka2SparkStreaming2Kudu.scala:141, took 0.040158 s
2020-04-08 12:13:20,246 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586319200000 ms.0 from job set of time 1586319200000 ms
2020-04-08 12:13:20,247 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.246 s for time 1586319200000 ms (execution: 0.057 s)
2020-04-08 12:13:20,247 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 36 from persistence list
2020-04-08 12:13:20,248 INFO org.apache.spark.storage.BlockManager: Removing RDD 36
2020-04-08 12:13:20,248 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 12:13:20,248 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586319180000 ms
2020-04-08 12:13:30,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 12:13:30,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 12:13:30,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 12:13:30,180 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 151.
2020-04-08 12:13:30,182 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 151.
2020-04-08 12:13:30,183 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 12:13:30,184 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586319210000 ms
2020-04-08 12:13:30,184 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586319210000 ms.0 from job set of time 1586319210000 ms
2020-04-08 12:13:30,191 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:141
2020-04-08 12:13:30,192 INFO org.apache.spark.scheduler.DAGScheduler: Got job 14 (collect at Kafka2SparkStreaming2Kudu.scala:141) with 3 output partitions
2020-04-08 12:13:30,192 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 14 (collect at Kafka2SparkStreaming2Kudu.scala:141)
2020-04-08 12:13:30,192 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 12:13:30,192 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 12:13:30,193 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[43] at map at Kafka2SparkStreaming2Kudu.scala:126), which has no missing parents
2020-04-08 12:13:30,196 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 12:13:30,198 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 12:13:30,198 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 192.168.0.102:52741 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:13:30,199 INFO org.apache.spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1161
2020-04-08 12:13:30,199 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 14 (MapPartitionsRDD[43] at map at Kafka2SparkStreaming2Kudu.scala:126) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 12:13:30,200 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 14.0 with 3 tasks
2020-04-08 12:13:30,201 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 42, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:13:30,201 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 14.0 (TID 42)
2020-04-08 12:13:30,202 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 1
2020-04-08 12:13:30,203 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 14.0 (TID 42). 679 bytes result sent to driver
2020-04-08 12:13:30,204 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 14.0 (TID 43, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:13:30,204 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 14.0 (TID 43)
2020-04-08 12:13:30,204 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 42) in 3 ms on localhost (executor driver) (1/3)
2020-04-08 12:13:30,206 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 2
2020-04-08 12:13:30,206 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 14.0 (TID 43). 679 bytes result sent to driver
2020-04-08 12:13:30,207 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 14.0 (TID 44, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:13:30,207 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 14.0 (TID 43) in 3 ms on localhost (executor driver) (2/3)
2020-04-08 12:13:30,207 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 14.0 (TID 44)
2020-04-08 12:13:30,209 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 12:13:30,209 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 14.0 (TID 44). 679 bytes result sent to driver
2020-04-08 12:13:30,210 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 14.0 (TID 44) in 3 ms on localhost (executor driver) (3/3)
2020-04-08 12:13:30,210 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
2020-04-08 12:13:30,211 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 14 (collect at Kafka2SparkStreaming2Kudu.scala:141) finished in 0.015 s
2020-04-08 12:13:30,211 INFO org.apache.spark.scheduler.DAGScheduler: Job 14 finished: collect at Kafka2SparkStreaming2Kudu.scala:141, took 0.019817 s
2020-04-08 12:13:30,223 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586319210000 ms.0 from job set of time 1586319210000 ms
2020-04-08 12:13:30,224 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 39 from persistence list
2020-04-08 12:13:30,224 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.223 s for time 1586319210000 ms (execution: 0.039 s)
2020-04-08 12:13:30,225 INFO org.apache.spark.storage.BlockManager: Removing RDD 39
2020-04-08 12:13:30,225 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 12:13:30,225 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586319190000 ms
2020-04-08 12:13:40,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 12:13:40,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 12:13:40,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 12:13:40,185 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 151.
2020-04-08 12:13:40,185 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 151.
2020-04-08 12:13:40,186 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 12:13:40,187 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586319220000 ms
2020-04-08 12:13:40,187 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586319220000 ms.0 from job set of time 1586319220000 ms
2020-04-08 12:13:40,193 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:141
2020-04-08 12:13:40,194 INFO org.apache.spark.scheduler.DAGScheduler: Got job 15 (collect at Kafka2SparkStreaming2Kudu.scala:141) with 3 output partitions
2020-04-08 12:13:40,194 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 15 (collect at Kafka2SparkStreaming2Kudu.scala:141)
2020-04-08 12:13:40,194 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 12:13:40,194 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 12:13:40,195 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[46] at map at Kafka2SparkStreaming2Kudu.scala:126), which has no missing parents
2020-04-08 12:13:40,199 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 12:13:40,200 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 12:13:40,201 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 192.168.0.102:52741 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:13:40,201 INFO org.apache.spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1161
2020-04-08 12:13:40,202 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 15 (MapPartitionsRDD[46] at map at Kafka2SparkStreaming2Kudu.scala:126) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 12:13:40,202 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 15.0 with 3 tasks
2020-04-08 12:13:40,204 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 45, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:13:40,204 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 15.0 (TID 45)
2020-04-08 12:13:40,205 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 1
2020-04-08 12:13:40,206 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 15.0 (TID 45). 679 bytes result sent to driver
2020-04-08 12:13:40,206 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 15.0 (TID 46, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:13:40,207 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 15.0 (TID 46)
2020-04-08 12:13:40,207 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 45) in 4 ms on localhost (executor driver) (1/3)
2020-04-08 12:13:40,209 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 2
2020-04-08 12:13:40,210 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 15.0 (TID 46). 722 bytes result sent to driver
2020-04-08 12:13:40,211 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 15.0 (TID 47, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:13:40,211 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 15.0 (TID 47)
2020-04-08 12:13:40,211 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 15.0 (TID 46) in 5 ms on localhost (executor driver) (2/3)
2020-04-08 12:13:40,213 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 12:13:40,213 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 15.0 (TID 47). 679 bytes result sent to driver
2020-04-08 12:13:40,214 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 15.0 (TID 47) in 3 ms on localhost (executor driver) (3/3)
2020-04-08 12:13:40,214 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
2020-04-08 12:13:40,215 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 15 (collect at Kafka2SparkStreaming2Kudu.scala:141) finished in 0.016 s
2020-04-08 12:13:40,215 INFO org.apache.spark.scheduler.DAGScheduler: Job 15 finished: collect at Kafka2SparkStreaming2Kudu.scala:141, took 0.021321 s
2020-04-08 12:13:40,220 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586319220000 ms.0 from job set of time 1586319220000 ms
2020-04-08 12:13:40,221 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 42 from persistence list
2020-04-08 12:13:40,221 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.220 s for time 1586319220000 ms (execution: 0.033 s)
2020-04-08 12:13:40,221 INFO org.apache.spark.storage.BlockManager: Removing RDD 42
2020-04-08 12:13:40,221 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 12:13:40,221 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586319200000 ms
2020-04-08 12:13:50,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 12:13:50,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 12:13:50,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 12:13:50,178 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 151.
2020-04-08 12:13:50,178 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 151.
2020-04-08 12:13:50,178 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 12:13:50,179 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586319230000 ms
2020-04-08 12:13:50,180 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586319230000 ms.0 from job set of time 1586319230000 ms
2020-04-08 12:13:50,185 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:141
2020-04-08 12:13:50,186 INFO org.apache.spark.scheduler.DAGScheduler: Got job 16 (collect at Kafka2SparkStreaming2Kudu.scala:141) with 3 output partitions
2020-04-08 12:13:50,186 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 16 (collect at Kafka2SparkStreaming2Kudu.scala:141)
2020-04-08 12:13:50,186 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 12:13:50,187 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 12:13:50,188 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[49] at map at Kafka2SparkStreaming2Kudu.scala:126), which has no missing parents
2020-04-08 12:13:50,191 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 12:13:50,196 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 12:13:50,196 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 192.168.0.102:52741 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:13:50,197 INFO org.apache.spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1161
2020-04-08 12:13:50,197 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 16 (MapPartitionsRDD[49] at map at Kafka2SparkStreaming2Kudu.scala:126) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 12:13:50,198 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 16.0 with 3 tasks
2020-04-08 12:13:50,199 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 16.0 (TID 48, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:13:50,199 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 16.0 (TID 48)
2020-04-08 12:13:50,200 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 1
2020-04-08 12:13:50,201 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 16.0 (TID 48). 679 bytes result sent to driver
2020-04-08 12:13:50,201 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 16.0 (TID 49, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:13:50,202 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 16.0 (TID 49)
2020-04-08 12:13:50,202 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 16.0 (TID 48) in 4 ms on localhost (executor driver) (1/3)
2020-04-08 12:13:50,204 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 2
2020-04-08 12:13:50,207 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 16.0 (TID 49). 722 bytes result sent to driver
2020-04-08 12:13:50,209 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 16.0 (TID 50, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:13:50,210 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 16.0 (TID 49) in 9 ms on localhost (executor driver) (2/3)
2020-04-08 12:13:50,210 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 16.0 (TID 50)
2020-04-08 12:13:50,212 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 12:13:50,213 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 16.0 (TID 50). 722 bytes result sent to driver
2020-04-08 12:13:50,214 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 16.0 (TID 50) in 5 ms on localhost (executor driver) (3/3)
2020-04-08 12:13:50,214 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
2020-04-08 12:13:50,214 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 16 (collect at Kafka2SparkStreaming2Kudu.scala:141) finished in 0.025 s
2020-04-08 12:13:50,215 INFO org.apache.spark.scheduler.DAGScheduler: Job 16 finished: collect at Kafka2SparkStreaming2Kudu.scala:141, took 0.029749 s
2020-04-08 12:13:50,223 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586319230000 ms.0 from job set of time 1586319230000 ms
2020-04-08 12:13:50,223 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 45 from persistence list
2020-04-08 12:13:50,223 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.223 s for time 1586319230000 ms (execution: 0.043 s)
2020-04-08 12:13:50,224 INFO org.apache.spark.storage.BlockManager: Removing RDD 45
2020-04-08 12:13:50,224 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 12:13:50,224 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586319210000 ms
2020-04-08 12:14:00,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 12:14:00,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 12:14:00,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 12:14:00,179 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 151.
2020-04-08 12:14:00,190 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 12:14:00,194 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 151.
2020-04-08 12:14:00,194 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586319240000 ms
2020-04-08 12:14:00,195 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586319240000 ms.0 from job set of time 1586319240000 ms
2020-04-08 12:14:00,198 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:141
2020-04-08 12:14:00,199 INFO org.apache.spark.scheduler.DAGScheduler: Got job 17 (collect at Kafka2SparkStreaming2Kudu.scala:141) with 3 output partitions
2020-04-08 12:14:00,199 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 17 (collect at Kafka2SparkStreaming2Kudu.scala:141)
2020-04-08 12:14:00,199 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 12:14:00,199 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 12:14:00,200 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[52] at map at Kafka2SparkStreaming2Kudu.scala:126), which has no missing parents
2020-04-08 12:14:00,201 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 12:14:00,202 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 12:14:00,202 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 192.168.0.102:52741 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:14:00,203 INFO org.apache.spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1161
2020-04-08 12:14:00,203 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 17 (MapPartitionsRDD[52] at map at Kafka2SparkStreaming2Kudu.scala:126) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 12:14:00,203 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 17.0 with 3 tasks
2020-04-08 12:14:00,204 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 17.0 (TID 51, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:14:00,204 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 17.0 (TID 51)
2020-04-08 12:14:00,205 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 1
2020-04-08 12:14:00,205 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 17.0 (TID 51). 679 bytes result sent to driver
2020-04-08 12:14:00,205 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 17.0 (TID 52, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:14:00,206 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 17.0 (TID 52)
2020-04-08 12:14:00,206 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 17.0 (TID 51) in 2 ms on localhost (executor driver) (1/3)
2020-04-08 12:14:00,206 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 2
2020-04-08 12:14:00,207 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 17.0 (TID 52). 679 bytes result sent to driver
2020-04-08 12:14:00,207 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 17.0 (TID 53, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:14:00,207 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 17.0 (TID 52) in 2 ms on localhost (executor driver) (2/3)
2020-04-08 12:14:00,207 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 17.0 (TID 53)
2020-04-08 12:14:00,208 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 12:14:00,208 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 17.0 (TID 53). 679 bytes result sent to driver
2020-04-08 12:14:00,209 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 17.0 (TID 53) in 2 ms on localhost (executor driver) (3/3)
2020-04-08 12:14:00,209 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool 
2020-04-08 12:14:00,209 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 17 (collect at Kafka2SparkStreaming2Kudu.scala:141) finished in 0.009 s
2020-04-08 12:14:00,210 INFO org.apache.spark.scheduler.DAGScheduler: Job 17 finished: collect at Kafka2SparkStreaming2Kudu.scala:141, took 0.011353 s
2020-04-08 12:14:00,215 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586319240000 ms.0 from job set of time 1586319240000 ms
2020-04-08 12:14:00,215 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.215 s for time 1586319240000 ms (execution: 0.020 s)
2020-04-08 12:14:00,215 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 48 from persistence list
2020-04-08 12:14:00,216 INFO org.apache.spark.storage.BlockManager: Removing RDD 48
2020-04-08 12:14:00,216 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 12:14:00,216 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586319220000 ms
2020-04-08 12:14:10,001 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 12:14:10,001 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 12:14:10,001 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 12:14:10,191 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 151.
2020-04-08 12:14:10,191 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 12:14:10,192 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 151.
2020-04-08 12:14:10,192 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586319250000 ms
2020-04-08 12:14:10,193 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586319250000 ms.0 from job set of time 1586319250000 ms
2020-04-08 12:14:10,196 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:141
2020-04-08 12:14:10,196 INFO org.apache.spark.scheduler.DAGScheduler: Got job 18 (collect at Kafka2SparkStreaming2Kudu.scala:141) with 3 output partitions
2020-04-08 12:14:10,196 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 18 (collect at Kafka2SparkStreaming2Kudu.scala:141)
2020-04-08 12:14:10,196 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 12:14:10,196 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 12:14:10,197 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[55] at map at Kafka2SparkStreaming2Kudu.scala:126), which has no missing parents
2020-04-08 12:14:10,198 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 12:14:10,200 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 12:14:10,200 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 192.168.0.102:52741 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:14:10,200 INFO org.apache.spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1161
2020-04-08 12:14:10,200 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 18 (MapPartitionsRDD[55] at map at Kafka2SparkStreaming2Kudu.scala:126) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 12:14:10,200 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 18.0 with 3 tasks
2020-04-08 12:14:10,201 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 54, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:14:10,201 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 18.0 (TID 54)
2020-04-08 12:14:10,202 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 1
2020-04-08 12:14:10,202 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 18.0 (TID 54). 679 bytes result sent to driver
2020-04-08 12:14:10,203 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 18.0 (TID 55, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:14:10,203 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 18.0 (TID 55)
2020-04-08 12:14:10,203 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 54) in 2 ms on localhost (executor driver) (1/3)
2020-04-08 12:14:10,204 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 2
2020-04-08 12:14:10,204 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 18.0 (TID 55). 679 bytes result sent to driver
2020-04-08 12:14:10,204 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 18.0 (TID 56, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:14:10,205 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 18.0 (TID 56)
2020-04-08 12:14:10,205 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 18.0 (TID 55) in 2 ms on localhost (executor driver) (2/3)
2020-04-08 12:14:10,206 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 12:14:10,206 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 18.0 (TID 56). 679 bytes result sent to driver
2020-04-08 12:14:10,206 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 18.0 (TID 56) in 2 ms on localhost (executor driver) (3/3)
2020-04-08 12:14:10,206 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
2020-04-08 12:14:10,207 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 18 (collect at Kafka2SparkStreaming2Kudu.scala:141) finished in 0.009 s
2020-04-08 12:14:10,207 INFO org.apache.spark.scheduler.DAGScheduler: Job 18 finished: collect at Kafka2SparkStreaming2Kudu.scala:141, took 0.011528 s
2020-04-08 12:14:10,212 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586319250000 ms.0 from job set of time 1586319250000 ms
2020-04-08 12:14:10,212 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 51 from persistence list
2020-04-08 12:14:10,212 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.212 s for time 1586319250000 ms (execution: 0.019 s)
2020-04-08 12:14:10,215 INFO org.apache.spark.storage.BlockManager: Removing RDD 51
2020-04-08 12:14:10,215 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 12:14:10,215 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586319230000 ms
2020-04-08 12:14:20,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 12:14:20,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 12:14:20,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 12:14:20,206 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 12:14:20,206 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 151.
2020-04-08 12:14:20,206 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 151.
2020-04-08 12:14:20,207 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586319260000 ms
2020-04-08 12:14:20,208 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586319260000 ms.0 from job set of time 1586319260000 ms
2020-04-08 12:14:20,212 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:141
2020-04-08 12:14:20,212 INFO org.apache.spark.scheduler.DAGScheduler: Got job 19 (collect at Kafka2SparkStreaming2Kudu.scala:141) with 3 output partitions
2020-04-08 12:14:20,212 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 19 (collect at Kafka2SparkStreaming2Kudu.scala:141)
2020-04-08 12:14:20,212 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 12:14:20,212 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 12:14:20,213 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[58] at map at Kafka2SparkStreaming2Kudu.scala:126), which has no missing parents
2020-04-08 12:14:20,215 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 12:14:20,219 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 12:14:20,219 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 192.168.0.102:52741 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:14:20,220 INFO org.apache.spark.SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1161
2020-04-08 12:14:20,221 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 19 (MapPartitionsRDD[58] at map at Kafka2SparkStreaming2Kudu.scala:126) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 12:14:20,221 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 19.0 with 3 tasks
2020-04-08 12:14:20,222 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 19.0 (TID 57, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:14:20,223 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 19.0 (TID 57)
2020-04-08 12:14:20,224 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 1
2020-04-08 12:14:20,224 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 19.0 (TID 57). 679 bytes result sent to driver
2020-04-08 12:14:20,225 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 19.0 (TID 58, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:14:20,225 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 19.0 (TID 58)
2020-04-08 12:14:20,225 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 19.0 (TID 57) in 3 ms on localhost (executor driver) (1/3)
2020-04-08 12:14:20,226 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 2
2020-04-08 12:14:20,226 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 19.0 (TID 58). 679 bytes result sent to driver
2020-04-08 12:14:20,226 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 19.0 (TID 59, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:14:20,226 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 19.0 (TID 59)
2020-04-08 12:14:20,226 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 19.0 (TID 58) in 1 ms on localhost (executor driver) (2/3)
2020-04-08 12:14:20,227 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 12:14:20,227 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 19.0 (TID 59). 679 bytes result sent to driver
2020-04-08 12:14:20,228 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 19.0 (TID 59) in 2 ms on localhost (executor driver) (3/3)
2020-04-08 12:14:20,228 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool 
2020-04-08 12:14:20,228 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 19 (collect at Kafka2SparkStreaming2Kudu.scala:141) finished in 0.014 s
2020-04-08 12:14:20,228 INFO org.apache.spark.scheduler.DAGScheduler: Job 19 finished: collect at Kafka2SparkStreaming2Kudu.scala:141, took 0.016554 s
2020-04-08 12:14:20,233 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586319260000 ms.0 from job set of time 1586319260000 ms
2020-04-08 12:14:20,233 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.233 s for time 1586319260000 ms (execution: 0.025 s)
2020-04-08 12:14:20,233 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 54 from persistence list
2020-04-08 12:14:20,234 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 12:14:20,234 INFO org.apache.spark.storage.BlockManager: Removing RDD 54
2020-04-08 12:14:20,234 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586319240000 ms
2020-04-08 12:14:30,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 12:14:30,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 12:14:30,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 12:14:30,185 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 151.
2020-04-08 12:14:30,185 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 12:14:30,188 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 151.
2020-04-08 12:14:30,188 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586319270000 ms
2020-04-08 12:14:30,189 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586319270000 ms.0 from job set of time 1586319270000 ms
2020-04-08 12:14:30,192 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:141
2020-04-08 12:14:30,192 INFO org.apache.spark.scheduler.DAGScheduler: Got job 20 (collect at Kafka2SparkStreaming2Kudu.scala:141) with 3 output partitions
2020-04-08 12:14:30,192 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 20 (collect at Kafka2SparkStreaming2Kudu.scala:141)
2020-04-08 12:14:30,192 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 12:14:30,193 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 12:14:30,193 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[61] at map at Kafka2SparkStreaming2Kudu.scala:126), which has no missing parents
2020-04-08 12:14:30,194 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 12:14:30,196 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 12:14:30,197 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 192.168.0.102:52741 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:14:30,197 INFO org.apache.spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1161
2020-04-08 12:14:30,197 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 20 (MapPartitionsRDD[61] at map at Kafka2SparkStreaming2Kudu.scala:126) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 12:14:30,197 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 20.0 with 3 tasks
2020-04-08 12:14:30,198 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 20.0 (TID 60, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:14:30,198 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 20.0 (TID 60)
2020-04-08 12:14:30,199 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 1
2020-04-08 12:14:30,199 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 20.0 (TID 60). 679 bytes result sent to driver
2020-04-08 12:14:30,200 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 20.0 (TID 61, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:14:30,200 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 20.0 (TID 61)
2020-04-08 12:14:30,200 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 20.0 (TID 60) in 2 ms on localhost (executor driver) (1/3)
2020-04-08 12:14:30,200 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 2
2020-04-08 12:14:30,201 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 20.0 (TID 61). 679 bytes result sent to driver
2020-04-08 12:14:30,201 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 20.0 (TID 62, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:14:30,201 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 20.0 (TID 61) in 2 ms on localhost (executor driver) (2/3)
2020-04-08 12:14:30,201 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 20.0 (TID 62)
2020-04-08 12:14:30,202 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 12:14:30,202 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 20.0 (TID 62). 679 bytes result sent to driver
2020-04-08 12:14:30,203 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 20.0 (TID 62) in 2 ms on localhost (executor driver) (3/3)
2020-04-08 12:14:30,203 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool 
2020-04-08 12:14:30,203 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 20 (collect at Kafka2SparkStreaming2Kudu.scala:141) finished in 0.009 s
2020-04-08 12:14:30,203 INFO org.apache.spark.scheduler.DAGScheduler: Job 20 finished: collect at Kafka2SparkStreaming2Kudu.scala:141, took 0.011329 s
2020-04-08 12:14:30,208 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586319270000 ms.0 from job set of time 1586319270000 ms
2020-04-08 12:14:30,208 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.208 s for time 1586319270000 ms (execution: 0.019 s)
2020-04-08 12:14:30,208 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 57 from persistence list
2020-04-08 12:14:30,209 INFO org.apache.spark.storage.BlockManager: Removing RDD 57
2020-04-08 12:14:30,209 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 12:14:30,209 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586319250000 ms
2020-04-08 12:14:30,223 WARN org.apache.spark.util.Utils: Your hostname, MacBook-Pro-CW.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)
2020-04-08 12:14:30,224 WARN org.apache.spark.util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
2020-04-08 12:14:30,259 INFO org.apache.spark.SparkContext: Running Spark version 2.4.0.cloudera2
2020-04-08 12:14:30,449 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-04-08 12:14:30,581 INFO org.apache.spark.SparkContext: Submitted application: Kafka2-kerberos
2020-04-08 12:14:30,622 INFO org.apache.spark.SecurityManager: Changing view acls to: godiswc
2020-04-08 12:14:30,623 INFO org.apache.spark.SecurityManager: Changing modify acls to: godiswc
2020-04-08 12:14:30,624 INFO org.apache.spark.SecurityManager: Changing view acls groups to: 
2020-04-08 12:14:30,624 INFO org.apache.spark.SecurityManager: Changing modify acls groups to: 
2020-04-08 12:14:30,624 INFO org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(godiswc); groups with view permissions: Set(); users  with modify permissions: Set(godiswc); groups with modify permissions: Set()
2020-04-08 12:14:30,776 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 12:14:30,855 INFO org.apache.spark.util.Utils: Successfully started service 'sparkDriver' on port 53519.
2020-04-08 12:14:30,871 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
2020-04-08 12:14:30,882 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
2020-04-08 12:14:30,883 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-04-08 12:14:30,884 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2020-04-08 12:14:30,894 INFO org.apache.spark.storage.DiskBlockManager: Created local directory at /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/blockmgr-c327603a-d23a-4584-bc86-0640938e3ffb
2020-04-08 12:14:30,924 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 4.1 GB
2020-04-08 12:14:30,932 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
2020-04-08 12:14:30,984 INFO org.spark_project.jetty.util.log: Logging initialized @1533ms
2020-04-08 12:14:31,039 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2019-04-30T04:42:08+08:00, git hash: e1bc35120a6617ee3df052294e433f3a25ce7097
2020-04-08 12:14:31,051 INFO org.spark_project.jetty.server.Server: Started @1601ms
2020-04-08 12:14:31,054 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 12:14:31,064 WARN org.apache.spark.util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2020-04-08 12:14:31,072 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@757d6814{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
2020-04-08 12:14:31,072 INFO org.apache.spark.util.Utils: Successfully started service 'SparkUI' on port 4041.
2020-04-08 12:14:31,091 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@117e0fe5{/jobs,null,AVAILABLE,@Spark}
2020-04-08 12:14:31,092 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@9635fa{/jobs/json,null,AVAILABLE,@Spark}
2020-04-08 12:14:31,093 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@53ab0286{/jobs/job,null,AVAILABLE,@Spark}
2020-04-08 12:14:31,097 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1c98290c{/jobs/job/json,null,AVAILABLE,@Spark}
2020-04-08 12:14:31,097 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@172ca72b{/stages,null,AVAILABLE,@Spark}
2020-04-08 12:14:31,098 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5bda80bf{/stages/json,null,AVAILABLE,@Spark}
2020-04-08 12:14:31,098 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@71e5f61d{/stages/stage,null,AVAILABLE,@Spark}
2020-04-08 12:14:31,099 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@51df223b{/stages/stage/json,null,AVAILABLE,@Spark}
2020-04-08 12:14:31,099 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@fd46303{/stages/pool,null,AVAILABLE,@Spark}
2020-04-08 12:14:31,100 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60d8c0dc{/stages/pool/json,null,AVAILABLE,@Spark}
2020-04-08 12:14:31,100 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4204541c{/storage,null,AVAILABLE,@Spark}
2020-04-08 12:14:31,101 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a62689d{/storage/json,null,AVAILABLE,@Spark}
2020-04-08 12:14:31,101 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4602c2a9{/storage/rdd,null,AVAILABLE,@Spark}
2020-04-08 12:14:31,102 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60fa3495{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-04-08 12:14:31,102 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3e2822{/environment,null,AVAILABLE,@Spark}
2020-04-08 12:14:31,103 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@79e18e38{/environment/json,null,AVAILABLE,@Spark}
2020-04-08 12:14:31,104 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@29a60c27{/executors,null,AVAILABLE,@Spark}
2020-04-08 12:14:31,104 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1849db1a{/executors/json,null,AVAILABLE,@Spark}
2020-04-08 12:14:31,104 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@69c79f09{/executors/threadDump,null,AVAILABLE,@Spark}
2020-04-08 12:14:31,105 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1ca25c47{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-04-08 12:14:31,110 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fcacc0{/static,null,AVAILABLE,@Spark}
2020-04-08 12:14:31,110 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@94f6bfb{/,null,AVAILABLE,@Spark}
2020-04-08 12:14:31,111 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@34645867{/api,null,AVAILABLE,@Spark}
2020-04-08 12:14:31,112 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@464649c{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-04-08 12:14:31,112 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7c22d4f{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-04-08 12:14:31,114 INFO org.apache.spark.ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4041
2020-04-08 12:14:31,242 INFO org.apache.spark.executor.Executor: Starting executor ID driver on host localhost
2020-04-08 12:14:31,322 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 12:14:31,323 INFO org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53521.
2020-04-08 12:14:31,324 INFO org.apache.spark.network.netty.NettyBlockTransferService: Server created on 192.168.0.102:53521
2020-04-08 12:14:31,325 INFO org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-04-08 12:14:31,347 INFO org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 53521, None)
2020-04-08 12:14:31,349 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:53521 with 4.1 GB RAM, BlockManagerId(driver, 192.168.0.102, 53521, None)
2020-04-08 12:14:31,352 INFO org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 53521, None)
2020-04-08 12:14:31,352 INFO org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 53521, None)
2020-04-08 12:14:31,536 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@55120f99{/metrics/json,null,AVAILABLE,@Spark}
2020-04-08 12:14:31,882 INFO org.apache.spark.SparkContext: Starting job: collect at test.scala:16
2020-04-08 12:14:31,900 INFO org.apache.spark.scheduler.DAGScheduler: Got job 0 (collect at test.scala:16) with 1 output partitions
2020-04-08 12:14:31,900 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 0 (collect at test.scala:16)
2020-04-08 12:14:31,901 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 12:14:31,902 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 12:14:31,906 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at test.scala:14), which has no missing parents
2020-04-08 12:14:31,965 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 1448.0 B, free 4.1 GB)
2020-04-08 12:14:32,345 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 975.0 B, free 4.1 GB)
2020-04-08 12:14:32,346 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:53521 (size: 975.0 B, free: 4.1 GB)
2020-04-08 12:14:32,348 INFO org.apache.spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2020-04-08 12:14:32,363 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at test.scala:14) (first 15 tasks are for partitions Vector(0))
2020-04-08 12:14:32,364 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
2020-04-08 12:14:32,435 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 9516 bytes)
2020-04-08 12:14:32,443 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
2020-04-08 12:14:32,482 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 2379 bytes result sent to driver
2020-04-08 12:14:32,487 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 65 ms on localhost (executor driver) (1/1)
2020-04-08 12:14:32,490 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-04-08 12:14:32,494 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 0 (collect at test.scala:16) finished in 0.573 s
2020-04-08 12:14:32,498 INFO org.apache.spark.scheduler.DAGScheduler: Job 0 finished: collect at test.scala:16, took 0.616262 s
2020-04-08 12:14:32,504 INFO org.apache.spark.SparkContext: Invoking stop() from shutdown hook
2020-04-08 12:14:32,515 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@757d6814{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
2020-04-08 12:14:32,517 INFO org.apache.spark.ui.SparkUI: Stopped Spark web UI at http://192.168.0.102:4041
2020-04-08 12:14:32,524 INFO org.apache.spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2020-04-08 12:14:32,536 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2020-04-08 12:14:32,537 INFO org.apache.spark.storage.BlockManager: BlockManager stopped
2020-04-08 12:14:32,540 INFO org.apache.spark.storage.BlockManagerMaster: BlockManagerMaster stopped
2020-04-08 12:14:32,542 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2020-04-08 12:14:32,550 INFO org.apache.spark.SparkContext: Successfully stopped SparkContext
2020-04-08 12:14:32,550 INFO org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2020-04-08 12:14:32,551 INFO org.apache.spark.util.ShutdownHookManager: Deleting directory /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/spark-374690f4-7e06-4c3b-9c8f-b5ff5576e696
2020-04-08 12:14:40,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 12:14:40,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 12:14:40,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 12:14:40,179 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 12:14:40,179 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 151.
2020-04-08 12:14:40,181 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 151.
2020-04-08 12:14:40,182 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586319280000 ms
2020-04-08 12:14:40,182 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586319280000 ms.0 from job set of time 1586319280000 ms
2020-04-08 12:14:40,187 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:141
2020-04-08 12:14:40,187 INFO org.apache.spark.scheduler.DAGScheduler: Got job 21 (collect at Kafka2SparkStreaming2Kudu.scala:141) with 3 output partitions
2020-04-08 12:14:40,187 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 21 (collect at Kafka2SparkStreaming2Kudu.scala:141)
2020-04-08 12:14:40,187 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 12:14:40,187 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 12:14:40,188 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[64] at map at Kafka2SparkStreaming2Kudu.scala:126), which has no missing parents
2020-04-08 12:14:40,190 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 12:14:40,191 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 12:14:40,191 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 192.168.0.102:52741 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:14:40,192 INFO org.apache.spark.SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1161
2020-04-08 12:14:40,192 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 21 (MapPartitionsRDD[64] at map at Kafka2SparkStreaming2Kudu.scala:126) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 12:14:40,192 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 21.0 with 3 tasks
2020-04-08 12:14:40,193 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 21.0 (TID 63, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:14:40,193 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 21.0 (TID 63)
2020-04-08 12:14:40,194 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 1
2020-04-08 12:14:40,194 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 21.0 (TID 63). 679 bytes result sent to driver
2020-04-08 12:14:40,194 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 21.0 (TID 64, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:14:40,195 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 21.0 (TID 64)
2020-04-08 12:14:40,195 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 21.0 (TID 63) in 3 ms on localhost (executor driver) (1/3)
2020-04-08 12:14:40,195 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 2
2020-04-08 12:14:40,196 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 21.0 (TID 64). 679 bytes result sent to driver
2020-04-08 12:14:40,196 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 21.0 (TID 65, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:14:40,196 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 21.0 (TID 64) in 2 ms on localhost (executor driver) (2/3)
2020-04-08 12:14:40,196 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 21.0 (TID 65)
2020-04-08 12:14:40,197 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 12:14:40,197 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 21.0 (TID 65). 679 bytes result sent to driver
2020-04-08 12:14:40,198 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 21.0 (TID 65) in 2 ms on localhost (executor driver) (3/3)
2020-04-08 12:14:40,198 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool 
2020-04-08 12:14:40,198 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 21 (collect at Kafka2SparkStreaming2Kudu.scala:141) finished in 0.009 s
2020-04-08 12:14:40,198 INFO org.apache.spark.scheduler.DAGScheduler: Job 21 finished: collect at Kafka2SparkStreaming2Kudu.scala:141, took 0.011419 s
2020-04-08 12:14:40,202 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586319280000 ms.0 from job set of time 1586319280000 ms
2020-04-08 12:14:40,202 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.202 s for time 1586319280000 ms (execution: 0.020 s)
2020-04-08 12:14:40,202 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 60 from persistence list
2020-04-08 12:14:40,203 INFO org.apache.spark.storage.BlockManager: Removing RDD 60
2020-04-08 12:14:40,203 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 12:14:40,203 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586319260000 ms
2020-04-08 12:14:50,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 12:14:50,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 12:14:50,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 12:14:50,188 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 12:14:50,188 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 151.
2020-04-08 12:14:50,188 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 151.
2020-04-08 12:14:50,189 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586319290000 ms
2020-04-08 12:14:50,189 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586319290000 ms.0 from job set of time 1586319290000 ms
2020-04-08 12:14:50,193 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:141
2020-04-08 12:14:50,194 INFO org.apache.spark.scheduler.DAGScheduler: Got job 22 (collect at Kafka2SparkStreaming2Kudu.scala:141) with 3 output partitions
2020-04-08 12:14:50,194 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 22 (collect at Kafka2SparkStreaming2Kudu.scala:141)
2020-04-08 12:14:50,194 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 12:14:50,194 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 12:14:50,194 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[67] at map at Kafka2SparkStreaming2Kudu.scala:126), which has no missing parents
2020-04-08 12:14:50,196 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 12:14:50,198 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 12:14:50,199 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 192.168.0.102:52741 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:14:50,199 INFO org.apache.spark.SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1161
2020-04-08 12:14:50,199 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 22 (MapPartitionsRDD[67] at map at Kafka2SparkStreaming2Kudu.scala:126) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 12:14:50,199 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 22.0 with 3 tasks
2020-04-08 12:14:50,200 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 22.0 (TID 66, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:14:50,200 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 22.0 (TID 66)
2020-04-08 12:14:50,201 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 1
2020-04-08 12:14:50,201 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 22.0 (TID 66). 679 bytes result sent to driver
2020-04-08 12:14:50,201 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 22.0 (TID 67, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:14:50,202 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 22.0 (TID 66) in 2 ms on localhost (executor driver) (1/3)
2020-04-08 12:14:50,202 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 22.0 (TID 67)
2020-04-08 12:14:50,202 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 2
2020-04-08 12:14:50,203 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 22.0 (TID 67). 636 bytes result sent to driver
2020-04-08 12:14:50,203 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 22.0 (TID 68, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:14:50,203 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 22.0 (TID 67) in 2 ms on localhost (executor driver) (2/3)
2020-04-08 12:14:50,203 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 22.0 (TID 68)
2020-04-08 12:14:50,204 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 12:14:50,204 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 22.0 (TID 68). 679 bytes result sent to driver
2020-04-08 12:14:50,205 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 22.0 (TID 68) in 2 ms on localhost (executor driver) (3/3)
2020-04-08 12:14:50,205 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool 
2020-04-08 12:14:50,205 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 22 (collect at Kafka2SparkStreaming2Kudu.scala:141) finished in 0.010 s
2020-04-08 12:14:50,205 INFO org.apache.spark.scheduler.DAGScheduler: Job 22 finished: collect at Kafka2SparkStreaming2Kudu.scala:141, took 0.011912 s
2020-04-08 12:14:50,209 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586319290000 ms.0 from job set of time 1586319290000 ms
2020-04-08 12:14:50,209 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.209 s for time 1586319290000 ms (execution: 0.020 s)
2020-04-08 12:14:50,209 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 63 from persistence list
2020-04-08 12:14:50,210 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 12:14:50,210 INFO org.apache.spark.storage.BlockManager: Removing RDD 63
2020-04-08 12:14:50,210 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586319270000 ms
2020-04-08 12:15:00,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 12:15:00,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 12:15:00,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 12:15:00,195 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 151.
2020-04-08 12:15:00,195 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 12:15:00,197 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 151.
2020-04-08 12:15:00,198 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586319300000 ms
2020-04-08 12:15:00,198 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586319300000 ms.0 from job set of time 1586319300000 ms
2020-04-08 12:15:00,202 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:141
2020-04-08 12:15:00,203 INFO org.apache.spark.scheduler.DAGScheduler: Got job 23 (collect at Kafka2SparkStreaming2Kudu.scala:141) with 3 output partitions
2020-04-08 12:15:00,203 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 23 (collect at Kafka2SparkStreaming2Kudu.scala:141)
2020-04-08 12:15:00,203 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 12:15:00,203 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 12:15:00,203 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[70] at map at Kafka2SparkStreaming2Kudu.scala:126), which has no missing parents
2020-04-08 12:15:00,205 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 12:15:00,207 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 12:15:00,208 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 192.168.0.102:52741 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:15:00,208 INFO org.apache.spark.SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1161
2020-04-08 12:15:00,208 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 23 (MapPartitionsRDD[70] at map at Kafka2SparkStreaming2Kudu.scala:126) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 12:15:00,208 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 23.0 with 3 tasks
2020-04-08 12:15:00,209 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 23.0 (TID 69, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:15:00,209 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 23.0 (TID 69)
2020-04-08 12:15:00,210 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 1
2020-04-08 12:15:00,210 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 23.0 (TID 69). 679 bytes result sent to driver
2020-04-08 12:15:00,211 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 23.0 (TID 70, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:15:00,211 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 23.0 (TID 69) in 2 ms on localhost (executor driver) (1/3)
2020-04-08 12:15:00,211 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 23.0 (TID 70)
2020-04-08 12:15:00,212 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 2
2020-04-08 12:15:00,212 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 23.0 (TID 70). 679 bytes result sent to driver
2020-04-08 12:15:00,212 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 23.0 (TID 71, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:15:00,212 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 23.0 (TID 70) in 1 ms on localhost (executor driver) (2/3)
2020-04-08 12:15:00,212 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 23.0 (TID 71)
2020-04-08 12:15:00,213 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 12:15:00,214 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 23.0 (TID 71). 679 bytes result sent to driver
2020-04-08 12:15:00,214 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 23.0 (TID 71) in 2 ms on localhost (executor driver) (3/3)
2020-04-08 12:15:00,214 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool 
2020-04-08 12:15:00,214 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 23 (collect at Kafka2SparkStreaming2Kudu.scala:141) finished in 0.010 s
2020-04-08 12:15:00,214 INFO org.apache.spark.scheduler.DAGScheduler: Job 23 finished: collect at Kafka2SparkStreaming2Kudu.scala:141, took 0.012290 s
2020-04-08 12:15:00,218 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586319300000 ms.0 from job set of time 1586319300000 ms
2020-04-08 12:15:00,219 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.218 s for time 1586319300000 ms (execution: 0.020 s)
2020-04-08 12:15:00,219 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 66 from persistence list
2020-04-08 12:15:00,219 INFO org.apache.spark.storage.BlockManager: Removing RDD 66
2020-04-08 12:15:00,219 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 12:15:00,219 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586319280000 ms
2020-04-08 12:15:10,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 12:15:10,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 12:15:10,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 12:15:10,183 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 12:15:10,184 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 151.
2020-04-08 12:15:10,184 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 151.
2020-04-08 12:15:10,185 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586319310000 ms
2020-04-08 12:15:10,185 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586319310000 ms.0 from job set of time 1586319310000 ms
2020-04-08 12:15:10,190 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:141
2020-04-08 12:15:10,191 INFO org.apache.spark.scheduler.DAGScheduler: Got job 24 (collect at Kafka2SparkStreaming2Kudu.scala:141) with 3 output partitions
2020-04-08 12:15:10,191 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 24 (collect at Kafka2SparkStreaming2Kudu.scala:141)
2020-04-08 12:15:10,191 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 12:15:10,191 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 12:15:10,191 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[73] at map at Kafka2SparkStreaming2Kudu.scala:126), which has no missing parents
2020-04-08 12:15:10,193 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 12:15:10,196 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 12:15:10,196 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 192.168.0.102:52741 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:15:10,196 INFO org.apache.spark.SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1161
2020-04-08 12:15:10,197 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 24 (MapPartitionsRDD[73] at map at Kafka2SparkStreaming2Kudu.scala:126) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 12:15:10,197 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 24.0 with 3 tasks
2020-04-08 12:15:10,197 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 24.0 (TID 72, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:15:10,197 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 24.0 (TID 72)
2020-04-08 12:15:10,198 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 1
2020-04-08 12:15:10,199 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 24.0 (TID 72). 722 bytes result sent to driver
2020-04-08 12:15:10,199 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 24.0 (TID 73, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:15:10,199 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 24.0 (TID 72) in 2 ms on localhost (executor driver) (1/3)
2020-04-08 12:15:10,199 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 24.0 (TID 73)
2020-04-08 12:15:10,200 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 2
2020-04-08 12:15:10,200 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 24.0 (TID 73). 679 bytes result sent to driver
2020-04-08 12:15:10,201 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 24.0 (TID 74, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:15:10,201 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 24.0 (TID 73) in 2 ms on localhost (executor driver) (2/3)
2020-04-08 12:15:10,201 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 24.0 (TID 74)
2020-04-08 12:15:10,202 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 12:15:10,202 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 24.0 (TID 74). 679 bytes result sent to driver
2020-04-08 12:15:10,203 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 24.0 (TID 74) in 2 ms on localhost (executor driver) (3/3)
2020-04-08 12:15:10,203 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool 
2020-04-08 12:15:10,203 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 24 (collect at Kafka2SparkStreaming2Kudu.scala:141) finished in 0.011 s
2020-04-08 12:15:10,203 INFO org.apache.spark.scheduler.DAGScheduler: Job 24 finished: collect at Kafka2SparkStreaming2Kudu.scala:141, took 0.012945 s
2020-04-08 12:15:10,208 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586319310000 ms.0 from job set of time 1586319310000 ms
2020-04-08 12:15:10,208 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.208 s for time 1586319310000 ms (execution: 0.023 s)
2020-04-08 12:15:10,208 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 69 from persistence list
2020-04-08 12:15:10,208 INFO org.apache.spark.storage.BlockManager: Removing RDD 69
2020-04-08 12:15:10,209 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 12:15:10,209 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586319290000 ms
2020-04-08 12:15:20,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 12:15:20,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 12:15:20,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 12:15:20,182 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 151.
2020-04-08 12:15:20,182 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 151.
2020-04-08 12:15:20,183 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 12:15:20,186 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586319320000 ms
2020-04-08 12:15:20,187 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586319320000 ms.0 from job set of time 1586319320000 ms
2020-04-08 12:15:20,201 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:141
2020-04-08 12:15:20,202 INFO org.apache.spark.scheduler.DAGScheduler: Got job 25 (collect at Kafka2SparkStreaming2Kudu.scala:141) with 3 output partitions
2020-04-08 12:15:20,202 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 25 (collect at Kafka2SparkStreaming2Kudu.scala:141)
2020-04-08 12:15:20,202 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 12:15:20,202 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 12:15:20,203 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[76] at map at Kafka2SparkStreaming2Kudu.scala:126), which has no missing parents
2020-04-08 12:15:20,209 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 12:15:20,217 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 12:15:20,218 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 192.168.0.102:52741 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:15:20,218 INFO org.apache.spark.SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1161
2020-04-08 12:15:20,219 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 25 (MapPartitionsRDD[76] at map at Kafka2SparkStreaming2Kudu.scala:126) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 12:15:20,219 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 25.0 with 3 tasks
2020-04-08 12:15:20,220 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 25.0 (TID 75, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:15:20,221 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 25.0 (TID 75)
2020-04-08 12:15:20,222 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 1
2020-04-08 12:15:20,222 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 25.0 (TID 75). 679 bytes result sent to driver
2020-04-08 12:15:20,223 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 25.0 (TID 76, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:15:20,223 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 25.0 (TID 75) in 3 ms on localhost (executor driver) (1/3)
2020-04-08 12:15:20,223 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 25.0 (TID 76)
2020-04-08 12:15:20,226 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 2
2020-04-08 12:15:20,226 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 25.0 (TID 76). 679 bytes result sent to driver
2020-04-08 12:15:20,226 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 25.0 (TID 77, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:15:20,227 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 25.0 (TID 76) in 4 ms on localhost (executor driver) (2/3)
2020-04-08 12:15:20,227 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 25.0 (TID 77)
2020-04-08 12:15:20,228 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 12:15:20,228 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 25.0 (TID 77). 679 bytes result sent to driver
2020-04-08 12:15:20,228 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 25.0 (TID 77) in 2 ms on localhost (executor driver) (3/3)
2020-04-08 12:15:20,228 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool 
2020-04-08 12:15:20,229 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 25 (collect at Kafka2SparkStreaming2Kudu.scala:141) finished in 0.024 s
2020-04-08 12:15:20,229 INFO org.apache.spark.scheduler.DAGScheduler: Job 25 finished: collect at Kafka2SparkStreaming2Kudu.scala:141, took 0.027893 s
2020-04-08 12:15:20,243 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586319320000 ms.0 from job set of time 1586319320000 ms
2020-04-08 12:15:20,243 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.243 s for time 1586319320000 ms (execution: 0.057 s)
2020-04-08 12:15:20,243 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 72 from persistence list
2020-04-08 12:15:20,245 INFO org.apache.spark.storage.BlockManager: Removing RDD 72
2020-04-08 12:15:20,245 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 12:15:20,245 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586319300000 ms
2020-04-08 12:15:30,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 12:15:30,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 12:15:30,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 12:15:30,185 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 151.
2020-04-08 12:15:30,185 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 151.
2020-04-08 12:15:30,186 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 12:15:30,186 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586319330000 ms
2020-04-08 12:15:30,187 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586319330000 ms.0 from job set of time 1586319330000 ms
2020-04-08 12:15:30,191 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:141
2020-04-08 12:15:30,191 INFO org.apache.spark.scheduler.DAGScheduler: Got job 26 (collect at Kafka2SparkStreaming2Kudu.scala:141) with 3 output partitions
2020-04-08 12:15:30,191 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 26 (collect at Kafka2SparkStreaming2Kudu.scala:141)
2020-04-08 12:15:30,191 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 12:15:30,192 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 12:15:30,192 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[79] at map at Kafka2SparkStreaming2Kudu.scala:126), which has no missing parents
2020-04-08 12:15:30,194 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 12:15:30,196 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 12:15:30,196 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 192.168.0.102:52741 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:15:30,197 INFO org.apache.spark.SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1161
2020-04-08 12:15:30,197 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 26 (MapPartitionsRDD[79] at map at Kafka2SparkStreaming2Kudu.scala:126) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 12:15:30,197 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 26.0 with 3 tasks
2020-04-08 12:15:30,198 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 26.0 (TID 78, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:15:30,198 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 26.0 (TID 78)
2020-04-08 12:15:30,199 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 1
2020-04-08 12:15:30,199 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 26.0 (TID 78). 679 bytes result sent to driver
2020-04-08 12:15:30,200 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 26.0 (TID 79, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:15:30,200 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 26.0 (TID 78) in 2 ms on localhost (executor driver) (1/3)
2020-04-08 12:15:30,200 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 26.0 (TID 79)
2020-04-08 12:15:30,200 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 2
2020-04-08 12:15:30,201 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 26.0 (TID 79). 679 bytes result sent to driver
2020-04-08 12:15:30,201 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 26.0 (TID 80, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:15:30,201 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 26.0 (TID 80)
2020-04-08 12:15:30,201 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 26.0 (TID 79) in 2 ms on localhost (executor driver) (2/3)
2020-04-08 12:15:30,202 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 12:15:30,202 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 26.0 (TID 80). 679 bytes result sent to driver
2020-04-08 12:15:30,203 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 26.0 (TID 80) in 2 ms on localhost (executor driver) (3/3)
2020-04-08 12:15:30,203 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool 
2020-04-08 12:15:30,203 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 26 (collect at Kafka2SparkStreaming2Kudu.scala:141) finished in 0.010 s
2020-04-08 12:15:30,203 INFO org.apache.spark.scheduler.DAGScheduler: Job 26 finished: collect at Kafka2SparkStreaming2Kudu.scala:141, took 0.012226 s
2020-04-08 12:15:30,208 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586319330000 ms.0 from job set of time 1586319330000 ms
2020-04-08 12:15:30,208 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.208 s for time 1586319330000 ms (execution: 0.021 s)
2020-04-08 12:15:30,208 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 75 from persistence list
2020-04-08 12:15:30,208 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 12:15:30,208 INFO org.apache.spark.storage.BlockManager: Removing RDD 75
2020-04-08 12:15:30,208 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586319310000 ms
2020-04-08 12:15:40,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 12:15:40,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 12:15:40,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 12:15:40,185 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 12:15:40,186 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 151.
2020-04-08 12:15:40,186 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 151.
2020-04-08 12:15:40,187 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586319340000 ms
2020-04-08 12:15:40,187 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586319340000 ms.0 from job set of time 1586319340000 ms
2020-04-08 12:15:40,191 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:141
2020-04-08 12:15:40,192 INFO org.apache.spark.scheduler.DAGScheduler: Got job 27 (collect at Kafka2SparkStreaming2Kudu.scala:141) with 3 output partitions
2020-04-08 12:15:40,192 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 27 (collect at Kafka2SparkStreaming2Kudu.scala:141)
2020-04-08 12:15:40,192 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 12:15:40,192 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 12:15:40,192 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 27 (MapPartitionsRDD[82] at map at Kafka2SparkStreaming2Kudu.scala:126), which has no missing parents
2020-04-08 12:15:40,194 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 12:15:40,196 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 12:15:40,197 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on 192.168.0.102:52741 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:15:40,197 INFO org.apache.spark.SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1161
2020-04-08 12:15:40,197 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 27 (MapPartitionsRDD[82] at map at Kafka2SparkStreaming2Kudu.scala:126) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 12:15:40,198 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 27.0 with 3 tasks
2020-04-08 12:15:40,198 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 27.0 (TID 81, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:15:40,198 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 27.0 (TID 81)
2020-04-08 12:15:40,199 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 1
2020-04-08 12:15:40,200 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 27.0 (TID 81). 679 bytes result sent to driver
2020-04-08 12:15:40,200 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 27.0 (TID 82, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:15:40,200 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 27.0 (TID 82)
2020-04-08 12:15:40,200 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 27.0 (TID 81) in 2 ms on localhost (executor driver) (1/3)
2020-04-08 12:15:40,201 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 2
2020-04-08 12:15:40,201 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 27.0 (TID 82). 679 bytes result sent to driver
2020-04-08 12:15:40,202 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 27.0 (TID 83, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:15:40,202 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 27.0 (TID 82) in 2 ms on localhost (executor driver) (2/3)
2020-04-08 12:15:40,202 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 27.0 (TID 83)
2020-04-08 12:15:40,203 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 12:15:40,203 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 27.0 (TID 83). 679 bytes result sent to driver
2020-04-08 12:15:40,203 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 27.0 (TID 83) in 1 ms on localhost (executor driver) (3/3)
2020-04-08 12:15:40,203 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool 
2020-04-08 12:15:40,204 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 27 (collect at Kafka2SparkStreaming2Kudu.scala:141) finished in 0.011 s
2020-04-08 12:15:40,204 INFO org.apache.spark.scheduler.DAGScheduler: Job 27 finished: collect at Kafka2SparkStreaming2Kudu.scala:141, took 0.012474 s
2020-04-08 12:15:40,209 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586319340000 ms.0 from job set of time 1586319340000 ms
2020-04-08 12:15:40,209 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.209 s for time 1586319340000 ms (execution: 0.022 s)
2020-04-08 12:15:40,209 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 78 from persistence list
2020-04-08 12:15:40,210 INFO org.apache.spark.storage.BlockManager: Removing RDD 78
2020-04-08 12:15:40,210 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 12:15:40,210 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586319320000 ms
2020-04-08 12:15:50,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 12:15:50,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 12:15:50,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 12:15:50,182 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 12:15:50,182 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 151.
2020-04-08 12:15:50,186 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 151.
2020-04-08 12:15:50,187 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586319350000 ms
2020-04-08 12:15:50,187 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586319350000 ms.0 from job set of time 1586319350000 ms
2020-04-08 12:15:50,192 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:141
2020-04-08 12:15:50,192 INFO org.apache.spark.scheduler.DAGScheduler: Got job 28 (collect at Kafka2SparkStreaming2Kudu.scala:141) with 3 output partitions
2020-04-08 12:15:50,192 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 28 (collect at Kafka2SparkStreaming2Kudu.scala:141)
2020-04-08 12:15:50,192 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 12:15:50,192 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 12:15:50,193 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[85] at map at Kafka2SparkStreaming2Kudu.scala:126), which has no missing parents
2020-04-08 12:15:50,194 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_28 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 12:15:50,197 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 12:15:50,197 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on 192.168.0.102:52741 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:15:50,197 INFO org.apache.spark.SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1161
2020-04-08 12:15:50,197 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 28 (MapPartitionsRDD[85] at map at Kafka2SparkStreaming2Kudu.scala:126) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 12:15:50,198 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 28.0 with 3 tasks
2020-04-08 12:15:50,198 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 28.0 (TID 84, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:15:50,198 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 28.0 (TID 84)
2020-04-08 12:15:50,199 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 1
2020-04-08 12:15:50,199 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 28.0 (TID 84). 679 bytes result sent to driver
2020-04-08 12:15:50,200 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 28.0 (TID 85, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:15:50,200 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 28.0 (TID 85)
2020-04-08 12:15:50,200 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 28.0 (TID 84) in 2 ms on localhost (executor driver) (1/3)
2020-04-08 12:15:50,201 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 2
2020-04-08 12:15:50,201 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 28.0 (TID 85). 679 bytes result sent to driver
2020-04-08 12:15:50,201 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 28.0 (TID 86, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:15:50,202 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 28.0 (TID 86)
2020-04-08 12:15:50,202 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 28.0 (TID 85) in 2 ms on localhost (executor driver) (2/3)
2020-04-08 12:15:50,202 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 12:15:50,203 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 28.0 (TID 86). 636 bytes result sent to driver
2020-04-08 12:15:50,203 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 28.0 (TID 86) in 2 ms on localhost (executor driver) (3/3)
2020-04-08 12:15:50,203 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool 
2020-04-08 12:15:50,203 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 28 (collect at Kafka2SparkStreaming2Kudu.scala:141) finished in 0.010 s
2020-04-08 12:15:50,203 INFO org.apache.spark.scheduler.DAGScheduler: Job 28 finished: collect at Kafka2SparkStreaming2Kudu.scala:141, took 0.011641 s
2020-04-08 12:15:50,207 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586319350000 ms.0 from job set of time 1586319350000 ms
2020-04-08 12:15:50,208 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.207 s for time 1586319350000 ms (execution: 0.020 s)
2020-04-08 12:15:50,208 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 81 from persistence list
2020-04-08 12:15:50,208 INFO org.apache.spark.storage.BlockManager: Removing RDD 81
2020-04-08 12:15:50,208 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 12:15:50,208 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586319330000 ms
2020-04-08 12:15:59,522 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 230
2020-04-08 12:15:59,522 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 336
2020-04-08 12:15:59,522 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 391
2020-04-08 12:15:59,528 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_7_piece0 on 192.168.0.102:52741 in memory (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:15:59,529 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 98
2020-04-08 12:15:59,529 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 459
2020-04-08 12:15:59,529 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 134
2020-04-08 12:15:59,530 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_20_piece0 on 192.168.0.102:52741 in memory (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:15:59,531 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 717
2020-04-08 12:15:59,531 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 668
2020-04-08 12:15:59,532 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 531
2020-04-08 12:15:59,532 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 505
2020-04-08 12:15:59,532 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_23_piece0 on 192.168.0.102:52741 in memory (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:15:59,534 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 718
2020-04-08 12:15:59,534 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 590
2020-04-08 12:15:59,534 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 667
2020-04-08 12:15:59,534 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 205
2020-04-08 12:15:59,534 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 553
2020-04-08 12:15:59,534 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 105
2020-04-08 12:15:59,534 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 287
2020-04-08 12:15:59,534 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 700
2020-04-08 12:15:59,534 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 479
2020-04-08 12:15:59,534 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 213
2020-04-08 12:15:59,534 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 624
2020-04-08 12:15:59,534 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 698
2020-04-08 12:15:59,534 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 269
2020-04-08 12:15:59,534 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 406
2020-04-08 12:15:59,534 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 354
2020-04-08 12:15:59,534 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 13
2020-04-08 12:15:59,534 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 154
2020-04-08 12:15:59,534 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 599
2020-04-08 12:15:59,534 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 637
2020-04-08 12:15:59,534 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 163
2020-04-08 12:15:59,534 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 102
2020-04-08 12:15:59,534 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 271
2020-04-08 12:15:59,534 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 538
2020-04-08 12:15:59,534 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 331
2020-04-08 12:15:59,534 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 114
2020-04-08 12:15:59,534 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 255
2020-04-08 12:15:59,534 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 608
2020-04-08 12:15:59,534 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 429
2020-04-08 12:15:59,534 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 690
2020-04-08 12:15:59,534 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 337
2020-04-08 12:15:59,534 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 467
2020-04-08 12:15:59,534 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 260
2020-04-08 12:15:59,535 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 405
2020-04-08 12:15:59,535 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 150
2020-04-08 12:15:59,535 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 477
2020-04-08 12:15:59,535 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 292
2020-04-08 12:15:59,535 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 630
2020-04-08 12:15:59,535 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 364
2020-04-08 12:15:59,535 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 23
2020-04-08 12:15:59,535 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 652
2020-04-08 12:15:59,535 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 339
2020-04-08 12:15:59,535 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 200
2020-04-08 12:15:59,536 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.0.102:52741 in memory (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:15:59,537 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 195
2020-04-08 12:15:59,537 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 545
2020-04-08 12:15:59,537 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 199
2020-04-08 12:15:59,537 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 716
2020-04-08 12:15:59,537 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 594
2020-04-08 12:15:59,537 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 457
2020-04-08 12:15:59,537 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 515
2020-04-08 12:15:59,537 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 526
2020-04-08 12:15:59,537 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 724
2020-04-08 12:15:59,537 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 36
2020-04-08 12:15:59,537 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 463
2020-04-08 12:15:59,537 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 20
2020-04-08 12:15:59,537 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 573
2020-04-08 12:15:59,537 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 672
2020-04-08 12:15:59,537 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 330
2020-04-08 12:15:59,537 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 349
2020-04-08 12:15:59,537 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 381
2020-04-08 12:15:59,537 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 587
2020-04-08 12:15:59,537 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 547
2020-04-08 12:15:59,537 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 695
2020-04-08 12:15:59,537 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 234
2020-04-08 12:15:59,537 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 81
2020-04-08 12:15:59,537 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 53
2020-04-08 12:15:59,537 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 498
2020-04-08 12:15:59,537 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 157
2020-04-08 12:15:59,537 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 151
2020-04-08 12:15:59,537 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 167
2020-04-08 12:15:59,538 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_28_piece0 on 192.168.0.102:52741 in memory (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:15:59,539 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 651
2020-04-08 12:15:59,540 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 592
2020-04-08 12:15:59,540 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 678
2020-04-08 12:15:59,540 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 348
2020-04-08 12:15:59,540 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 493
2020-04-08 12:15:59,540 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 375
2020-04-08 12:15:59,540 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 63
2020-04-08 12:15:59,540 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 696
2020-04-08 12:15:59,540 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 97
2020-04-08 12:15:59,540 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 89
2020-04-08 12:15:59,540 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 241
2020-04-08 12:15:59,540 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_6_piece0 on 192.168.0.102:52741 in memory (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:15:59,541 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 4
2020-04-08 12:15:59,542 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 373
2020-04-08 12:15:59,542 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 356
2020-04-08 12:15:59,542 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 361
2020-04-08 12:15:59,542 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 11
2020-04-08 12:15:59,542 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 189
2020-04-08 12:15:59,542 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 719
2020-04-08 12:15:59,542 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 46
2020-04-08 12:15:59,542 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 709
2020-04-08 12:15:59,542 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 389
2020-04-08 12:15:59,542 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 433
2020-04-08 12:15:59,542 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 301
2020-04-08 12:15:59,542 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 403
2020-04-08 12:15:59,542 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 371
2020-04-08 12:15:59,542 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 589
2020-04-08 12:15:59,542 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 60
2020-04-08 12:15:59,542 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 565
2020-04-08 12:15:59,542 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 664
2020-04-08 12:15:59,542 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 699
2020-04-08 12:15:59,542 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 523
2020-04-08 12:15:59,542 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 540
2020-04-08 12:15:59,542 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 165
2020-04-08 12:15:59,542 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 554
2020-04-08 12:15:59,542 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 564
2020-04-08 12:15:59,542 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 168
2020-04-08 12:15:59,542 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 481
2020-04-08 12:15:59,542 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 240
2020-04-08 12:15:59,542 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 117
2020-04-08 12:15:59,542 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 35
2020-04-08 12:15:59,542 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 660
2020-04-08 12:15:59,542 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 456
2020-04-08 12:15:59,542 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 239
2020-04-08 12:15:59,542 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 272
2020-04-08 12:15:59,542 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 218
2020-04-08 12:15:59,542 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 249
2020-04-08 12:15:59,542 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 159
2020-04-08 12:15:59,542 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 483
2020-04-08 12:15:59,542 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 516
2020-04-08 12:15:59,543 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 443
2020-04-08 12:15:59,543 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 453
2020-04-08 12:15:59,543 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 350
2020-04-08 12:15:59,543 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 640
2020-04-08 12:15:59,543 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 666
2020-04-08 12:15:59,543 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 721
2020-04-08 12:15:59,543 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 473
2020-04-08 12:15:59,543 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 665
2020-04-08 12:15:59,543 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_12_piece0 on 192.168.0.102:52741 in memory (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:15:59,544 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 455
2020-04-08 12:15:59,544 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 125
2020-04-08 12:15:59,544 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 140
2020-04-08 12:15:59,544 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 367
2020-04-08 12:15:59,544 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 439
2020-04-08 12:15:59,544 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 497
2020-04-08 12:15:59,544 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 622
2020-04-08 12:15:59,544 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 120
2020-04-08 12:15:59,544 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 570
2020-04-08 12:15:59,544 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 609
2020-04-08 12:15:59,544 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 232
2020-04-08 12:15:59,545 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_9_piece0 on 192.168.0.102:52741 in memory (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:15:59,546 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 462
2020-04-08 12:15:59,546 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 512
2020-04-08 12:15:59,546 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 691
2020-04-08 12:15:59,546 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 212
2020-04-08 12:15:59,546 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 327
2020-04-08 12:15:59,546 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 418
2020-04-08 12:15:59,546 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 166
2020-04-08 12:15:59,546 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 715
2020-04-08 12:15:59,546 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 476
2020-04-08 12:15:59,546 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 387
2020-04-08 12:15:59,546 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 569
2020-04-08 12:15:59,546 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 52
2020-04-08 12:15:59,546 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 30
2020-04-08 12:15:59,546 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 172
2020-04-08 12:15:59,546 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 613
2020-04-08 12:15:59,547 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 82
2020-04-08 12:15:59,547 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 659
2020-04-08 12:15:59,547 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 137
2020-04-08 12:15:59,547 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 323
2020-04-08 12:15:59,547 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 580
2020-04-08 12:15:59,547 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 58
2020-04-08 12:15:59,547 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 300
2020-04-08 12:15:59,547 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 480
2020-04-08 12:15:59,547 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 450
2020-04-08 12:15:59,547 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 252
2020-04-08 12:15:59,547 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 299
2020-04-08 12:15:59,547 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 576
2020-04-08 12:15:59,547 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 268
2020-04-08 12:15:59,547 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 452
2020-04-08 12:15:59,547 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 21
2020-04-08 12:15:59,547 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 14
2020-04-08 12:15:59,547 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 31
2020-04-08 12:15:59,547 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 196
2020-04-08 12:15:59,547 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 392
2020-04-08 12:15:59,547 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 527
2020-04-08 12:15:59,547 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 559
2020-04-08 12:15:59,547 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 74
2020-04-08 12:15:59,547 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 636
2020-04-08 12:15:59,547 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 149
2020-04-08 12:15:59,547 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 507
2020-04-08 12:15:59,547 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 317
2020-04-08 12:15:59,547 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 68
2020-04-08 12:15:59,547 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 369
2020-04-08 12:15:59,547 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 528
2020-04-08 12:15:59,547 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 158
2020-04-08 12:15:59,547 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 130
2020-04-08 12:15:59,547 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 383
2020-04-08 12:15:59,547 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 113
2020-04-08 12:15:59,547 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 109
2020-04-08 12:15:59,547 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 93
2020-04-08 12:15:59,547 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 448
2020-04-08 12:15:59,547 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 492
2020-04-08 12:15:59,547 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 426
2020-04-08 12:15:59,547 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 506
2020-04-08 12:15:59,547 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 621
2020-04-08 12:15:59,548 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 654
2020-04-08 12:15:59,548 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 653
2020-04-08 12:15:59,548 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 521
2020-04-08 12:15:59,548 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 485
2020-04-08 12:15:59,548 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 713
2020-04-08 12:15:59,548 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 432
2020-04-08 12:15:59,548 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 518
2020-04-08 12:15:59,548 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 376
2020-04-08 12:15:59,548 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 431
2020-04-08 12:15:59,548 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 412
2020-04-08 12:15:59,548 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 482
2020-04-08 12:15:59,548 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 496
2020-04-08 12:15:59,548 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 178
2020-04-08 12:15:59,548 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 566
2020-04-08 12:15:59,548 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 650
2020-04-08 12:15:59,548 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.102:52741 in memory (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:15:59,549 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 226
2020-04-08 12:15:59,549 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 329
2020-04-08 12:15:59,549 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 470
2020-04-08 12:15:59,549 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 488
2020-04-08 12:15:59,549 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 399
2020-04-08 12:15:59,549 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 524
2020-04-08 12:15:59,549 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 274
2020-04-08 12:15:59,550 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 368
2020-04-08 12:15:59,550 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 359
2020-04-08 12:15:59,550 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 83
2020-04-08 12:15:59,550 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 182
2020-04-08 12:15:59,550 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 73
2020-04-08 12:15:59,550 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 66
2020-04-08 12:15:59,550 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 19
2020-04-08 12:15:59,550 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 707
2020-04-08 12:15:59,550 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 160
2020-04-08 12:15:59,550 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 681
2020-04-08 12:15:59,550 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 509
2020-04-08 12:15:59,550 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 647
2020-04-08 12:15:59,550 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 365
2020-04-08 12:15:59,550 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 10
2020-04-08 12:15:59,550 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 604
2020-04-08 12:15:59,550 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 136
2020-04-08 12:15:59,550 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 284
2020-04-08 12:15:59,550 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 257
2020-04-08 12:15:59,550 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 106
2020-04-08 12:15:59,550 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 313
2020-04-08 12:15:59,550 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 51
2020-04-08 12:15:59,550 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 314
2020-04-08 12:15:59,550 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 424
2020-04-08 12:15:59,550 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 311
2020-04-08 12:15:59,550 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 267
2020-04-08 12:15:59,550 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 687
2020-04-08 12:15:59,550 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 285
2020-04-08 12:15:59,550 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 187
2020-04-08 12:15:59,550 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 346
2020-04-08 12:15:59,550 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 123
2020-04-08 12:15:59,550 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 147
2020-04-08 12:15:59,550 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 525
2020-04-08 12:15:59,550 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 96
2020-04-08 12:15:59,550 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 50
2020-04-08 12:15:59,550 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 533
2020-04-08 12:15:59,550 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 176
2020-04-08 12:15:59,551 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_16_piece0 on 192.168.0.102:52741 in memory (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:15:59,552 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 710
2020-04-08 12:15:59,552 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 558
2020-04-08 12:15:59,552 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 605
2020-04-08 12:15:59,552 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 539
2020-04-08 12:15:59,552 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 595
2020-04-08 12:15:59,552 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 222
2020-04-08 12:15:59,552 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 611
2020-04-08 12:15:59,552 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 468
2020-04-08 12:15:59,552 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 265
2020-04-08 12:15:59,552 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 446
2020-04-08 12:15:59,552 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 438
2020-04-08 12:15:59,552 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 270
2020-04-08 12:15:59,552 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 298
2020-04-08 12:15:59,552 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 72
2020-04-08 12:15:59,552 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 225
2020-04-08 12:15:59,552 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 338
2020-04-08 12:15:59,552 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 607
2020-04-08 12:15:59,552 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 119
2020-04-08 12:15:59,553 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.102:52741 in memory (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:15:59,554 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.0.102:52741 in memory (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:15:59,555 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 41
2020-04-08 12:15:59,555 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 38
2020-04-08 12:15:59,555 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 625
2020-04-08 12:15:59,555 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 711
2020-04-08 12:15:59,555 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 469
2020-04-08 12:15:59,555 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 639
2020-04-08 12:15:59,555 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 261
2020-04-08 12:15:59,555 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 148
2020-04-08 12:15:59,555 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 192
2020-04-08 12:15:59,555 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 722
2020-04-08 12:15:59,555 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 415
2020-04-08 12:15:59,555 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 308
2020-04-08 12:15:59,556 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 325
2020-04-08 12:15:59,556 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 2
2020-04-08 12:15:59,556 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 642
2020-04-08 12:15:59,556 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 62
2020-04-08 12:15:59,556 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 508
2020-04-08 12:15:59,556 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 537
2020-04-08 12:15:59,556 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 471
2020-04-08 12:15:59,556 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_18_piece0 on 192.168.0.102:52741 in memory (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:15:59,557 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 152
2020-04-08 12:15:59,557 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 233
2020-04-08 12:15:59,557 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 3
2020-04-08 12:15:59,557 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 32
2020-04-08 12:15:59,557 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 33
2020-04-08 12:15:59,557 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 99
2020-04-08 12:15:59,557 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 161
2020-04-08 12:15:59,557 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 78
2020-04-08 12:15:59,557 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 164
2020-04-08 12:15:59,557 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 614
2020-04-08 12:15:59,557 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 238
2020-04-08 12:15:59,557 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 635
2020-04-08 12:15:59,557 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 510
2020-04-08 12:15:59,557 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 12
2020-04-08 12:15:59,557 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 410
2020-04-08 12:15:59,557 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 16
2020-04-08 12:15:59,557 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 414
2020-04-08 12:15:59,557 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 680
2020-04-08 12:15:59,557 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 231
2020-04-08 12:15:59,558 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 116
2020-04-08 12:15:59,558 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 282
2020-04-08 12:15:59,558 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 464
2020-04-08 12:15:59,558 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 94
2020-04-08 12:15:59,558 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 575
2020-04-08 12:15:59,558 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 685
2020-04-08 12:15:59,558 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 100
2020-04-08 12:15:59,558 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 25
2020-04-08 12:15:59,558 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 397
2020-04-08 12:15:59,558 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 169
2020-04-08 12:15:59,558 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 441
2020-04-08 12:15:59,558 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 708
2020-04-08 12:15:59,558 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 615
2020-04-08 12:15:59,558 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 297
2020-04-08 12:15:59,558 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 534
2020-04-08 12:15:59,558 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 603
2020-04-08 12:15:59,558 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 156
2020-04-08 12:15:59,558 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 663
2020-04-08 12:15:59,558 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 228
2020-04-08 12:15:59,558 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 76
2020-04-08 12:15:59,558 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 404
2020-04-08 12:15:59,558 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 29
2020-04-08 12:15:59,558 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 139
2020-04-08 12:15:59,558 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 501
2020-04-08 12:15:59,558 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 514
2020-04-08 12:15:59,558 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 606
2020-04-08 12:15:59,558 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 37
2020-04-08 12:15:59,558 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 71
2020-04-08 12:15:59,559 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_26_piece0 on 192.168.0.102:52741 in memory (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:15:59,560 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 677
2020-04-08 12:15:59,560 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 436
2020-04-08 12:15:59,560 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 623
2020-04-08 12:15:59,560 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 725
2020-04-08 12:15:59,560 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 92
2020-04-08 12:15:59,560 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 598
2020-04-08 12:15:59,560 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 320
2020-04-08 12:15:59,560 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 355
2020-04-08 12:15:59,560 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 572
2020-04-08 12:15:59,560 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 579
2020-04-08 12:15:59,560 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 250
2020-04-08 12:15:59,560 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 5
2020-04-08 12:15:59,560 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 126
2020-04-08 12:15:59,560 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 520
2020-04-08 12:15:59,560 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 596
2020-04-08 12:15:59,560 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 207
2020-04-08 12:15:59,560 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 45
2020-04-08 12:15:59,560 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 276
2020-04-08 12:15:59,560 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 174
2020-04-08 12:15:59,561 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.0.102:52741 in memory (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:15:59,562 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 283
2020-04-08 12:15:59,562 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 561
2020-04-08 12:15:59,562 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 61
2020-04-08 12:15:59,562 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 435
2020-04-08 12:15:59,562 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 343
2020-04-08 12:15:59,562 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 474
2020-04-08 12:15:59,562 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 316
2020-04-08 12:15:59,562 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 714
2020-04-08 12:15:59,562 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 519
2020-04-08 12:15:59,562 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 67
2020-04-08 12:15:59,562 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 675
2020-04-08 12:15:59,562 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 44
2020-04-08 12:15:59,562 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 334
2020-04-08 12:15:59,562 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 648
2020-04-08 12:15:59,562 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 378
2020-04-08 12:15:59,562 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 616
2020-04-08 12:15:59,562 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 289
2020-04-08 12:15:59,563 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_15_piece0 on 192.168.0.102:52741 in memory (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:15:59,564 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 578
2020-04-08 12:15:59,564 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 517
2020-04-08 12:15:59,564 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 662
2020-04-08 12:15:59,564 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 201
2020-04-08 12:15:59,564 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 80
2020-04-08 12:15:59,564 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 194
2020-04-08 12:15:59,564 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 363
2020-04-08 12:15:59,564 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 487
2020-04-08 12:15:59,564 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 555
2020-04-08 12:15:59,564 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 104
2020-04-08 12:15:59,564 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 466
2020-04-08 12:15:59,564 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 251
2020-04-08 12:15:59,564 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 95
2020-04-08 12:15:59,564 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 122
2020-04-08 12:15:59,564 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 244
2020-04-08 12:15:59,564 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 686
2020-04-08 12:15:59,564 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 243
2020-04-08 12:15:59,564 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 190
2020-04-08 12:15:59,564 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 628
2020-04-08 12:15:59,564 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 188
2020-04-08 12:15:59,564 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 627
2020-04-08 12:15:59,564 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 333
2020-04-08 12:15:59,564 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 143
2020-04-08 12:15:59,564 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 90
2020-04-08 12:15:59,564 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 215
2020-04-08 12:15:59,564 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 491
2020-04-08 12:15:59,564 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 236
2020-04-08 12:15:59,564 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 645
2020-04-08 12:15:59,564 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 15
2020-04-08 12:15:59,564 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 318
2020-04-08 12:15:59,564 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 673
2020-04-08 12:15:59,565 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.0.102:52741 in memory (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:15:59,566 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 484
2020-04-08 12:15:59,566 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 162
2020-04-08 12:15:59,566 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 588
2020-04-08 12:15:59,566 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 6
2020-04-08 12:15:59,566 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 649
2020-04-08 12:15:59,566 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 18
2020-04-08 12:15:59,566 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 245
2020-04-08 12:15:59,566 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 40
2020-04-08 12:15:59,566 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 315
2020-04-08 12:15:59,566 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 682
2020-04-08 12:15:59,566 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 351
2020-04-08 12:15:59,566 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 720
2020-04-08 12:15:59,566 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 444
2020-04-08 12:15:59,566 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 544
2020-04-08 12:15:59,566 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 581
2020-04-08 12:15:59,566 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 307
2020-04-08 12:15:59,566 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 115
2020-04-08 12:15:59,566 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 694
2020-04-08 12:15:59,566 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 197
2020-04-08 12:15:59,566 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 87
2020-04-08 12:15:59,566 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 442
2020-04-08 12:15:59,566 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 619
2020-04-08 12:15:59,566 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 585
2020-04-08 12:15:59,566 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 495
2020-04-08 12:15:59,566 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 550
2020-04-08 12:15:59,566 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 416
2020-04-08 12:15:59,566 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 417
2020-04-08 12:15:59,566 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 155
2020-04-08 12:15:59,566 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 211
2020-04-08 12:15:59,566 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 322
2020-04-08 12:15:59,566 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 390
2020-04-08 12:15:59,566 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 48
2020-04-08 12:15:59,566 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 541
2020-04-08 12:15:59,567 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 646
2020-04-08 12:15:59,567 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 395
2020-04-08 12:15:59,567 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 91
2020-04-08 12:15:59,567 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 340
2020-04-08 12:15:59,567 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 631
2020-04-08 12:15:59,567 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 103
2020-04-08 12:15:59,567 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 671
2020-04-08 12:15:59,567 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 77
2020-04-08 12:15:59,567 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 277
2020-04-08 12:15:59,567 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 655
2020-04-08 12:15:59,567 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 697
2020-04-08 12:15:59,567 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 428
2020-04-08 12:15:59,567 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 465
2020-04-08 12:15:59,567 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 591
2020-04-08 12:15:59,567 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 175
2020-04-08 12:15:59,567 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 142
2020-04-08 12:15:59,567 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 676
2020-04-08 12:15:59,567 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 489
2020-04-08 12:15:59,567 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 513
2020-04-08 12:15:59,567 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 181
2020-04-08 12:15:59,567 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 209
2020-04-08 12:15:59,567 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 401
2020-04-08 12:15:59,567 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 683
2020-04-08 12:15:59,567 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 214
2020-04-08 12:15:59,567 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 198
2020-04-08 12:15:59,567 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 552
2020-04-08 12:15:59,567 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 393
2020-04-08 12:15:59,567 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 411
2020-04-08 12:15:59,567 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 461
2020-04-08 12:15:59,567 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 705
2020-04-08 12:15:59,567 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 425
2020-04-08 12:15:59,567 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 264
2020-04-08 12:15:59,567 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 380
2020-04-08 12:15:59,567 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 347
2020-04-08 12:15:59,567 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 42
2020-04-08 12:15:59,567 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 632
2020-04-08 12:15:59,567 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 254
2020-04-08 12:15:59,567 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 423
2020-04-08 12:15:59,567 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 419
2020-04-08 12:15:59,567 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 543
2020-04-08 12:15:59,567 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 641
2020-04-08 12:15:59,567 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 217
2020-04-08 12:15:59,567 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 434
2020-04-08 12:15:59,567 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 9
2020-04-08 12:15:59,567 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 153
2020-04-08 12:15:59,567 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 328
2020-04-08 12:15:59,568 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 203
2020-04-08 12:15:59,568 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 360
2020-04-08 12:15:59,568 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 560
2020-04-08 12:15:59,568 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 246
2020-04-08 12:15:59,568 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 49
2020-04-08 12:15:59,568 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 626
2020-04-08 12:15:59,568 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 129
2020-04-08 12:15:59,568 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_21_piece0 on 192.168.0.102:52741 in memory (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:15:59,569 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 475
2020-04-08 12:15:59,569 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 503
2020-04-08 12:15:59,569 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 586
2020-04-08 12:15:59,569 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 55
2020-04-08 12:15:59,569 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 88
2020-04-08 12:15:59,569 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 220
2020-04-08 12:15:59,569 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 237
2020-04-08 12:15:59,569 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 549
2020-04-08 12:15:59,569 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 500
2020-04-08 12:15:59,569 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 275
2020-04-08 12:15:59,569 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 342
2020-04-08 12:15:59,570 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_17_piece0 on 192.168.0.102:52741 in memory (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:15:59,571 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 345
2020-04-08 12:15:59,571 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 280
2020-04-08 12:15:59,571 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 382
2020-04-08 12:15:59,571 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 193
2020-04-08 12:15:59,571 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 43
2020-04-08 12:15:59,571 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 54
2020-04-08 12:15:59,571 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 701
2020-04-08 12:15:59,571 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 28
2020-04-08 12:15:59,571 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 288
2020-04-08 12:15:59,571 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 398
2020-04-08 12:15:59,571 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 127
2020-04-08 12:15:59,572 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_14_piece0 on 192.168.0.102:52741 in memory (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:15:59,572 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 633
2020-04-08 12:15:59,572 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 223
2020-04-08 12:15:59,572 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 610
2020-04-08 12:15:59,572 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 69
2020-04-08 12:15:59,572 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 567
2020-04-08 12:15:59,572 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 536
2020-04-08 12:15:59,572 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 407
2020-04-08 12:15:59,573 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_24_piece0 on 192.168.0.102:52741 in memory (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:15:59,573 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 141
2020-04-08 12:15:59,573 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 248
2020-04-08 12:15:59,573 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 396
2020-04-08 12:15:59,573 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 437
2020-04-08 12:15:59,573 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 601
2020-04-08 12:15:59,573 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 451
2020-04-08 12:15:59,573 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 551
2020-04-08 12:15:59,573 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 421
2020-04-08 12:15:59,573 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 629
2020-04-08 12:15:59,573 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 262
2020-04-08 12:15:59,573 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 210
2020-04-08 12:15:59,573 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 661
2020-04-08 12:15:59,573 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 643
2020-04-08 12:15:59,574 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 65
2020-04-08 12:15:59,574 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 422
2020-04-08 12:15:59,574 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 79
2020-04-08 12:15:59,574 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 377
2020-04-08 12:15:59,574 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 656
2020-04-08 12:15:59,574 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 24
2020-04-08 12:15:59,574 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_11_piece0 on 192.168.0.102:52741 in memory (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:15:59,575 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 535
2020-04-08 12:15:59,575 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 291
2020-04-08 12:15:59,575 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 522
2020-04-08 12:15:59,575 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 617
2020-04-08 12:15:59,575 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 458
2020-04-08 12:15:59,575 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 146
2020-04-08 12:15:59,575 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 600
2020-04-08 12:15:59,575 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 111
2020-04-08 12:15:59,575 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 357
2020-04-08 12:15:59,575 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 8
2020-04-08 12:15:59,575 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 353
2020-04-08 12:15:59,575 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 303
2020-04-08 12:15:59,575 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 184
2020-04-08 12:15:59,575 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 341
2020-04-08 12:15:59,575 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 574
2020-04-08 12:15:59,575 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 692
2020-04-08 12:15:59,575 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 344
2020-04-08 12:15:59,575 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 445
2020-04-08 12:15:59,575 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 511
2020-04-08 12:15:59,575 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 128
2020-04-08 12:15:59,575 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 296
2020-04-08 12:15:59,575 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 388
2020-04-08 12:15:59,575 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 85
2020-04-08 12:15:59,575 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 571
2020-04-08 12:15:59,575 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 75
2020-04-08 12:15:59,575 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 204
2020-04-08 12:15:59,576 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_25_piece0 on 192.168.0.102:52741 in memory (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:15:59,576 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 34
2020-04-08 12:15:59,576 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 47
2020-04-08 12:15:59,576 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 278
2020-04-08 12:15:59,576 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 183
2020-04-08 12:15:59,576 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 326
2020-04-08 12:15:59,576 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 64
2020-04-08 12:15:59,576 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 84
2020-04-08 12:15:59,576 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 108
2020-04-08 12:15:59,577 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_22_piece0 on 192.168.0.102:52741 in memory (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:15:59,577 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 242
2020-04-08 12:15:59,577 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 370
2020-04-08 12:15:59,577 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 124
2020-04-08 12:15:59,577 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 110
2020-04-08 12:15:59,577 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 70
2020-04-08 12:15:59,577 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 490
2020-04-08 12:15:59,577 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 306
2020-04-08 12:15:59,578 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_13_piece0 on 192.168.0.102:52741 in memory (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:15:59,578 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 568
2020-04-08 12:15:59,578 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 39
2020-04-08 12:15:59,579 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 171
2020-04-08 12:15:59,579 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 504
2020-04-08 12:15:59,579 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 472
2020-04-08 12:15:59,579 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 556
2020-04-08 12:15:59,579 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 362
2020-04-08 12:15:59,579 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 494
2020-04-08 12:15:59,579 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 17
2020-04-08 12:15:59,579 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 101
2020-04-08 12:15:59,579 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 170
2020-04-08 12:15:59,579 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 235
2020-04-08 12:15:59,579 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_8_piece0 on 192.168.0.102:52741 in memory (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:15:59,580 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 227
2020-04-08 12:15:59,580 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 144
2020-04-08 12:15:59,580 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 202
2020-04-08 12:15:59,580 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 279
2020-04-08 12:15:59,580 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 335
2020-04-08 12:15:59,580 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 693
2020-04-08 12:15:59,580 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 294
2020-04-08 12:15:59,580 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 400
2020-04-08 12:15:59,580 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 319
2020-04-08 12:15:59,580 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 582
2020-04-08 12:15:59,580 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 689
2020-04-08 12:15:59,580 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 684
2020-04-08 12:15:59,580 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 302
2020-04-08 12:15:59,580 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 293
2020-04-08 12:15:59,580 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 352
2020-04-08 12:15:59,580 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 312
2020-04-08 12:15:59,580 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 557
2020-04-08 12:15:59,580 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 286
2020-04-08 12:15:59,580 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 394
2020-04-08 12:15:59,580 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 247
2020-04-08 12:15:59,580 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 723
2020-04-08 12:15:59,580 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 379
2020-04-08 12:15:59,580 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 478
2020-04-08 12:15:59,580 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 620
2020-04-08 12:15:59,580 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 208
2020-04-08 12:15:59,580 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 669
2020-04-08 12:15:59,580 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 26
2020-04-08 12:15:59,580 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 562
2020-04-08 12:15:59,580 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 548
2020-04-08 12:15:59,580 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 295
2020-04-08 12:15:59,580 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 712
2020-04-08 12:15:59,580 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 138
2020-04-08 12:15:59,580 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 440
2020-04-08 12:15:59,581 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_27_piece0 on 192.168.0.102:52741 in memory (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:15:59,581 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 224
2020-04-08 12:15:59,581 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 358
2020-04-08 12:15:59,581 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 657
2020-04-08 12:15:59,581 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 374
2020-04-08 12:15:59,581 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 447
2020-04-08 12:15:59,581 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 583
2020-04-08 12:15:59,581 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 309
2020-04-08 12:15:59,581 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 135
2020-04-08 12:15:59,581 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 206
2020-04-08 12:15:59,581 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 310
2020-04-08 12:15:59,581 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 112
2020-04-08 12:15:59,582 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 584
2020-04-08 12:15:59,582 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 644
2020-04-08 12:15:59,582 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 216
2020-04-08 12:15:59,582 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 266
2020-04-08 12:15:59,582 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 408
2020-04-08 12:15:59,582 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 372
2020-04-08 12:15:59,582 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 679
2020-04-08 12:15:59,582 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 702
2020-04-08 12:15:59,582 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 1
2020-04-08 12:15:59,582 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 530
2020-04-08 12:15:59,582 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 638
2020-04-08 12:15:59,582 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 145
2020-04-08 12:15:59,582 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 324
2020-04-08 12:15:59,582 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 546
2020-04-08 12:15:59,582 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 321
2020-04-08 12:15:59,582 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 259
2020-04-08 12:15:59,582 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 486
2020-04-08 12:15:59,582 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 7
2020-04-08 12:15:59,582 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 132
2020-04-08 12:15:59,582 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 386
2020-04-08 12:15:59,582 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 427
2020-04-08 12:15:59,582 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 499
2020-04-08 12:15:59,582 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 634
2020-04-08 12:15:59,582 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 402
2020-04-08 12:15:59,582 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 593
2020-04-08 12:15:59,582 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 273
2020-04-08 12:15:59,582 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 290
2020-04-08 12:15:59,582 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 670
2020-04-08 12:15:59,582 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 177
2020-04-08 12:15:59,582 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 449
2020-04-08 12:15:59,582 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 221
2020-04-08 12:15:59,582 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 502
2020-04-08 12:15:59,582 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 185
2020-04-08 12:15:59,582 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 602
2020-04-08 12:15:59,582 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 263
2020-04-08 12:15:59,582 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 133
2020-04-08 12:15:59,582 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 305
2020-04-08 12:15:59,582 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 131
2020-04-08 12:15:59,582 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 706
2020-04-08 12:15:59,582 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 22
2020-04-08 12:15:59,582 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 173
2020-04-08 12:15:59,583 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_19_piece0 on 192.168.0.102:52741 in memory (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:15:59,583 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 618
2020-04-08 12:15:59,583 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 27
2020-04-08 12:15:59,583 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 332
2020-04-08 12:15:59,584 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_10_piece0 on 192.168.0.102:52741 in memory (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:15:59,584 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 409
2020-04-08 12:15:59,584 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 688
2020-04-08 12:15:59,584 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 186
2020-04-08 12:15:59,584 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 460
2020-04-08 12:15:59,584 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 118
2020-04-08 12:15:59,584 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 674
2020-04-08 12:15:59,584 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 59
2020-04-08 12:15:59,584 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 107
2020-04-08 12:15:59,584 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 219
2020-04-08 12:15:59,584 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 385
2020-04-08 12:15:59,584 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 703
2020-04-08 12:15:59,584 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 597
2020-04-08 12:15:59,584 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 529
2020-04-08 12:15:59,584 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 563
2020-04-08 12:15:59,584 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 454
2020-04-08 12:15:59,584 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 256
2020-04-08 12:15:59,584 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 86
2020-04-08 12:15:59,584 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 179
2020-04-08 12:15:59,584 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 658
2020-04-08 12:15:59,584 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 258
2020-04-08 12:15:59,584 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 180
2020-04-08 12:15:59,584 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 253
2020-04-08 12:15:59,584 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 704
2020-04-08 12:15:59,584 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 420
2020-04-08 12:15:59,584 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 56
2020-04-08 12:15:59,584 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 612
2020-04-08 12:15:59,584 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 281
2020-04-08 12:15:59,584 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 413
2020-04-08 12:15:59,584 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 430
2020-04-08 12:15:59,584 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 542
2020-04-08 12:15:59,584 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 304
2020-04-08 12:15:59,584 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 366
2020-04-08 12:15:59,584 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 57
2020-04-08 12:15:59,584 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 191
2020-04-08 12:15:59,584 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 577
2020-04-08 12:15:59,584 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 384
2020-04-08 12:15:59,584 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 229
2020-04-08 12:15:59,585 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 532
2020-04-08 12:15:59,585 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 121
2020-04-08 12:16:00,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 12:16:00,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 12:16:00,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 12:16:00,186 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 151.
2020-04-08 12:16:00,186 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 12:16:00,186 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 151.
2020-04-08 12:16:00,187 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586319360000 ms
2020-04-08 12:16:00,187 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586319360000 ms.0 from job set of time 1586319360000 ms
2020-04-08 12:16:00,190 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:141
2020-04-08 12:16:00,191 INFO org.apache.spark.scheduler.DAGScheduler: Got job 29 (collect at Kafka2SparkStreaming2Kudu.scala:141) with 3 output partitions
2020-04-08 12:16:00,191 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 29 (collect at Kafka2SparkStreaming2Kudu.scala:141)
2020-04-08 12:16:00,191 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 12:16:00,191 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 12:16:00,192 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[88] at map at Kafka2SparkStreaming2Kudu.scala:126), which has no missing parents
2020-04-08 12:16:00,193 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_29 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 12:16:00,194 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 12:16:00,194 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on 192.168.0.102:52741 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:16:00,194 INFO org.apache.spark.SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1161
2020-04-08 12:16:00,194 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 29 (MapPartitionsRDD[88] at map at Kafka2SparkStreaming2Kudu.scala:126) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 12:16:00,194 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 29.0 with 3 tasks
2020-04-08 12:16:00,195 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 29.0 (TID 87, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:16:00,195 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 29.0 (TID 87)
2020-04-08 12:16:00,196 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 1
2020-04-08 12:16:00,196 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 29.0 (TID 87). 679 bytes result sent to driver
2020-04-08 12:16:00,197 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 29.0 (TID 88, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:16:00,197 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 29.0 (TID 88)
2020-04-08 12:16:00,197 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 29.0 (TID 87) in 2 ms on localhost (executor driver) (1/3)
2020-04-08 12:16:00,198 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 2
2020-04-08 12:16:00,198 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 29.0 (TID 88). 679 bytes result sent to driver
2020-04-08 12:16:00,199 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 29.0 (TID 89, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:16:00,199 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 29.0 (TID 89)
2020-04-08 12:16:00,199 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 29.0 (TID 88) in 2 ms on localhost (executor driver) (2/3)
2020-04-08 12:16:00,200 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 12:16:00,200 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 29.0 (TID 89). 679 bytes result sent to driver
2020-04-08 12:16:00,200 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 29.0 (TID 89) in 1 ms on localhost (executor driver) (3/3)
2020-04-08 12:16:00,200 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool 
2020-04-08 12:16:00,201 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 29 (collect at Kafka2SparkStreaming2Kudu.scala:141) finished in 0.009 s
2020-04-08 12:16:00,201 INFO org.apache.spark.scheduler.DAGScheduler: Job 29 finished: collect at Kafka2SparkStreaming2Kudu.scala:141, took 0.010701 s
2020-04-08 12:16:00,205 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586319360000 ms.0 from job set of time 1586319360000 ms
2020-04-08 12:16:00,206 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.205 s for time 1586319360000 ms (execution: 0.018 s)
2020-04-08 12:16:00,206 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 84 from persistence list
2020-04-08 12:16:00,206 INFO org.apache.spark.storage.BlockManager: Removing RDD 84
2020-04-08 12:16:00,206 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 12:16:00,206 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586319340000 ms
2020-04-08 12:16:10,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 12:16:10,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 12:16:10,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 12:16:10,188 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 12:16:10,188 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 151.
2020-04-08 12:16:10,188 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 151.
2020-04-08 12:16:10,189 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586319370000 ms
2020-04-08 12:16:10,189 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586319370000 ms.0 from job set of time 1586319370000 ms
2020-04-08 12:16:10,194 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:141
2020-04-08 12:16:10,194 INFO org.apache.spark.scheduler.DAGScheduler: Got job 30 (collect at Kafka2SparkStreaming2Kudu.scala:141) with 3 output partitions
2020-04-08 12:16:10,195 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 30 (collect at Kafka2SparkStreaming2Kudu.scala:141)
2020-04-08 12:16:10,195 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 12:16:10,195 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 12:16:10,195 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 30 (MapPartitionsRDD[91] at map at Kafka2SparkStreaming2Kudu.scala:126), which has no missing parents
2020-04-08 12:16:10,197 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_30 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 12:16:10,197 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 12:16:10,198 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on 192.168.0.102:52741 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:16:10,198 INFO org.apache.spark.SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1161
2020-04-08 12:16:10,199 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 30 (MapPartitionsRDD[91] at map at Kafka2SparkStreaming2Kudu.scala:126) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 12:16:10,199 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 30.0 with 3 tasks
2020-04-08 12:16:10,200 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 30.0 (TID 90, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:16:10,200 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 30.0 (TID 90)
2020-04-08 12:16:10,201 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 1
2020-04-08 12:16:10,201 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 30.0 (TID 90). 679 bytes result sent to driver
2020-04-08 12:16:10,202 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 30.0 (TID 91, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:16:10,202 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 30.0 (TID 91)
2020-04-08 12:16:10,202 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 30.0 (TID 90) in 2 ms on localhost (executor driver) (1/3)
2020-04-08 12:16:10,203 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 2
2020-04-08 12:16:10,203 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 30.0 (TID 91). 679 bytes result sent to driver
2020-04-08 12:16:10,203 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 30.0 (TID 92, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:16:10,203 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 30.0 (TID 92)
2020-04-08 12:16:10,203 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 30.0 (TID 91) in 1 ms on localhost (executor driver) (2/3)
2020-04-08 12:16:10,204 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 12:16:10,204 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 30.0 (TID 92). 679 bytes result sent to driver
2020-04-08 12:16:10,204 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 30.0 (TID 92) in 1 ms on localhost (executor driver) (3/3)
2020-04-08 12:16:10,205 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool 
2020-04-08 12:16:10,205 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 30 (collect at Kafka2SparkStreaming2Kudu.scala:141) finished in 0.009 s
2020-04-08 12:16:10,205 INFO org.apache.spark.scheduler.DAGScheduler: Job 30 finished: collect at Kafka2SparkStreaming2Kudu.scala:141, took 0.010783 s
2020-04-08 12:16:10,208 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586319370000 ms.0 from job set of time 1586319370000 ms
2020-04-08 12:16:10,209 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.208 s for time 1586319370000 ms (execution: 0.019 s)
2020-04-08 12:16:10,209 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 87 from persistence list
2020-04-08 12:16:10,209 INFO org.apache.spark.storage.BlockManager: Removing RDD 87
2020-04-08 12:16:10,209 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 12:16:10,209 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586319350000 ms
2020-04-08 12:16:20,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 12:16:20,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 12:16:20,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 12:16:20,185 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 151.
2020-04-08 12:16:20,188 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 151.
2020-04-08 12:16:20,188 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 12:16:20,189 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586319380000 ms
2020-04-08 12:16:20,189 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586319380000 ms.0 from job set of time 1586319380000 ms
2020-04-08 12:16:20,192 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:141
2020-04-08 12:16:20,193 INFO org.apache.spark.scheduler.DAGScheduler: Got job 31 (collect at Kafka2SparkStreaming2Kudu.scala:141) with 3 output partitions
2020-04-08 12:16:20,193 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 31 (collect at Kafka2SparkStreaming2Kudu.scala:141)
2020-04-08 12:16:20,193 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 12:16:20,193 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 12:16:20,194 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[94] at map at Kafka2SparkStreaming2Kudu.scala:126), which has no missing parents
2020-04-08 12:16:20,195 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_31 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 12:16:20,196 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 12:16:20,196 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on 192.168.0.102:52741 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:16:20,197 INFO org.apache.spark.SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1161
2020-04-08 12:16:20,197 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 31 (MapPartitionsRDD[94] at map at Kafka2SparkStreaming2Kudu.scala:126) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 12:16:20,197 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 31.0 with 3 tasks
2020-04-08 12:16:20,197 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 31.0 (TID 93, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:16:20,198 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 31.0 (TID 93)
2020-04-08 12:16:20,198 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 1
2020-04-08 12:16:20,199 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 31.0 (TID 93). 679 bytes result sent to driver
2020-04-08 12:16:20,199 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 31.0 (TID 94, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:16:20,199 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 31.0 (TID 94)
2020-04-08 12:16:20,199 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 31.0 (TID 93) in 2 ms on localhost (executor driver) (1/3)
2020-04-08 12:16:20,200 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 2
2020-04-08 12:16:20,200 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 31.0 (TID 94). 679 bytes result sent to driver
2020-04-08 12:16:20,200 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 31.0 (TID 95, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:16:20,200 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 31.0 (TID 94) in 1 ms on localhost (executor driver) (2/3)
2020-04-08 12:16:20,200 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 31.0 (TID 95)
2020-04-08 12:16:20,201 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 12:16:20,201 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 31.0 (TID 95). 679 bytes result sent to driver
2020-04-08 12:16:20,202 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 31.0 (TID 95) in 2 ms on localhost (executor driver) (3/3)
2020-04-08 12:16:20,202 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool 
2020-04-08 12:16:20,202 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 31 (collect at Kafka2SparkStreaming2Kudu.scala:141) finished in 0.008 s
2020-04-08 12:16:20,202 INFO org.apache.spark.scheduler.DAGScheduler: Job 31 finished: collect at Kafka2SparkStreaming2Kudu.scala:141, took 0.009916 s
2020-04-08 12:16:20,206 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586319380000 ms.0 from job set of time 1586319380000 ms
2020-04-08 12:16:20,206 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 90 from persistence list
2020-04-08 12:16:20,206 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.206 s for time 1586319380000 ms (execution: 0.017 s)
2020-04-08 12:16:20,206 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 12:16:20,206 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586319360000 ms
2020-04-08 12:16:20,206 INFO org.apache.spark.storage.BlockManager: Removing RDD 90
2020-04-08 12:16:30,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 12:16:30,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 12:16:30,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 12:16:30,185 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 151.
2020-04-08 12:16:30,185 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 151.
2020-04-08 12:16:30,185 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 12:16:30,186 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586319390000 ms
2020-04-08 12:16:30,186 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586319390000 ms.0 from job set of time 1586319390000 ms
2020-04-08 12:16:30,189 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:141
2020-04-08 12:16:30,190 INFO org.apache.spark.scheduler.DAGScheduler: Got job 32 (collect at Kafka2SparkStreaming2Kudu.scala:141) with 3 output partitions
2020-04-08 12:16:30,190 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 32 (collect at Kafka2SparkStreaming2Kudu.scala:141)
2020-04-08 12:16:30,190 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 12:16:30,190 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 12:16:30,191 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[97] at map at Kafka2SparkStreaming2Kudu.scala:126), which has no missing parents
2020-04-08 12:16:30,192 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_32 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 12:16:30,193 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 12:16:30,193 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on 192.168.0.102:52741 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:16:30,193 INFO org.apache.spark.SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1161
2020-04-08 12:16:30,194 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 32 (MapPartitionsRDD[97] at map at Kafka2SparkStreaming2Kudu.scala:126) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 12:16:30,194 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 32.0 with 3 tasks
2020-04-08 12:16:30,194 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 32.0 (TID 96, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:16:30,194 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 32.0 (TID 96)
2020-04-08 12:16:30,195 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 1
2020-04-08 12:16:30,195 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 32.0 (TID 96). 679 bytes result sent to driver
2020-04-08 12:16:30,196 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 32.0 (TID 97, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:16:30,196 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 32.0 (TID 97)
2020-04-08 12:16:30,196 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 32.0 (TID 96) in 2 ms on localhost (executor driver) (1/3)
2020-04-08 12:16:30,197 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 2
2020-04-08 12:16:30,197 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 32.0 (TID 97). 679 bytes result sent to driver
2020-04-08 12:16:30,197 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 32.0 (TID 98, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:16:30,197 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 32.0 (TID 98)
2020-04-08 12:16:30,197 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 32.0 (TID 97) in 1 ms on localhost (executor driver) (2/3)
2020-04-08 12:16:30,198 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 12:16:30,198 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 32.0 (TID 98). 679 bytes result sent to driver
2020-04-08 12:16:30,199 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 32.0 (TID 98) in 2 ms on localhost (executor driver) (3/3)
2020-04-08 12:16:30,199 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool 
2020-04-08 12:16:30,199 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 32 (collect at Kafka2SparkStreaming2Kudu.scala:141) finished in 0.008 s
2020-04-08 12:16:30,199 INFO org.apache.spark.scheduler.DAGScheduler: Job 32 finished: collect at Kafka2SparkStreaming2Kudu.scala:141, took 0.009926 s
2020-04-08 12:16:30,203 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586319390000 ms.0 from job set of time 1586319390000 ms
2020-04-08 12:16:30,203 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.203 s for time 1586319390000 ms (execution: 0.017 s)
2020-04-08 12:16:30,203 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 93 from persistence list
2020-04-08 12:16:30,204 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 12:16:30,204 INFO org.apache.spark.storage.BlockManager: Removing RDD 93
2020-04-08 12:16:30,204 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586319370000 ms
2020-04-08 12:16:40,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 12:16:40,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 12:16:40,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 12:16:40,184 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 12:16:40,184 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 151.
2020-04-08 12:16:40,184 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 151.
2020-04-08 12:16:40,185 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586319400000 ms
2020-04-08 12:16:40,185 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586319400000 ms.0 from job set of time 1586319400000 ms
2020-04-08 12:16:40,188 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:141
2020-04-08 12:16:40,189 INFO org.apache.spark.scheduler.DAGScheduler: Got job 33 (collect at Kafka2SparkStreaming2Kudu.scala:141) with 3 output partitions
2020-04-08 12:16:40,189 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 33 (collect at Kafka2SparkStreaming2Kudu.scala:141)
2020-04-08 12:16:40,189 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 12:16:40,189 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 12:16:40,191 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 33 (MapPartitionsRDD[100] at map at Kafka2SparkStreaming2Kudu.scala:126), which has no missing parents
2020-04-08 12:16:40,192 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_33 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 12:16:40,193 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 12:16:40,194 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on 192.168.0.102:52741 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:16:40,194 INFO org.apache.spark.SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1161
2020-04-08 12:16:40,194 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 33 (MapPartitionsRDD[100] at map at Kafka2SparkStreaming2Kudu.scala:126) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 12:16:40,194 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 33.0 with 3 tasks
2020-04-08 12:16:40,195 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 33.0 (TID 99, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:16:40,195 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 33.0 (TID 99)
2020-04-08 12:16:40,196 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 1
2020-04-08 12:16:40,196 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 33.0 (TID 99). 679 bytes result sent to driver
2020-04-08 12:16:40,196 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 33.0 (TID 100, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:16:40,196 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 33.0 (TID 99) in 1 ms on localhost (executor driver) (1/3)
2020-04-08 12:16:40,196 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 33.0 (TID 100)
2020-04-08 12:16:40,197 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 2
2020-04-08 12:16:40,197 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 33.0 (TID 100). 679 bytes result sent to driver
2020-04-08 12:16:40,199 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 33.0 (TID 101, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:16:40,199 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 33.0 (TID 101)
2020-04-08 12:16:40,199 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 33.0 (TID 100) in 3 ms on localhost (executor driver) (2/3)
2020-04-08 12:16:40,199 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 12:16:40,200 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 33.0 (TID 101). 636 bytes result sent to driver
2020-04-08 12:16:40,200 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 33.0 (TID 101) in 1 ms on localhost (executor driver) (3/3)
2020-04-08 12:16:40,200 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool 
2020-04-08 12:16:40,200 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 33 (collect at Kafka2SparkStreaming2Kudu.scala:141) finished in 0.008 s
2020-04-08 12:16:40,200 INFO org.apache.spark.scheduler.DAGScheduler: Job 33 finished: collect at Kafka2SparkStreaming2Kudu.scala:141, took 0.011928 s
2020-04-08 12:16:40,203 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586319400000 ms.0 from job set of time 1586319400000 ms
2020-04-08 12:16:40,204 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.203 s for time 1586319400000 ms (execution: 0.018 s)
2020-04-08 12:16:40,204 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 96 from persistence list
2020-04-08 12:16:40,204 INFO org.apache.spark.storage.BlockManager: Removing RDD 96
2020-04-08 12:16:40,204 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 12:16:40,204 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586319380000 ms
2020-04-08 12:16:50,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 12:16:50,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 12:16:50,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 12:16:50,185 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 151.
2020-04-08 12:16:50,186 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 151.
2020-04-08 12:16:50,186 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 12:16:50,187 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586319410000 ms
2020-04-08 12:16:50,188 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586319410000 ms.0 from job set of time 1586319410000 ms
2020-04-08 12:16:50,197 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:141
2020-04-08 12:16:50,198 INFO org.apache.spark.scheduler.DAGScheduler: Got job 34 (collect at Kafka2SparkStreaming2Kudu.scala:141) with 3 output partitions
2020-04-08 12:16:50,198 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 34 (collect at Kafka2SparkStreaming2Kudu.scala:141)
2020-04-08 12:16:50,198 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 12:16:50,198 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 12:16:50,199 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 34 (MapPartitionsRDD[103] at map at Kafka2SparkStreaming2Kudu.scala:126), which has no missing parents
2020-04-08 12:16:50,202 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_34 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 12:16:50,206 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 12:16:50,207 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_34_piece0 in memory on 192.168.0.102:52741 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:16:50,207 INFO org.apache.spark.SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1161
2020-04-08 12:16:50,208 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 34 (MapPartitionsRDD[103] at map at Kafka2SparkStreaming2Kudu.scala:126) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 12:16:50,208 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 34.0 with 3 tasks
2020-04-08 12:16:50,209 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 34.0 (TID 102, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:16:50,209 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 34.0 (TID 102)
2020-04-08 12:16:50,211 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 1
2020-04-08 12:16:50,211 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 34.0 (TID 102). 679 bytes result sent to driver
2020-04-08 12:16:50,214 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 34.0 (TID 103, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:16:50,214 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 34.0 (TID 103)
2020-04-08 12:16:50,214 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 34.0 (TID 102) in 5 ms on localhost (executor driver) (1/3)
2020-04-08 12:16:50,216 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 2
2020-04-08 12:16:50,218 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 34.0 (TID 103). 722 bytes result sent to driver
2020-04-08 12:16:50,219 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 34.0 (TID 104, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:16:50,220 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 34.0 (TID 103) in 6 ms on localhost (executor driver) (2/3)
2020-04-08 12:16:50,220 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 34.0 (TID 104)
2020-04-08 12:16:50,221 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 12:16:50,222 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 34.0 (TID 104). 679 bytes result sent to driver
2020-04-08 12:16:50,222 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 34.0 (TID 104) in 3 ms on localhost (executor driver) (3/3)
2020-04-08 12:16:50,222 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool 
2020-04-08 12:16:50,223 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 34 (collect at Kafka2SparkStreaming2Kudu.scala:141) finished in 0.023 s
2020-04-08 12:16:50,223 INFO org.apache.spark.scheduler.DAGScheduler: Job 34 finished: collect at Kafka2SparkStreaming2Kudu.scala:141, took 0.025986 s
2020-04-08 12:16:50,234 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586319410000 ms.0 from job set of time 1586319410000 ms
2020-04-08 12:16:50,234 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 99 from persistence list
2020-04-08 12:16:50,234 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.234 s for time 1586319410000 ms (execution: 0.046 s)
2020-04-08 12:16:50,236 INFO org.apache.spark.storage.BlockManager: Removing RDD 99
2020-04-08 12:16:50,236 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 12:16:50,236 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586319390000 ms
2020-04-08 12:17:00,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 12:17:00,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 12:17:00,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 12:17:00,231 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 12:17:00,231 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 151.
2020-04-08 12:17:00,675 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 151.
2020-04-08 12:17:00,675 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586319420000 ms
2020-04-08 12:17:00,676 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586319420000 ms.0 from job set of time 1586319420000 ms
2020-04-08 12:17:00,679 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:141
2020-04-08 12:17:00,680 INFO org.apache.spark.scheduler.DAGScheduler: Got job 35 (collect at Kafka2SparkStreaming2Kudu.scala:141) with 3 output partitions
2020-04-08 12:17:00,680 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 35 (collect at Kafka2SparkStreaming2Kudu.scala:141)
2020-04-08 12:17:00,680 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 12:17:00,680 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 12:17:00,681 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[106] at map at Kafka2SparkStreaming2Kudu.scala:126), which has no missing parents
2020-04-08 12:17:00,683 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_35 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 12:17:00,685 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 12:17:00,686 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on 192.168.0.102:52741 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:17:00,686 INFO org.apache.spark.SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1161
2020-04-08 12:17:00,687 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 35 (MapPartitionsRDD[106] at map at Kafka2SparkStreaming2Kudu.scala:126) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 12:17:00,687 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 35.0 with 3 tasks
2020-04-08 12:17:00,687 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 35.0 (TID 105, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:17:00,687 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 35.0 (TID 105)
2020-04-08 12:17:00,688 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 1
2020-04-08 12:17:00,689 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 35.0 (TID 105). 722 bytes result sent to driver
2020-04-08 12:17:00,689 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 35.0 (TID 106, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:17:00,690 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 35.0 (TID 105) in 3 ms on localhost (executor driver) (1/3)
2020-04-08 12:17:00,690 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 35.0 (TID 106)
2020-04-08 12:17:00,690 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 2
2020-04-08 12:17:00,691 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 35.0 (TID 106). 636 bytes result sent to driver
2020-04-08 12:17:00,692 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 35.0 (TID 107, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:17:00,692 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 35.0 (TID 106) in 3 ms on localhost (executor driver) (2/3)
2020-04-08 12:17:00,692 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 35.0 (TID 107)
2020-04-08 12:17:00,694 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 12:17:00,694 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 35.0 (TID 107). 722 bytes result sent to driver
2020-04-08 12:17:00,695 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 35.0 (TID 107) in 4 ms on localhost (executor driver) (3/3)
2020-04-08 12:17:00,695 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool 
2020-04-08 12:17:00,695 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 35 (collect at Kafka2SparkStreaming2Kudu.scala:141) finished in 0.013 s
2020-04-08 12:17:00,695 INFO org.apache.spark.scheduler.DAGScheduler: Job 35 finished: collect at Kafka2SparkStreaming2Kudu.scala:141, took 0.015788 s
2020-04-08 12:17:00,700 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586319420000 ms.0 from job set of time 1586319420000 ms
2020-04-08 12:17:00,700 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.700 s for time 1586319420000 ms (execution: 0.024 s)
2020-04-08 12:17:00,700 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 102 from persistence list
2020-04-08 12:17:00,701 INFO org.apache.spark.storage.BlockManager: Removing RDD 102
2020-04-08 12:17:00,701 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 12:17:00,701 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586319400000 ms
2020-04-08 12:17:09,981 INFO org.apache.spark.streaming.StreamingContext: Invoking stop(stopGracefully=false) from shutdown hook
2020-04-08 12:17:09,983 INFO org.apache.spark.streaming.scheduler.ReceiverTracker: ReceiverTracker stopped
2020-04-08 12:17:09,984 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopping JobGenerator immediately
2020-04-08 12:17:09,985 INFO org.apache.spark.streaming.util.RecurringTimer: Stopped timer for JobGenerator after time 1586319420000
2020-04-08 12:17:10,175 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Member consumer-1-5a2837dd-0ebf-4e7d-9f19-9078d21ff85a sending LeaveGroup request to coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 12:17:10,364 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopped JobGenerator
2020-04-08 12:17:10,367 INFO org.apache.spark.streaming.scheduler.JobScheduler: Stopped JobScheduler
2020-04-08 12:17:10,370 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@33a3c44a{/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 12:17:10,371 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@288214b1{/streaming/batch,null,UNAVAILABLE,@Spark}
2020-04-08 12:17:10,372 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@639aba11{/static/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 12:17:10,372 INFO org.apache.spark.streaming.StreamingContext: StreamingContext stopped successfully
2020-04-08 12:17:10,372 INFO org.apache.spark.SparkContext: Invoking stop() from shutdown hook
2020-04-08 12:17:10,377 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@71e5f61d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 12:17:10,378 INFO org.apache.spark.ui.SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
2020-04-08 12:17:10,384 INFO org.apache.spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2020-04-08 12:17:10,398 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2020-04-08 12:17:10,399 INFO org.apache.spark.storage.BlockManager: BlockManager stopped
2020-04-08 12:17:10,399 INFO org.apache.spark.storage.BlockManagerMaster: BlockManagerMaster stopped
2020-04-08 12:17:10,401 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2020-04-08 12:17:10,408 INFO org.apache.spark.SparkContext: Successfully stopped SparkContext
2020-04-08 12:17:10,408 INFO org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2020-04-08 12:17:10,409 INFO org.apache.spark.util.ShutdownHookManager: Deleting directory /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/spark-cc8f9d48-85d6-42ca-adf8-9e44f643fa03
2020-04-08 12:17:15,045 WARN org.apache.spark.util.Utils: Your hostname, MacBook-Pro-CW.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)
2020-04-08 12:17:15,046 WARN org.apache.spark.util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
2020-04-08 12:17:15,084 INFO org.apache.spark.SparkContext: Running Spark version 2.4.0.cloudera2
2020-04-08 12:17:15,328 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-04-08 12:17:15,446 INFO org.apache.spark.SparkContext: Submitted application: Kafka2SparkStreaming2Kudu-kerberos
2020-04-08 12:17:15,486 INFO org.apache.spark.SecurityManager: Changing view acls to: godiswc
2020-04-08 12:17:15,486 INFO org.apache.spark.SecurityManager: Changing modify acls to: godiswc
2020-04-08 12:17:15,486 INFO org.apache.spark.SecurityManager: Changing view acls groups to: 
2020-04-08 12:17:15,487 INFO org.apache.spark.SecurityManager: Changing modify acls groups to: 
2020-04-08 12:17:15,487 INFO org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(godiswc); groups with view permissions: Set(); users  with modify permissions: Set(godiswc); groups with modify permissions: Set()
2020-04-08 12:17:15,652 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 12:17:15,742 INFO org.apache.spark.util.Utils: Successfully started service 'sparkDriver' on port 54139.
2020-04-08 12:17:15,760 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
2020-04-08 12:17:15,772 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
2020-04-08 12:17:15,775 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-04-08 12:17:15,775 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2020-04-08 12:17:15,808 INFO org.apache.spark.storage.DiskBlockManager: Created local directory at /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/blockmgr-744b9fed-327f-4d3d-b152-c7837a759ebe
2020-04-08 12:17:15,820 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 4.1 GB
2020-04-08 12:17:15,829 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
2020-04-08 12:17:15,881 INFO org.spark_project.jetty.util.log: Logging initialized @1628ms
2020-04-08 12:17:15,941 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2019-04-30T04:42:08+08:00, git hash: e1bc35120a6617ee3df052294e433f3a25ce7097
2020-04-08 12:17:15,957 INFO org.spark_project.jetty.server.Server: Started @1706ms
2020-04-08 12:17:15,959 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 12:17:15,974 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@71e5f61d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 12:17:15,974 INFO org.apache.spark.util.Utils: Successfully started service 'SparkUI' on port 4040.
2020-04-08 12:17:15,993 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fd62371{/jobs,null,AVAILABLE,@Spark}
2020-04-08 12:17:15,994 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1d9bec4d{/jobs/json,null,AVAILABLE,@Spark}
2020-04-08 12:17:15,995 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5c48c0c0{/jobs/job,null,AVAILABLE,@Spark}
2020-04-08 12:17:15,997 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@25f7391e{/jobs/job/json,null,AVAILABLE,@Spark}
2020-04-08 12:17:16,000 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3f23a3a0{/stages,null,AVAILABLE,@Spark}
2020-04-08 12:17:16,000 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ab14cb9{/stages/json,null,AVAILABLE,@Spark}
2020-04-08 12:17:16,001 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fb97279{/stages/stage,null,AVAILABLE,@Spark}
2020-04-08 12:17:16,002 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@31024624{/stages/stage/json,null,AVAILABLE,@Spark}
2020-04-08 12:17:16,002 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@25bcd0c7{/stages/pool,null,AVAILABLE,@Spark}
2020-04-08 12:17:16,003 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@32cb636e{/stages/pool/json,null,AVAILABLE,@Spark}
2020-04-08 12:17:16,003 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@63cd604c{/storage,null,AVAILABLE,@Spark}
2020-04-08 12:17:16,003 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@40dd3977{/storage/json,null,AVAILABLE,@Spark}
2020-04-08 12:17:16,004 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3a4e343{/storage/rdd,null,AVAILABLE,@Spark}
2020-04-08 12:17:16,004 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a1d204a{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-04-08 12:17:16,005 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62dae245{/environment,null,AVAILABLE,@Spark}
2020-04-08 12:17:16,005 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4b6579e8{/environment/json,null,AVAILABLE,@Spark}
2020-04-08 12:17:16,006 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6fff253c{/executors,null,AVAILABLE,@Spark}
2020-04-08 12:17:16,006 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c6357f9{/executors/json,null,AVAILABLE,@Spark}
2020-04-08 12:17:16,007 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@591e58fa{/executors/threadDump,null,AVAILABLE,@Spark}
2020-04-08 12:17:16,007 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3954d008{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-04-08 12:17:16,012 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f94c4db{/static,null,AVAILABLE,@Spark}
2020-04-08 12:17:16,013 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@669513d8{/,null,AVAILABLE,@Spark}
2020-04-08 12:17:16,013 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3a1d593e{/api,null,AVAILABLE,@Spark}
2020-04-08 12:17:16,014 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@285d851a{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-04-08 12:17:16,014 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@314b8f2d{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-04-08 12:17:16,015 INFO org.apache.spark.ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
2020-04-08 12:17:16,088 INFO org.apache.spark.executor.Executor: Starting executor ID driver on host localhost
2020-04-08 12:17:16,196 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 12:17:16,197 INFO org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54141.
2020-04-08 12:17:16,198 INFO org.apache.spark.network.netty.NettyBlockTransferService: Server created on 192.168.0.102:54141
2020-04-08 12:17:16,199 INFO org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-04-08 12:17:16,222 INFO org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 54141, None)
2020-04-08 12:17:16,224 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:54141 with 4.1 GB RAM, BlockManagerId(driver, 192.168.0.102, 54141, None)
2020-04-08 12:17:16,226 INFO org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 54141, None)
2020-04-08 12:17:16,227 INFO org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 54141, None)
2020-04-08 12:17:16,429 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@49b07ee3{/metrics/json,null,AVAILABLE,@Spark}
2020-04-08 12:17:16,498 WARN org.apache.spark.streaming.StreamingContext: spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
2020-04-08 12:17:16,714 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding enable.auto.commit to false for executor
2020-04-08 12:17:16,714 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding auto.offset.reset to none for executor
2020-04-08 12:17:16,715 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding executor group.id to spark-executor-testgroup1
2020-04-08 12:17:16,715 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
2020-04-08 12:17:17,278 WARN org.apache.kudu.util.NetUtil: Slow DNS lookup! Resolved IP of `cw-instances-2.vpc.cloudera.com' to cw-instances-2.vpc.cloudera.com/10.65.1.47 in 155616192ns
2020-04-08 12:17:17,501 WARN org.apache.kudu.util.NetUtil: Slow DNS lookup! Resolved IP of `cw-instances-3.vpc.cloudera.com' to cw-instances-3.vpc.cloudera.com/10.65.9.108 in 154856120ns
2020-04-08 12:17:17,659 WARN org.apache.kudu.util.NetUtil: Slow DNS lookup! Resolved IP of `cw-instances-4.vpc.cloudera.com' to cw-instances-4.vpc.cloudera.com/10.65.12.223 in 155797072ns
2020-04-08 12:17:18,843 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:126
2020-04-08 12:17:18,854 INFO org.apache.spark.scheduler.DAGScheduler: Got job 0 (collect at Kafka2SparkStreaming2Kudu.scala:126) with 1 output partitions
2020-04-08 12:17:18,854 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 0 (collect at Kafka2SparkStreaming2Kudu.scala:126)
2020-04-08 12:17:18,854 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 12:17:18,855 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 12:17:18,858 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Kafka2SparkStreaming2Kudu.scala:124), which has no missing parents
2020-04-08 12:17:18,889 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 1448.0 B, free 4.1 GB)
2020-04-08 12:17:19,340 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 975.0 B, free 4.1 GB)
2020-04-08 12:17:19,342 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:54141 (size: 975.0 B, free: 4.1 GB)
2020-04-08 12:17:19,343 INFO org.apache.spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2020-04-08 12:17:19,358 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Kafka2SparkStreaming2Kudu.scala:124) (first 15 tasks are for partitions Vector(0))
2020-04-08 12:17:19,359 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
2020-04-08 12:17:19,395 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 9516 bytes)
2020-04-08 12:17:19,402 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
2020-04-08 12:17:19,433 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 2379 bytes result sent to driver
2020-04-08 12:17:19,437 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 57 ms on localhost (executor driver) (1/1)
2020-04-08 12:17:19,439 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-04-08 12:17:19,443 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 0 (collect at Kafka2SparkStreaming2Kudu.scala:126) finished in 0.576 s
2020-04-08 12:17:19,446 INFO org.apache.spark.scheduler.DAGScheduler: Job 0 finished: collect at Kafka2SparkStreaming2Kudu.scala:126, took 0.602916 s
2020-04-08 12:17:19,468 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Slide time = 10000 ms
2020-04-08 12:17:19,469 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Storage level = Serialized 1x Replicated
2020-04-08 12:17:19,469 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Checkpoint interval = null
2020-04-08 12:17:19,469 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Remember interval = 10000 ms
2020-04-08 12:17:19,469 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@8a4e5b6
2020-04-08 12:17:19,470 INFO org.apache.spark.streaming.dstream.ForEachDStream: Slide time = 10000 ms
2020-04-08 12:17:19,470 INFO org.apache.spark.streaming.dstream.ForEachDStream: Storage level = Serialized 1x Replicated
2020-04-08 12:17:19,470 INFO org.apache.spark.streaming.dstream.ForEachDStream: Checkpoint interval = null
2020-04-08 12:17:19,470 INFO org.apache.spark.streaming.dstream.ForEachDStream: Remember interval = 10000 ms
2020-04-08 12:17:19,470 INFO org.apache.spark.streaming.dstream.ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@4d09ed9f
2020-04-08 12:17:19,511 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = testgroup1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-08 12:17:19,583 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-08 12:17:19,583 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-08 12:17:19,583 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586319439580
2020-04-08 12:17:19,587 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-1, groupId=testgroup1] Subscribed to topic(s): PABUSI2
2020-04-08 12:17:20,351 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-1, groupId=testgroup1] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-08 12:17:20,352 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Discovered group coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 12:17:20,356 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Revoking previously assigned partitions []
2020-04-08 12:17:20,356 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 12:17:20,882 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 12:17:24,308 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Successfully joined group with generation 27
2020-04-08 12:17:24,314 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting newly assigned partitions: PABUSI2-2, PABUSI2-0, PABUSI2-1
2020-04-08 12:17:24,502 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-2 to the committed offset FetchPosition{offset=151, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-2.vpc.cloudera.com:9092 (id: 52 rack: null), epoch=0}}
2020-04-08 12:17:24,504 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-0 to the committed offset FetchPosition{offset=153, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-4.vpc.cloudera.com:9092 (id: 53 rack: null), epoch=2}}
2020-04-08 12:17:24,504 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-1 to the committed offset FetchPosition{offset=151, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-3.vpc.cloudera.com:9092 (id: 54 rack: null), epoch=2}}
2020-04-08 12:17:24,513 INFO org.apache.spark.streaming.util.RecurringTimer: Started timer for JobGenerator at time 1586319440000
2020-04-08 12:17:24,514 INFO org.apache.spark.streaming.scheduler.JobGenerator: Started JobGenerator at 1586319440000 ms
2020-04-08 12:17:24,515 INFO org.apache.spark.streaming.scheduler.JobScheduler: Started JobScheduler
2020-04-08 12:17:24,519 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@19ee1ae6{/streaming,null,AVAILABLE,@Spark}
2020-04-08 12:17:24,519 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5f117b3d{/streaming/json,null,AVAILABLE,@Spark}
2020-04-08 12:17:24,521 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4cfa8227{/streaming/batch,null,AVAILABLE,@Spark}
2020-04-08 12:17:24,521 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@78226c36{/streaming/batch/json,null,AVAILABLE,@Spark}
2020-04-08 12:17:24,523 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@987455b{/static/streaming,null,AVAILABLE,@Spark}
2020-04-08 12:17:24,524 INFO org.apache.spark.streaming.StreamingContext: StreamingContext started
2020-04-08 12:17:24,527 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 12:17:24,528 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 12:17:24,528 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 12:17:25,565 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 151.
2020-04-08 12:17:25,566 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 151.
2020-04-08 12:17:25,566 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 12:17:25,586 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586319440000 ms
2020-04-08 12:17:25,588 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586319440000 ms.0 from job set of time 1586319440000 ms
2020-04-08 12:17:25,606 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:145
2020-04-08 12:17:25,608 INFO org.apache.spark.scheduler.DAGScheduler: Got job 1 (collect at Kafka2SparkStreaming2Kudu.scala:145) with 3 output partitions
2020-04-08 12:17:25,608 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 1 (collect at Kafka2SparkStreaming2Kudu.scala:145)
2020-04-08 12:17:25,608 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 12:17:25,609 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 12:17:25,609 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[2] at map at Kafka2SparkStreaming2Kudu.scala:130), which has no missing parents
2020-04-08 12:17:25,615 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 12:17:25,619 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 12:17:25,620 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.102:54141 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:17:25,620 INFO org.apache.spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2020-04-08 12:17:25,621 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 1 (MapPartitionsRDD[2] at map at Kafka2SparkStreaming2Kudu.scala:130) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 12:17:25,621 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 1.0 with 3 tasks
2020-04-08 12:17:25,623 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:17:25,623 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 1.0 (TID 1)
2020-04-08 12:17:25,628 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 1
2020-04-08 12:17:25,629 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 1.0 (TID 1). 722 bytes result sent to driver
2020-04-08 12:17:25,630 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:17:25,630 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 1.0 (TID 2)
2020-04-08 12:17:25,630 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 8 ms on localhost (executor driver) (1/3)
2020-04-08 12:17:25,634 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 2
2020-04-08 12:17:25,636 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 1.0 (TID 2). 722 bytes result sent to driver
2020-04-08 12:17:25,638 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:17:25,639 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 1.0 (TID 3)
2020-04-08 12:17:25,639 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 9 ms on localhost (executor driver) (2/3)
2020-04-08 12:17:25,644 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 12:17:25,645 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 1.0 (TID 3). 722 bytes result sent to driver
2020-04-08 12:17:25,648 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 10 ms on localhost (executor driver) (3/3)
2020-04-08 12:17:25,648 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
2020-04-08 12:17:25,649 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 1 (collect at Kafka2SparkStreaming2Kudu.scala:145) finished in 0.034 s
2020-04-08 12:17:25,649 INFO org.apache.spark.scheduler.DAGScheduler: Job 1 finished: collect at Kafka2SparkStreaming2Kudu.scala:145, took 0.042494 s
2020-04-08 12:17:25,650 INFO org.apache.spark.streaming.StreamingContext: Invoking stop(stopGracefully=false) from shutdown hook
2020-04-08 12:17:25,675 INFO org.apache.spark.streaming.scheduler.ReceiverTracker: ReceiverTracker stopped
2020-04-08 12:17:25,678 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopping JobGenerator immediately
2020-04-08 12:17:25,679 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 10
2020-04-08 12:17:25,679 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 19
2020-04-08 12:17:25,679 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 39
2020-04-08 12:17:25,679 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 4
2020-04-08 12:17:25,679 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 18
2020-04-08 12:17:25,679 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 1
2020-04-08 12:17:25,680 INFO org.apache.spark.streaming.util.RecurringTimer: Stopped timer for JobGenerator after time 1586319440000
2020-04-08 12:17:25,680 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 11
2020-04-08 12:17:25,680 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 28
2020-04-08 12:17:25,680 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 34
2020-04-08 12:17:25,680 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 6
2020-04-08 12:17:25,680 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 30
2020-04-08 12:17:25,680 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 21
2020-04-08 12:17:25,680 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 17
2020-04-08 12:17:25,680 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 22
2020-04-08 12:17:25,680 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 44
2020-04-08 12:17:25,681 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 26
2020-04-08 12:17:25,681 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 33
2020-04-08 12:17:25,681 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 9
2020-04-08 12:17:25,681 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 50
2020-04-08 12:17:25,692 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.102:54141 in memory (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:17:25,694 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 27
2020-04-08 12:17:25,695 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 36
2020-04-08 12:17:25,695 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 13
2020-04-08 12:17:25,695 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 16
2020-04-08 12:17:25,695 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 5
2020-04-08 12:17:25,695 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 3
2020-04-08 12:17:25,695 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 23
2020-04-08 12:17:25,695 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 41
2020-04-08 12:17:25,695 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 24
2020-04-08 12:17:25,695 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 40
2020-04-08 12:17:25,695 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 8
2020-04-08 12:17:25,696 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 25
2020-04-08 12:17:25,696 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 29
2020-04-08 12:17:25,696 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 32
2020-04-08 12:17:25,696 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 20
2020-04-08 12:17:25,696 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 43
2020-04-08 12:17:25,696 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 49
2020-04-08 12:17:25,696 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 31
2020-04-08 12:17:25,696 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 37
2020-04-08 12:17:25,696 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 42
2020-04-08 12:17:25,697 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.102:54141 in memory (size: 975.0 B, free: 4.1 GB)
2020-04-08 12:17:25,699 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 38
2020-04-08 12:17:25,699 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 12
2020-04-08 12:17:25,699 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 35
2020-04-08 12:17:25,699 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 48
2020-04-08 12:17:25,699 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 45
2020-04-08 12:17:25,699 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 2
2020-04-08 12:17:25,699 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 7
2020-04-08 12:17:25,699 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 47
2020-04-08 12:17:25,699 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 14
2020-04-08 12:17:25,699 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 46
2020-04-08 12:17:25,700 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 15
2020-04-08 12:17:25,706 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586319440000 ms.0 from job set of time 1586319440000 ms
2020-04-08 12:17:25,707 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 5.706 s for time 1586319440000 ms (execution: 0.119 s)
2020-04-08 12:17:25,859 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Member consumer-1-b097d63d-1753-4583-b5ee-b4b891bef02a sending LeaveGroup request to coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 12:17:26,048 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 12:17:26,051 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 
2020-04-08 12:17:26,051 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopped JobGenerator
2020-04-08 12:17:26,054 INFO org.apache.spark.streaming.scheduler.JobScheduler: Stopped JobScheduler
2020-04-08 12:17:26,058 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@19ee1ae6{/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 12:17:26,058 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@4cfa8227{/streaming/batch,null,UNAVAILABLE,@Spark}
2020-04-08 12:17:26,059 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@987455b{/static/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 12:17:26,060 INFO org.apache.spark.streaming.StreamingContext: StreamingContext stopped successfully
2020-04-08 12:17:26,060 INFO org.apache.spark.SparkContext: Invoking stop() from shutdown hook
2020-04-08 12:17:26,067 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@71e5f61d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 12:17:26,069 INFO org.apache.spark.ui.SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
2020-04-08 12:17:26,077 INFO org.apache.spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2020-04-08 12:17:26,090 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2020-04-08 12:17:26,090 INFO org.apache.spark.storage.BlockManager: BlockManager stopped
2020-04-08 12:17:26,091 INFO org.apache.spark.storage.BlockManagerMaster: BlockManagerMaster stopped
2020-04-08 12:17:26,093 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2020-04-08 12:17:26,101 INFO org.apache.spark.SparkContext: Successfully stopped SparkContext
2020-04-08 12:17:26,102 INFO org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2020-04-08 12:17:26,102 INFO org.apache.spark.util.ShutdownHookManager: Deleting directory /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/spark-beace4ba-685a-4a88-a64c-94de0df48cde
2020-04-08 12:18:53,568 WARN org.apache.spark.util.Utils: Your hostname, MacBook-Pro-CW.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)
2020-04-08 12:18:53,569 WARN org.apache.spark.util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
2020-04-08 12:18:53,597 INFO org.apache.spark.SparkContext: Running Spark version 2.4.0.cloudera2
2020-04-08 12:18:53,784 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-04-08 12:18:53,887 INFO org.apache.spark.SparkContext: Submitted application: Kafka2SparkStreaming2Kudu-kerberos
2020-04-08 12:18:53,925 INFO org.apache.spark.SecurityManager: Changing view acls to: godiswc
2020-04-08 12:18:53,926 INFO org.apache.spark.SecurityManager: Changing modify acls to: godiswc
2020-04-08 12:18:53,926 INFO org.apache.spark.SecurityManager: Changing view acls groups to: 
2020-04-08 12:18:53,927 INFO org.apache.spark.SecurityManager: Changing modify acls groups to: 
2020-04-08 12:18:53,927 INFO org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(godiswc); groups with view permissions: Set(); users  with modify permissions: Set(godiswc); groups with modify permissions: Set()
2020-04-08 12:18:54,053 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 12:18:54,143 INFO org.apache.spark.util.Utils: Successfully started service 'sparkDriver' on port 54515.
2020-04-08 12:18:54,160 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
2020-04-08 12:18:54,172 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
2020-04-08 12:18:54,174 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-04-08 12:18:54,174 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2020-04-08 12:18:54,203 INFO org.apache.spark.storage.DiskBlockManager: Created local directory at /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/blockmgr-ceb05489-83de-4bcb-b3b6-c7a0c898474f
2020-04-08 12:18:54,217 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 4.1 GB
2020-04-08 12:18:54,227 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
2020-04-08 12:18:54,282 INFO org.spark_project.jetty.util.log: Logging initialized @1275ms
2020-04-08 12:18:54,345 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2019-04-30T04:42:08+08:00, git hash: e1bc35120a6617ee3df052294e433f3a25ce7097
2020-04-08 12:18:54,361 INFO org.spark_project.jetty.server.Server: Started @1355ms
2020-04-08 12:18:54,363 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 12:18:54,376 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@71e5f61d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 12:18:54,376 INFO org.apache.spark.util.Utils: Successfully started service 'SparkUI' on port 4040.
2020-04-08 12:18:54,396 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fd62371{/jobs,null,AVAILABLE,@Spark}
2020-04-08 12:18:54,396 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1d9bec4d{/jobs/json,null,AVAILABLE,@Spark}
2020-04-08 12:18:54,397 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5c48c0c0{/jobs/job,null,AVAILABLE,@Spark}
2020-04-08 12:18:54,399 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@25f7391e{/jobs/job/json,null,AVAILABLE,@Spark}
2020-04-08 12:18:54,402 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3f23a3a0{/stages,null,AVAILABLE,@Spark}
2020-04-08 12:18:54,402 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ab14cb9{/stages/json,null,AVAILABLE,@Spark}
2020-04-08 12:18:54,403 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fb97279{/stages/stage,null,AVAILABLE,@Spark}
2020-04-08 12:18:54,404 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@31024624{/stages/stage/json,null,AVAILABLE,@Spark}
2020-04-08 12:18:54,404 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@25bcd0c7{/stages/pool,null,AVAILABLE,@Spark}
2020-04-08 12:18:54,405 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@32cb636e{/stages/pool/json,null,AVAILABLE,@Spark}
2020-04-08 12:18:54,405 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@63cd604c{/storage,null,AVAILABLE,@Spark}
2020-04-08 12:18:54,406 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@40dd3977{/storage/json,null,AVAILABLE,@Spark}
2020-04-08 12:18:54,406 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3a4e343{/storage/rdd,null,AVAILABLE,@Spark}
2020-04-08 12:18:54,407 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a1d204a{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-04-08 12:18:54,407 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62dae245{/environment,null,AVAILABLE,@Spark}
2020-04-08 12:18:54,407 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4b6579e8{/environment/json,null,AVAILABLE,@Spark}
2020-04-08 12:18:54,408 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6fff253c{/executors,null,AVAILABLE,@Spark}
2020-04-08 12:18:54,408 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c6357f9{/executors/json,null,AVAILABLE,@Spark}
2020-04-08 12:18:54,409 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@591e58fa{/executors/threadDump,null,AVAILABLE,@Spark}
2020-04-08 12:18:54,409 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3954d008{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-04-08 12:18:54,414 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f94c4db{/static,null,AVAILABLE,@Spark}
2020-04-08 12:18:54,415 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@669513d8{/,null,AVAILABLE,@Spark}
2020-04-08 12:18:54,416 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3a1d593e{/api,null,AVAILABLE,@Spark}
2020-04-08 12:18:54,416 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@285d851a{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-04-08 12:18:54,417 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@314b8f2d{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-04-08 12:18:54,418 INFO org.apache.spark.ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
2020-04-08 12:18:54,485 INFO org.apache.spark.executor.Executor: Starting executor ID driver on host localhost
2020-04-08 12:18:54,567 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 12:18:54,569 INFO org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54519.
2020-04-08 12:18:54,570 INFO org.apache.spark.network.netty.NettyBlockTransferService: Server created on 192.168.0.102:54519
2020-04-08 12:18:54,571 INFO org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-04-08 12:18:54,604 INFO org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 54519, None)
2020-04-08 12:18:54,606 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:54519 with 4.1 GB RAM, BlockManagerId(driver, 192.168.0.102, 54519, None)
2020-04-08 12:18:54,608 INFO org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 54519, None)
2020-04-08 12:18:54,608 INFO org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 54519, None)
2020-04-08 12:18:54,787 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@49b07ee3{/metrics/json,null,AVAILABLE,@Spark}
2020-04-08 12:18:54,850 WARN org.apache.spark.streaming.StreamingContext: spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
2020-04-08 12:18:55,042 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding enable.auto.commit to false for executor
2020-04-08 12:18:55,043 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding auto.offset.reset to none for executor
2020-04-08 12:18:55,043 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding executor group.id to spark-executor-testgroup1
2020-04-08 12:18:55,044 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
2020-04-08 12:18:56,838 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:126
2020-04-08 12:18:56,853 INFO org.apache.spark.scheduler.DAGScheduler: Got job 0 (collect at Kafka2SparkStreaming2Kudu.scala:126) with 1 output partitions
2020-04-08 12:18:56,853 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 0 (collect at Kafka2SparkStreaming2Kudu.scala:126)
2020-04-08 12:18:56,854 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 12:18:56,855 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 12:18:56,863 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Kafka2SparkStreaming2Kudu.scala:124), which has no missing parents
2020-04-08 12:18:56,920 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 1448.0 B, free 4.1 GB)
2020-04-08 12:18:57,338 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 975.0 B, free 4.1 GB)
2020-04-08 12:18:57,340 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:54519 (size: 975.0 B, free: 4.1 GB)
2020-04-08 12:18:57,342 INFO org.apache.spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2020-04-08 12:18:57,356 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Kafka2SparkStreaming2Kudu.scala:124) (first 15 tasks are for partitions Vector(0))
2020-04-08 12:18:57,356 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
2020-04-08 12:18:57,403 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 9516 bytes)
2020-04-08 12:18:57,411 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
2020-04-08 12:18:57,447 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 2422 bytes result sent to driver
2020-04-08 12:18:57,453 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 66 ms on localhost (executor driver) (1/1)
2020-04-08 12:18:57,455 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-04-08 12:18:57,459 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 0 (collect at Kafka2SparkStreaming2Kudu.scala:126) finished in 0.583 s
2020-04-08 12:18:57,463 INFO org.apache.spark.scheduler.DAGScheduler: Job 0 finished: collect at Kafka2SparkStreaming2Kudu.scala:126, took 0.624898 s
2020-04-08 12:18:57,492 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Slide time = 10000 ms
2020-04-08 12:18:57,492 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Storage level = Serialized 1x Replicated
2020-04-08 12:18:57,492 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Checkpoint interval = null
2020-04-08 12:18:57,493 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Remember interval = 10000 ms
2020-04-08 12:18:57,493 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@585285e7
2020-04-08 12:18:57,493 INFO org.apache.spark.streaming.dstream.ForEachDStream: Slide time = 10000 ms
2020-04-08 12:18:57,494 INFO org.apache.spark.streaming.dstream.ForEachDStream: Storage level = Serialized 1x Replicated
2020-04-08 12:18:57,494 INFO org.apache.spark.streaming.dstream.ForEachDStream: Checkpoint interval = null
2020-04-08 12:18:57,494 INFO org.apache.spark.streaming.dstream.ForEachDStream: Remember interval = 10000 ms
2020-04-08 12:18:57,494 INFO org.apache.spark.streaming.dstream.ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@2959377
2020-04-08 12:18:57,565 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = testgroup1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-08 12:18:57,674 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-08 12:18:57,675 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-08 12:18:57,675 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586319537673
2020-04-08 12:18:57,677 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-1, groupId=testgroup1] Subscribed to topic(s): PABUSI2
2020-04-08 12:18:58,417 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-1, groupId=testgroup1] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-08 12:18:58,418 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Discovered group coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 12:18:58,421 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Revoking previously assigned partitions []
2020-04-08 12:18:58,421 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 12:18:58,942 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 12:19:02,401 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Successfully joined group with generation 29
2020-04-08 12:19:02,407 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting newly assigned partitions: PABUSI2-2, PABUSI2-0, PABUSI2-1
2020-04-08 12:19:02,593 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-2 to the committed offset FetchPosition{offset=151, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-2.vpc.cloudera.com:9092 (id: 52 rack: null), epoch=0}}
2020-04-08 12:19:02,595 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-0 to the committed offset FetchPosition{offset=153, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-4.vpc.cloudera.com:9092 (id: 53 rack: null), epoch=2}}
2020-04-08 12:19:02,595 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-1 to the committed offset FetchPosition{offset=151, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-3.vpc.cloudera.com:9092 (id: 54 rack: null), epoch=2}}
2020-04-08 12:19:02,604 INFO org.apache.spark.streaming.util.RecurringTimer: Started timer for JobGenerator at time 1586319540000
2020-04-08 12:19:02,605 INFO org.apache.spark.streaming.scheduler.JobGenerator: Started JobGenerator at 1586319540000 ms
2020-04-08 12:19:02,606 INFO org.apache.spark.streaming.scheduler.JobScheduler: Started JobScheduler
2020-04-08 12:19:02,611 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@19ee1ae6{/streaming,null,AVAILABLE,@Spark}
2020-04-08 12:19:02,612 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5f117b3d{/streaming/json,null,AVAILABLE,@Spark}
2020-04-08 12:19:02,613 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4cfa8227{/streaming/batch,null,AVAILABLE,@Spark}
2020-04-08 12:19:02,614 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@78226c36{/streaming/batch/json,null,AVAILABLE,@Spark}
2020-04-08 12:19:02,616 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@987455b{/static/streaming,null,AVAILABLE,@Spark}
2020-04-08 12:19:02,616 INFO org.apache.spark.streaming.StreamingContext: StreamingContext started
2020-04-08 12:19:02,620 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 12:19:02,621 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 12:19:02,621 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 12:19:02,777 INFO org.apache.spark.streaming.StreamingContext: Invoking stop(stopGracefully=false) from shutdown hook
2020-04-08 12:19:02,782 INFO org.apache.spark.streaming.scheduler.ReceiverTracker: ReceiverTracker stopped
2020-04-08 12:19:02,785 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopping JobGenerator immediately
2020-04-08 12:19:02,787 INFO org.apache.spark.streaming.util.RecurringTimer: Stopped timer for JobGenerator after time 1586319540000
2020-04-08 12:19:03,683 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 151.
2020-04-08 12:19:03,683 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 12:19:03,683 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 151.
2020-04-08 12:19:03,684 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 17
2020-04-08 12:19:03,684 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 13
2020-04-08 12:19:03,684 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 2
2020-04-08 12:19:03,684 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 8
2020-04-08 12:19:03,684 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 25
2020-04-08 12:19:03,684 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 16
2020-04-08 12:19:03,684 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 4
2020-04-08 12:19:03,684 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 1
2020-04-08 12:19:03,685 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 10
2020-04-08 12:19:03,685 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 21
2020-04-08 12:19:03,685 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 24
2020-04-08 12:19:03,685 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 18
2020-04-08 12:19:03,685 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 15
2020-04-08 12:19:03,685 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 22
2020-04-08 12:19:03,696 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.102:54519 in memory (size: 975.0 B, free: 4.1 GB)
2020-04-08 12:19:03,699 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 5
2020-04-08 12:19:03,699 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 20
2020-04-08 12:19:03,699 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 6
2020-04-08 12:19:03,699 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 7
2020-04-08 12:19:03,700 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 12
2020-04-08 12:19:03,700 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 23
2020-04-08 12:19:03,700 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 3
2020-04-08 12:19:03,700 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 11
2020-04-08 12:19:03,700 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 14
2020-04-08 12:19:03,700 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 19
2020-04-08 12:19:03,700 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 9
2020-04-08 12:19:03,702 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586319540000 ms
2020-04-08 12:19:03,704 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586319540000 ms.0 from job set of time 1586319540000 ms
2020-04-08 12:19:03,722 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:145
2020-04-08 12:19:03,723 INFO org.apache.spark.scheduler.DAGScheduler: Got job 1 (collect at Kafka2SparkStreaming2Kudu.scala:145) with 3 output partitions
2020-04-08 12:19:03,723 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 1 (collect at Kafka2SparkStreaming2Kudu.scala:145)
2020-04-08 12:19:03,724 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 12:19:03,724 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 12:19:03,724 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[2] at map at Kafka2SparkStreaming2Kudu.scala:130), which has no missing parents
2020-04-08 12:19:03,729 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 3.0 KB, free 4.1 GB)
2020-04-08 12:19:03,731 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2020-04-08 12:19:03,732 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.102:54519 (size: 2.0 KB, free: 4.1 GB)
2020-04-08 12:19:03,732 INFO org.apache.spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2020-04-08 12:19:03,733 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 1 (MapPartitionsRDD[2] at map at Kafka2SparkStreaming2Kudu.scala:130) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 12:19:03,733 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 1.0 with 3 tasks
2020-04-08 12:19:03,736 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:19:03,736 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 1.0 (TID 1)
2020-04-08 12:19:03,742 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 1
2020-04-08 12:19:03,743 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 1.0 (TID 1). 765 bytes result sent to driver
2020-04-08 12:19:03,744 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:19:03,745 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 1.0 (TID 2)
2020-04-08 12:19:03,745 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 10 ms on localhost (executor driver) (1/3)
2020-04-08 12:19:03,748 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 2
2020-04-08 12:19:03,749 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 1.0 (TID 2). 722 bytes result sent to driver
2020-04-08 12:19:03,750 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 12:19:03,750 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 1.0 (TID 3)
2020-04-08 12:19:03,750 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 6 ms on localhost (executor driver) (2/3)
2020-04-08 12:19:03,752 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 12:19:03,753 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 1.0 (TID 3). 722 bytes result sent to driver
2020-04-08 12:19:03,754 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 5 ms on localhost (executor driver) (3/3)
2020-04-08 12:19:03,754 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
2020-04-08 12:19:03,755 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 1 (collect at Kafka2SparkStreaming2Kudu.scala:145) finished in 0.027 s
2020-04-08 12:19:03,755 INFO org.apache.spark.scheduler.DAGScheduler: Job 1 finished: collect at Kafka2SparkStreaming2Kudu.scala:145, took 0.033267 s
2020-04-08 12:19:03,780 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586319540000 ms.0 from job set of time 1586319540000 ms
2020-04-08 12:19:03,781 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 3.780 s for time 1586319540000 ms (execution: 0.076 s)
2020-04-08 12:19:03,875 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Member consumer-1-cd63ac2d-429e-495c-ad78-dcb09213f2a8 sending LeaveGroup request to coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 12:19:04,063 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 12:19:04,066 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 
2020-04-08 12:19:04,067 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopped JobGenerator
2020-04-08 12:19:04,070 INFO org.apache.spark.streaming.scheduler.JobScheduler: Stopped JobScheduler
2020-04-08 12:19:04,074 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@19ee1ae6{/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 12:19:04,075 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@4cfa8227{/streaming/batch,null,UNAVAILABLE,@Spark}
2020-04-08 12:19:04,076 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@987455b{/static/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 12:19:04,076 INFO org.apache.spark.streaming.StreamingContext: StreamingContext stopped successfully
2020-04-08 12:19:04,077 INFO org.apache.spark.SparkContext: Invoking stop() from shutdown hook
2020-04-08 12:19:04,082 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@71e5f61d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 12:19:04,083 INFO org.apache.spark.ui.SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
2020-04-08 12:19:04,090 INFO org.apache.spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2020-04-08 12:19:04,101 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2020-04-08 12:19:04,101 INFO org.apache.spark.storage.BlockManager: BlockManager stopped
2020-04-08 12:19:04,102 INFO org.apache.spark.storage.BlockManagerMaster: BlockManagerMaster stopped
2020-04-08 12:19:04,104 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2020-04-08 12:19:04,111 INFO org.apache.spark.SparkContext: Successfully stopped SparkContext
2020-04-08 12:19:04,112 INFO org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2020-04-08 12:19:04,112 INFO org.apache.spark.util.ShutdownHookManager: Deleting directory /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/spark-cc7c84d9-e9d5-4fa1-bff2-aa76d8fe5f57
2020-04-08 13:02:15,303 WARN org.apache.spark.util.Utils: Your hostname, MacBook-Pro-CW.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)
2020-04-08 13:02:15,305 WARN org.apache.spark.util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
2020-04-08 13:02:15,339 INFO org.apache.spark.SparkContext: Running Spark version 2.4.0.cloudera2
2020-04-08 13:02:15,536 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-04-08 13:02:15,664 INFO org.apache.spark.SparkContext: Submitted application: Kafka2SparkStreaming2Kudu-kerberos
2020-04-08 13:02:15,709 INFO org.apache.spark.SecurityManager: Changing view acls to: godiswc
2020-04-08 13:02:15,709 INFO org.apache.spark.SecurityManager: Changing modify acls to: godiswc
2020-04-08 13:02:15,710 INFO org.apache.spark.SecurityManager: Changing view acls groups to: 
2020-04-08 13:02:15,710 INFO org.apache.spark.SecurityManager: Changing modify acls groups to: 
2020-04-08 13:02:15,710 INFO org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(godiswc); groups with view permissions: Set(); users  with modify permissions: Set(godiswc); groups with modify permissions: Set()
2020-04-08 13:02:15,836 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 13:02:15,905 INFO org.apache.spark.util.Utils: Successfully started service 'sparkDriver' on port 63559.
2020-04-08 13:02:15,920 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
2020-04-08 13:02:15,929 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
2020-04-08 13:02:15,931 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-04-08 13:02:15,931 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2020-04-08 13:02:15,963 INFO org.apache.spark.storage.DiskBlockManager: Created local directory at /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/blockmgr-23145e18-d797-4e3b-be02-3a6a80021adb
2020-04-08 13:02:15,977 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 4.1 GB
2020-04-08 13:02:15,986 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
2020-04-08 13:02:16,028 INFO org.spark_project.jetty.util.log: Logging initialized @1456ms
2020-04-08 13:02:16,074 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2019-04-30T04:42:08+08:00, git hash: e1bc35120a6617ee3df052294e433f3a25ce7097
2020-04-08 13:02:16,090 INFO org.spark_project.jetty.server.Server: Started @1518ms
2020-04-08 13:02:16,092 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 13:02:16,106 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@71e5f61d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 13:02:16,106 INFO org.apache.spark.util.Utils: Successfully started service 'SparkUI' on port 4040.
2020-04-08 13:02:16,131 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fd62371{/jobs,null,AVAILABLE,@Spark}
2020-04-08 13:02:16,132 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1d9bec4d{/jobs/json,null,AVAILABLE,@Spark}
2020-04-08 13:02:16,132 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5c48c0c0{/jobs/job,null,AVAILABLE,@Spark}
2020-04-08 13:02:16,136 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@25f7391e{/jobs/job/json,null,AVAILABLE,@Spark}
2020-04-08 13:02:16,141 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3f23a3a0{/stages,null,AVAILABLE,@Spark}
2020-04-08 13:02:16,141 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ab14cb9{/stages/json,null,AVAILABLE,@Spark}
2020-04-08 13:02:16,142 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fb97279{/stages/stage,null,AVAILABLE,@Spark}
2020-04-08 13:02:16,143 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@31024624{/stages/stage/json,null,AVAILABLE,@Spark}
2020-04-08 13:02:16,144 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@25bcd0c7{/stages/pool,null,AVAILABLE,@Spark}
2020-04-08 13:02:16,144 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@32cb636e{/stages/pool/json,null,AVAILABLE,@Spark}
2020-04-08 13:02:16,145 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@63cd604c{/storage,null,AVAILABLE,@Spark}
2020-04-08 13:02:16,145 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@40dd3977{/storage/json,null,AVAILABLE,@Spark}
2020-04-08 13:02:16,146 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3a4e343{/storage/rdd,null,AVAILABLE,@Spark}
2020-04-08 13:02:16,147 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a1d204a{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-04-08 13:02:16,148 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62dae245{/environment,null,AVAILABLE,@Spark}
2020-04-08 13:02:16,148 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4b6579e8{/environment/json,null,AVAILABLE,@Spark}
2020-04-08 13:02:16,149 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6fff253c{/executors,null,AVAILABLE,@Spark}
2020-04-08 13:02:16,150 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c6357f9{/executors/json,null,AVAILABLE,@Spark}
2020-04-08 13:02:16,151 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@591e58fa{/executors/threadDump,null,AVAILABLE,@Spark}
2020-04-08 13:02:16,152 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3954d008{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-04-08 13:02:16,160 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f94c4db{/static,null,AVAILABLE,@Spark}
2020-04-08 13:02:16,161 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@669513d8{/,null,AVAILABLE,@Spark}
2020-04-08 13:02:16,163 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3a1d593e{/api,null,AVAILABLE,@Spark}
2020-04-08 13:02:16,164 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@285d851a{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-04-08 13:02:16,165 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@314b8f2d{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-04-08 13:02:16,167 INFO org.apache.spark.ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
2020-04-08 13:02:16,245 INFO org.apache.spark.executor.Executor: Starting executor ID driver on host localhost
2020-04-08 13:02:16,291 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 13:02:16,292 INFO org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 63564.
2020-04-08 13:02:16,293 INFO org.apache.spark.network.netty.NettyBlockTransferService: Server created on 192.168.0.102:63564
2020-04-08 13:02:16,295 INFO org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-04-08 13:02:16,324 INFO org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 63564, None)
2020-04-08 13:02:16,329 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:63564 with 4.1 GB RAM, BlockManagerId(driver, 192.168.0.102, 63564, None)
2020-04-08 13:02:16,331 INFO org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 63564, None)
2020-04-08 13:02:16,332 INFO org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 63564, None)
2020-04-08 13:02:16,491 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@49b07ee3{/metrics/json,null,AVAILABLE,@Spark}
2020-04-08 13:02:16,583 WARN org.apache.spark.streaming.StreamingContext: spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
2020-04-08 13:02:16,759 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding enable.auto.commit to false for executor
2020-04-08 13:02:16,759 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding auto.offset.reset to none for executor
2020-04-08 13:02:16,759 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding executor group.id to spark-executor-testgroup1
2020-04-08 13:02:16,760 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
2020-04-08 13:02:17,333 WARN org.apache.kudu.util.NetUtil: Slow DNS lookup! Resolved IP of `cw-instances-2.vpc.cloudera.com' to cw-instances-2.vpc.cloudera.com/10.65.1.47 in 180396983ns
2020-04-08 13:02:17,568 WARN org.apache.kudu.util.NetUtil: Slow DNS lookup! Resolved IP of `cw-instances-3.vpc.cloudera.com' to cw-instances-3.vpc.cloudera.com/10.65.9.108 in 155621159ns
2020-04-08 13:02:17,726 WARN org.apache.kudu.util.NetUtil: Slow DNS lookup! Resolved IP of `cw-instances-4.vpc.cloudera.com' to cw-instances-4.vpc.cloudera.com/10.65.12.223 in 156772339ns
2020-04-08 13:02:19,051 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:126
2020-04-08 13:02:19,061 INFO org.apache.spark.scheduler.DAGScheduler: Got job 0 (collect at Kafka2SparkStreaming2Kudu.scala:126) with 1 output partitions
2020-04-08 13:02:19,062 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 0 (collect at Kafka2SparkStreaming2Kudu.scala:126)
2020-04-08 13:02:19,062 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 13:02:19,063 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 13:02:19,066 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Kafka2SparkStreaming2Kudu.scala:124), which has no missing parents
2020-04-08 13:02:19,106 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 1448.0 B, free 4.1 GB)
2020-04-08 13:02:19,541 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 975.0 B, free 4.1 GB)
2020-04-08 13:02:19,543 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:63564 (size: 975.0 B, free: 4.1 GB)
2020-04-08 13:02:19,545 INFO org.apache.spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2020-04-08 13:02:19,560 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Kafka2SparkStreaming2Kudu.scala:124) (first 15 tasks are for partitions Vector(0))
2020-04-08 13:02:19,560 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
2020-04-08 13:02:19,600 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 9516 bytes)
2020-04-08 13:02:19,607 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
2020-04-08 13:02:19,641 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 2379 bytes result sent to driver
2020-04-08 13:02:19,646 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 62 ms on localhost (executor driver) (1/1)
2020-04-08 13:02:19,648 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-04-08 13:02:19,652 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 0 (collect at Kafka2SparkStreaming2Kudu.scala:126) finished in 0.576 s
2020-04-08 13:02:19,656 INFO org.apache.spark.scheduler.DAGScheduler: Job 0 finished: collect at Kafka2SparkStreaming2Kudu.scala:126, took 0.604278 s
2020-04-08 13:02:19,680 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Slide time = 10000 ms
2020-04-08 13:02:19,680 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Storage level = Serialized 1x Replicated
2020-04-08 13:02:19,680 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Checkpoint interval = null
2020-04-08 13:02:19,681 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Remember interval = 10000 ms
2020-04-08 13:02:19,681 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@44f1c281
2020-04-08 13:02:19,681 INFO org.apache.spark.streaming.dstream.ForEachDStream: Slide time = 10000 ms
2020-04-08 13:02:19,681 INFO org.apache.spark.streaming.dstream.ForEachDStream: Storage level = Serialized 1x Replicated
2020-04-08 13:02:19,681 INFO org.apache.spark.streaming.dstream.ForEachDStream: Checkpoint interval = null
2020-04-08 13:02:19,681 INFO org.apache.spark.streaming.dstream.ForEachDStream: Remember interval = 10000 ms
2020-04-08 13:02:19,681 INFO org.apache.spark.streaming.dstream.ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@665cbe31
2020-04-08 13:02:19,726 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = testgroup1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-08 13:02:19,795 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-08 13:02:19,795 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-08 13:02:19,795 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586322139792
2020-04-08 13:02:19,797 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-1, groupId=testgroup1] Subscribed to topic(s): PABUSI2
2020-04-08 13:02:20,555 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-1, groupId=testgroup1] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-08 13:02:20,556 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Discovered group coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 13:02:20,559 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Revoking previously assigned partitions []
2020-04-08 13:02:20,559 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 13:02:22,092 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 13:02:25,527 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Successfully joined group with generation 31
2020-04-08 13:02:25,533 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting newly assigned partitions: PABUSI2-2, PABUSI2-0, PABUSI2-1
2020-04-08 13:02:25,719 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-2 to the committed offset FetchPosition{offset=151, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-2.vpc.cloudera.com:9092 (id: 52 rack: null), epoch=0}}
2020-04-08 13:02:25,721 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-0 to the committed offset FetchPosition{offset=153, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-4.vpc.cloudera.com:9092 (id: 53 rack: null), epoch=2}}
2020-04-08 13:02:25,721 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-1 to the committed offset FetchPosition{offset=151, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-3.vpc.cloudera.com:9092 (id: 54 rack: null), epoch=2}}
2020-04-08 13:02:25,730 INFO org.apache.spark.streaming.util.RecurringTimer: Started timer for JobGenerator at time 1586322140000
2020-04-08 13:02:25,731 INFO org.apache.spark.streaming.scheduler.JobGenerator: Started JobGenerator at 1586322140000 ms
2020-04-08 13:02:25,732 INFO org.apache.spark.streaming.scheduler.JobScheduler: Started JobScheduler
2020-04-08 13:02:25,736 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@19ee1ae6{/streaming,null,AVAILABLE,@Spark}
2020-04-08 13:02:25,736 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5f117b3d{/streaming/json,null,AVAILABLE,@Spark}
2020-04-08 13:02:25,737 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4cfa8227{/streaming/batch,null,AVAILABLE,@Spark}
2020-04-08 13:02:25,737 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@78226c36{/streaming/batch/json,null,AVAILABLE,@Spark}
2020-04-08 13:02:25,738 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@987455b{/static/streaming,null,AVAILABLE,@Spark}
2020-04-08 13:02:25,739 INFO org.apache.spark.streaming.StreamingContext: StreamingContext started
2020-04-08 13:02:25,742 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 13:02:25,743 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 13:02:25,743 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 13:02:26,448 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 152.
2020-04-08 13:02:26,780 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 13:02:26,780 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 151.
2020-04-08 13:02:26,799 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586322140000 ms
2020-04-08 13:02:26,801 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586322140000 ms.0 from job set of time 1586322140000 ms
2020-04-08 13:02:26,836 INFO org.apache.spark.SparkContext: Starting job: collect at Kafka2SparkStreaming2Kudu.scala:146
2020-04-08 13:02:26,838 INFO org.apache.spark.scheduler.DAGScheduler: Got job 1 (collect at Kafka2SparkStreaming2Kudu.scala:146) with 3 output partitions
2020-04-08 13:02:26,838 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 1 (collect at Kafka2SparkStreaming2Kudu.scala:146)
2020-04-08 13:02:26,838 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 13:02:26,838 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 13:02:26,838 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[2] at map at Kafka2SparkStreaming2Kudu.scala:130), which has no missing parents
2020-04-08 13:02:26,844 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 3.8 KB, free 4.1 GB)
2020-04-08 13:02:26,859 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.3 KB, free 4.1 GB)
2020-04-08 13:02:26,860 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.102:63564 (size: 2.3 KB, free: 4.1 GB)
2020-04-08 13:02:26,860 INFO org.apache.spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2020-04-08 13:02:26,861 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 16
2020-04-08 13:02:26,861 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 1 (MapPartitionsRDD[2] at map at Kafka2SparkStreaming2Kudu.scala:130) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 13:02:26,861 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 10
2020-04-08 13:02:26,861 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 1.0 with 3 tasks
2020-04-08 13:02:26,861 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 20
2020-04-08 13:02:26,862 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 8
2020-04-08 13:02:26,862 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 23
2020-04-08 13:02:26,862 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 1
2020-04-08 13:02:26,862 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 9
2020-04-08 13:02:26,862 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 17
2020-04-08 13:02:26,862 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 2
2020-04-08 13:02:26,863 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 13:02:26,863 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 1.0 (TID 1)
2020-04-08 13:02:26,868 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 2
2020-04-08 13:02:26,870 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 1.0 (TID 1). 722 bytes result sent to driver
2020-04-08 13:02:26,871 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 13:02:26,871 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 1.0 (TID 2)
2020-04-08 13:02:26,871 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 9 ms on localhost (executor driver) (1/3)
2020-04-08 13:02:26,872 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.102:63564 in memory (size: 975.0 B, free: 4.1 GB)
2020-04-08 13:02:26,874 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Computing topic PABUSI2, partition 1 offsets 151 -> 152
2020-04-08 13:02:26,875 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 14
2020-04-08 13:02:26,875 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 21
2020-04-08 13:02:26,875 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 13
2020-04-08 13:02:26,875 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 7
2020-04-08 13:02:26,875 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 12
2020-04-08 13:02:26,875 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 22
2020-04-08 13:02:26,875 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 3
2020-04-08 13:02:26,875 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 25
2020-04-08 13:02:26,875 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 15
2020-04-08 13:02:26,875 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 19
2020-04-08 13:02:26,876 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 6
2020-04-08 13:02:26,876 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 18
2020-04-08 13:02:26,876 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 11
2020-04-08 13:02:26,876 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 24
2020-04-08 13:02:26,876 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 5
2020-04-08 13:02:26,876 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 4
2020-04-08 13:02:26,878 INFO org.apache.spark.streaming.kafka010.KafkaDataConsumer: Initializing cache 16 64 0.75
2020-04-08 13:02:26,880 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-testgroup1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-08 13:02:26,886 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-08 13:02:26,886 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-08 13:02:26,886 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586322146885
2020-04-08 13:02:26,887 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Subscribed to partition(s): PABUSI2-1
2020-04-08 13:02:26,890 INFO org.apache.spark.streaming.kafka010.InternalKafkaConsumer: Initial fetch for spark-executor-testgroup1 PABUSI2-1 151
2020-04-08 13:02:26,890 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Seeking to offset 151 for partition PABUSI2-1
2020-04-08 13:02:27,424 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-08 13:02:28,164 ERROR org.apache.spark.executor.Executor: Exception in task 1.0 in stage 1.0 (TID 2)
java.lang.NullPointerException
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$2$$anonfun$2.apply(Kafka2SparkStreaming2Kudu.scala:139)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$2$$anonfun$2.apply(Kafka2SparkStreaming2Kudu.scala:130)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:363)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1174)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:296)
	at scala.collection.AbstractIterator.to(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:275)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1174)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 13:02:28,177 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 13:02:28,178 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 1.0 (TID 3)
2020-04-08 13:02:28,180 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 1.0 in stage 1.0 (TID 2, localhost, executor driver): java.lang.NullPointerException
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$2$$anonfun$2.apply(Kafka2SparkStreaming2Kudu.scala:139)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$2$$anonfun$2.apply(Kafka2SparkStreaming2Kudu.scala:130)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:363)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1174)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:296)
	at scala.collection.AbstractIterator.to(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:275)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1174)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2020-04-08 13:02:28,181 ERROR org.apache.spark.scheduler.TaskSetManager: Task 1 in stage 1.0 failed 1 times; aborting job
2020-04-08 13:02:28,182 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 13:02:28,183 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 1.0 (TID 3). 722 bytes result sent to driver
2020-04-08 13:02:28,184 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 7 ms on localhost (executor driver) (2/3)
2020-04-08 13:02:28,185 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
2020-04-08 13:02:28,185 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Cancelling stage 1
2020-04-08 13:02:28,185 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Killing all running tasks in stage 1: Stage cancelled
2020-04-08 13:02:28,186 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 1 (collect at Kafka2SparkStreaming2Kudu.scala:146) failed in 1.343 s due to Job aborted due to stage failure: Task 1 in stage 1.0 failed 1 times, most recent failure: Lost task 1.0 in stage 1.0 (TID 2, localhost, executor driver): java.lang.NullPointerException
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$2$$anonfun$2.apply(Kafka2SparkStreaming2Kudu.scala:139)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$2$$anonfun$2.apply(Kafka2SparkStreaming2Kudu.scala:130)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:363)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1174)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:296)
	at scala.collection.AbstractIterator.to(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:275)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1174)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
2020-04-08 13:02:28,187 INFO org.apache.spark.scheduler.DAGScheduler: Job 1 failed: collect at Kafka2SparkStreaming2Kudu.scala:146, took 1.350535 s
2020-04-08 13:02:28,189 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586322140000 ms.0 from job set of time 1586322140000 ms
2020-04-08 13:02:28,190 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1586322140000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 1.0 failed 1 times, most recent failure: Lost task 1.0 in stage 1.0 (TID 2, localhost, executor driver): java.lang.NullPointerException
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$2$$anonfun$2.apply(Kafka2SparkStreaming2Kudu.scala:139)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$2$$anonfun$2.apply(Kafka2SparkStreaming2Kudu.scala:130)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:363)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1174)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:296)
	at scala.collection.AbstractIterator.to(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:275)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1174)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:245)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:944)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$2.apply(Kafka2SparkStreaming2Kudu.scala:146)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$2.apply(Kafka2SparkStreaming2Kudu.scala:128)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:191)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.NullPointerException
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$2$$anonfun$2.apply(Kafka2SparkStreaming2Kudu.scala:139)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$2$$anonfun$2.apply(Kafka2SparkStreaming2Kudu.scala:130)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:363)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1174)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:296)
	at scala.collection.AbstractIterator.to(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1174)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:275)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1174)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	... 3 more
2020-04-08 13:02:28,201 INFO org.apache.spark.streaming.StreamingContext: Invoking stop(stopGracefully=false) from shutdown hook
2020-04-08 13:02:28,203 INFO org.apache.spark.streaming.scheduler.ReceiverTracker: ReceiverTracker stopped
2020-04-08 13:02:28,204 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopping JobGenerator immediately
2020-04-08 13:02:28,205 INFO org.apache.spark.streaming.util.RecurringTimer: Stopped timer for JobGenerator after time 1586322140000
2020-04-08 13:02:28,386 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Member consumer-1-7df3a48d-de96-4fb2-8252-d7de7efd422f sending LeaveGroup request to coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 13:02:28,567 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopped JobGenerator
2020-04-08 13:02:28,570 INFO org.apache.spark.streaming.scheduler.JobScheduler: Stopped JobScheduler
2020-04-08 13:02:28,573 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@19ee1ae6{/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 13:02:28,573 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@4cfa8227{/streaming/batch,null,UNAVAILABLE,@Spark}
2020-04-08 13:02:28,574 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@987455b{/static/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 13:02:28,574 INFO org.apache.spark.streaming.StreamingContext: StreamingContext stopped successfully
2020-04-08 13:02:28,574 INFO org.apache.spark.SparkContext: Invoking stop() from shutdown hook
2020-04-08 13:02:28,580 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@71e5f61d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 13:02:28,583 INFO org.apache.spark.ui.SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
2020-04-08 13:02:28,589 INFO org.apache.spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2020-04-08 13:02:28,605 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2020-04-08 13:02:28,605 INFO org.apache.spark.storage.BlockManager: BlockManager stopped
2020-04-08 13:02:28,607 INFO org.apache.spark.storage.BlockManagerMaster: BlockManagerMaster stopped
2020-04-08 13:02:28,608 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2020-04-08 13:02:28,619 INFO org.apache.spark.SparkContext: Successfully stopped SparkContext
2020-04-08 13:02:28,620 INFO org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2020-04-08 13:02:28,620 INFO org.apache.spark.util.ShutdownHookManager: Deleting directory /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/spark-79c17ea7-5a0c-47ca-a6a4-edc888ee3bd2
2020-04-08 13:50:26,270 WARN org.apache.spark.util.Utils: Your hostname, MacBook-Pro-CW.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)
2020-04-08 13:50:26,272 WARN org.apache.spark.util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
2020-04-08 13:50:26,458 INFO org.apache.spark.SparkContext: Running Spark version 2.4.0.cloudera2
2020-04-08 13:50:26,690 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-04-08 13:50:26,805 INFO org.apache.spark.SparkContext: Submitted application: Kafka2SparkStreaming2Kudu-kerberos
2020-04-08 13:50:26,848 INFO org.apache.spark.SecurityManager: Changing view acls to: godiswc
2020-04-08 13:50:26,848 INFO org.apache.spark.SecurityManager: Changing modify acls to: godiswc
2020-04-08 13:50:26,848 INFO org.apache.spark.SecurityManager: Changing view acls groups to: 
2020-04-08 13:50:26,849 INFO org.apache.spark.SecurityManager: Changing modify acls groups to: 
2020-04-08 13:50:26,849 INFO org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(godiswc); groups with view permissions: Set(); users  with modify permissions: Set(godiswc); groups with modify permissions: Set()
2020-04-08 13:50:26,994 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 13:50:27,086 INFO org.apache.spark.util.Utils: Successfully started service 'sparkDriver' on port 58543.
2020-04-08 13:50:27,105 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
2020-04-08 13:50:27,117 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
2020-04-08 13:50:27,119 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-04-08 13:50:27,119 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2020-04-08 13:50:27,150 INFO org.apache.spark.storage.DiskBlockManager: Created local directory at /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/blockmgr-0c725b1f-a1bf-46c9-a016-18efd3ec8cb9
2020-04-08 13:50:27,164 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 4.1 GB
2020-04-08 13:50:27,173 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
2020-04-08 13:50:27,234 INFO org.spark_project.jetty.util.log: Logging initialized @1786ms
2020-04-08 13:50:27,295 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2019-04-30T04:42:08+08:00, git hash: e1bc35120a6617ee3df052294e433f3a25ce7097
2020-04-08 13:50:27,311 INFO org.spark_project.jetty.server.Server: Started @1864ms
2020-04-08 13:50:27,313 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 13:50:27,326 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@5e8f9e2d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 13:50:27,326 INFO org.apache.spark.util.Utils: Successfully started service 'SparkUI' on port 4040.
2020-04-08 13:50:27,350 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2b62442c{/jobs,null,AVAILABLE,@Spark}
2020-04-08 13:50:27,351 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@10c8f62{/jobs/json,null,AVAILABLE,@Spark}
2020-04-08 13:50:27,351 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@674c583e{/jobs/job,null,AVAILABLE,@Spark}
2020-04-08 13:50:27,354 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ab14cb9{/jobs/job/json,null,AVAILABLE,@Spark}
2020-04-08 13:50:27,356 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fb97279{/stages,null,AVAILABLE,@Spark}
2020-04-08 13:50:27,357 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@439a8f59{/stages/json,null,AVAILABLE,@Spark}
2020-04-08 13:50:27,357 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@61861a29{/stages/stage,null,AVAILABLE,@Spark}
2020-04-08 13:50:27,358 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@32cb636e{/stages/stage/json,null,AVAILABLE,@Spark}
2020-04-08 13:50:27,359 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@63cd604c{/stages/pool,null,AVAILABLE,@Spark}
2020-04-08 13:50:27,359 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@40dd3977{/stages/pool/json,null,AVAILABLE,@Spark}
2020-04-08 13:50:27,360 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3a4e343{/storage,null,AVAILABLE,@Spark}
2020-04-08 13:50:27,360 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a1d204a{/storage/json,null,AVAILABLE,@Spark}
2020-04-08 13:50:27,361 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62dae245{/storage/rdd,null,AVAILABLE,@Spark}
2020-04-08 13:50:27,361 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4b6579e8{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-04-08 13:50:27,362 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6fff253c{/environment,null,AVAILABLE,@Spark}
2020-04-08 13:50:27,363 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c6357f9{/environment/json,null,AVAILABLE,@Spark}
2020-04-08 13:50:27,363 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@591e58fa{/executors,null,AVAILABLE,@Spark}
2020-04-08 13:50:27,364 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3954d008{/executors/json,null,AVAILABLE,@Spark}
2020-04-08 13:50:27,364 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f94c4db{/executors/threadDump,null,AVAILABLE,@Spark}
2020-04-08 13:50:27,365 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@593e824f{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-04-08 13:50:27,370 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@72ccd81a{/static,null,AVAILABLE,@Spark}
2020-04-08 13:50:27,371 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4a8a60bc{/,null,AVAILABLE,@Spark}
2020-04-08 13:50:27,372 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@361c294e{/api,null,AVAILABLE,@Spark}
2020-04-08 13:50:27,372 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@664a9613{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-04-08 13:50:27,373 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5118388b{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-04-08 13:50:27,374 INFO org.apache.spark.ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
2020-04-08 13:50:27,473 INFO org.apache.spark.executor.Executor: Starting executor ID driver on host localhost
2020-04-08 13:50:27,567 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 13:50:27,568 INFO org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 58547.
2020-04-08 13:50:27,569 INFO org.apache.spark.network.netty.NettyBlockTransferService: Server created on 192.168.0.102:58547
2020-04-08 13:50:27,570 INFO org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-04-08 13:50:27,591 INFO org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 58547, None)
2020-04-08 13:50:27,594 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:58547 with 4.1 GB RAM, BlockManagerId(driver, 192.168.0.102, 58547, None)
2020-04-08 13:50:27,595 INFO org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 58547, None)
2020-04-08 13:50:27,596 INFO org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 58547, None)
2020-04-08 13:50:27,802 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@65f00478{/metrics/json,null,AVAILABLE,@Spark}
2020-04-08 13:50:27,886 WARN org.apache.spark.streaming.StreamingContext: spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
2020-04-08 13:50:28,107 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding enable.auto.commit to false for executor
2020-04-08 13:50:28,107 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding auto.offset.reset to none for executor
2020-04-08 13:50:28,108 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding executor group.id to spark-executor-testgroup1
2020-04-08 13:50:28,108 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
2020-04-08 13:50:28,680 WARN org.apache.kudu.util.NetUtil: Slow DNS lookup! Resolved IP of `cw-instances-2.vpc.cloudera.com' to cw-instances-2.vpc.cloudera.com/10.65.1.47 in 161154641ns
2020-04-08 13:50:28,910 WARN org.apache.kudu.util.NetUtil: Slow DNS lookup! Resolved IP of `cw-instances-3.vpc.cloudera.com' to cw-instances-3.vpc.cloudera.com/10.65.9.108 in 154940675ns
2020-04-08 13:50:29,071 WARN org.apache.kudu.util.NetUtil: Slow DNS lookup! Resolved IP of `cw-instances-4.vpc.cloudera.com' to cw-instances-4.vpc.cloudera.com/10.65.12.223 in 157232717ns
2020-04-08 13:50:30,238 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Slide time = 10000 ms
2020-04-08 13:50:30,240 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Storage level = Serialized 1x Replicated
2020-04-08 13:50:30,240 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Checkpoint interval = null
2020-04-08 13:50:30,241 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Remember interval = 10000 ms
2020-04-08 13:50:30,242 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@3e99f3de
2020-04-08 13:50:30,242 INFO org.apache.spark.streaming.dstream.ForEachDStream: Slide time = 10000 ms
2020-04-08 13:50:30,242 INFO org.apache.spark.streaming.dstream.ForEachDStream: Storage level = Serialized 1x Replicated
2020-04-08 13:50:30,243 INFO org.apache.spark.streaming.dstream.ForEachDStream: Checkpoint interval = null
2020-04-08 13:50:30,243 INFO org.apache.spark.streaming.dstream.ForEachDStream: Remember interval = 10000 ms
2020-04-08 13:50:30,243 INFO org.apache.spark.streaming.dstream.ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@740573fb
2020-04-08 13:50:30,319 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = testgroup1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-08 13:50:30,407 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-08 13:50:30,407 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-08 13:50:30,407 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586325030406
2020-04-08 13:50:30,409 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-1, groupId=testgroup1] Subscribed to topic(s): PABUSI2
2020-04-08 13:50:31,187 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-1, groupId=testgroup1] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-08 13:50:31,188 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Discovered group coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 13:50:31,191 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Revoking previously assigned partitions []
2020-04-08 13:50:31,192 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 13:50:31,717 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 13:50:35,075 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Successfully joined group with generation 33
2020-04-08 13:50:35,081 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting newly assigned partitions: PABUSI2-2, PABUSI2-0, PABUSI2-1
2020-04-08 13:50:35,268 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-2 to the committed offset FetchPosition{offset=151, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-2.vpc.cloudera.com:9092 (id: 52 rack: null), epoch=0}}
2020-04-08 13:50:35,270 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-0 to the committed offset FetchPosition{offset=153, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-4.vpc.cloudera.com:9092 (id: 53 rack: null), epoch=2}}
2020-04-08 13:50:35,271 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-1 to the committed offset FetchPosition{offset=152, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-3.vpc.cloudera.com:9092 (id: 54 rack: null), epoch=2}}
2020-04-08 13:50:35,282 INFO org.apache.spark.streaming.util.RecurringTimer: Started timer for JobGenerator at time 1586325040000
2020-04-08 13:50:35,283 INFO org.apache.spark.streaming.scheduler.JobGenerator: Started JobGenerator at 1586325040000 ms
2020-04-08 13:50:35,284 INFO org.apache.spark.streaming.scheduler.JobScheduler: Started JobScheduler
2020-04-08 13:50:35,289 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c302a1d{/streaming,null,AVAILABLE,@Spark}
2020-04-08 13:50:35,290 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@33a3c44a{/streaming/json,null,AVAILABLE,@Spark}
2020-04-08 13:50:35,292 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@93f432e{/streaming/batch,null,AVAILABLE,@Spark}
2020-04-08 13:50:35,293 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@288214b1{/streaming/batch/json,null,AVAILABLE,@Spark}
2020-04-08 13:50:35,295 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5b051a5c{/static/streaming,null,AVAILABLE,@Spark}
2020-04-08 13:50:35,296 INFO org.apache.spark.streaming.StreamingContext: StreamingContext started
2020-04-08 13:50:40,040 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 13:50:40,040 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 13:50:40,040 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 13:50:40,223 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 151.
2020-04-08 13:50:40,224 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 13:50:40,224 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 153.
2020-04-08 13:50:40,264 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586325040000 ms
2020-04-08 13:50:40,270 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586325040000 ms.0 from job set of time 1586325040000 ms
2020-04-08 13:50:40,328 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:128
2020-04-08 13:50:40,343 INFO org.apache.spark.scheduler.DAGScheduler: Got job 0 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:128) with 3 output partitions
2020-04-08 13:50:40,344 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 0 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:128)
2020-04-08 13:50:40,344 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 13:50:40,345 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 13:50:40,349 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 0 (KafkaRDD[0] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:107), which has no missing parents
2020-04-08 13:50:40,409 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 5.4 KB, free 4.1 GB)
2020-04-08 13:50:40,779 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.6 KB, free 4.1 GB)
2020-04-08 13:50:40,780 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:58547 (size: 3.6 KB, free: 4.1 GB)
2020-04-08 13:50:40,782 INFO org.apache.spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2020-04-08 13:50:40,794 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 0 (KafkaRDD[0] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:107) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 13:50:40,795 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 0.0 with 3 tasks
2020-04-08 13:50:40,830 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 13:50:40,838 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
2020-04-08 13:50:40,867 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Computing topic PABUSI2, partition 1 offsets 152 -> 153
2020-04-08 13:50:40,871 INFO org.apache.spark.streaming.kafka010.KafkaDataConsumer: Initializing cache 16 64 0.75
2020-04-08 13:50:40,875 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-testgroup1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-08 13:50:40,881 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-08 13:50:40,881 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-08 13:50:40,881 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586325040880
2020-04-08 13:50:40,882 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Subscribed to partition(s): PABUSI2-1
2020-04-08 13:50:40,886 INFO org.apache.spark.streaming.kafka010.InternalKafkaConsumer: Initial fetch for spark-executor-testgroup1 PABUSI2-1 152
2020-04-08 13:50:40,886 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Seeking to offset 152 for partition PABUSI2-1
2020-04-08 13:50:41,415 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-08 13:50:42,202 ERROR org.apache.spark.executor.Executor: Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.NullPointerException
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:130)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:130)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:128)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 13:50:42,219 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 13:50:42,220 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 0.0 (TID 1)
2020-04-08 13:50:42,223 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.lang.NullPointerException
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:130)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:130)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:128)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2020-04-08 13:50:42,224 ERROR org.apache.spark.scheduler.TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job
2020-04-08 13:50:42,224 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 151 is the same as ending offset skipping PABUSI2 2
2020-04-08 13:50:42,228 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 0.0 (TID 1). 2828 bytes result sent to driver
2020-04-08 13:50:42,230 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Cancelling stage 0
2020-04-08 13:50:42,230 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Killing all running tasks in stage 0: Stage cancelled
2020-04-08 13:50:42,232 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-04-08 13:50:42,233 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Stage 0 was cancelled
2020-04-08 13:50:42,234 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 0 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:128) failed in 1.864 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.lang.NullPointerException
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:130)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:130)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:128)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
2020-04-08 13:50:42,235 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 15 ms on localhost (executor driver) (1/3)
2020-04-08 13:50:42,235 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-04-08 13:50:42,238 INFO org.apache.spark.scheduler.DAGScheduler: Job 0 failed: foreachPartition at Kafka2SparkStreaming2Kudu.scala:128, took 1.910504 s
2020-04-08 13:50:42,241 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586325040000 ms.0 from job set of time 1586325040000 ms
2020-04-08 13:50:42,242 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1586325040000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.lang.NullPointerException
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:130)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:130)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:128)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:245)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:933)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:933)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1.apply(Kafka2SparkStreaming2Kudu.scala:128)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1.apply(Kafka2SparkStreaming2Kudu.scala:125)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:191)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.NullPointerException
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:130)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:130)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:128)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	... 3 more
2020-04-08 13:50:42,249 INFO org.apache.spark.streaming.StreamingContext: Invoking stop(stopGracefully=false) from shutdown hook
2020-04-08 13:50:42,251 INFO org.apache.spark.streaming.scheduler.ReceiverTracker: ReceiverTracker stopped
2020-04-08 13:50:42,252 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopping JobGenerator immediately
2020-04-08 13:50:42,252 INFO org.apache.spark.streaming.util.RecurringTimer: Stopped timer for JobGenerator after time 1586325040000
2020-04-08 13:50:42,432 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Member consumer-1-04e3acc5-752b-46e5-9d1e-79ea2a17712f sending LeaveGroup request to coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 13:50:42,615 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopped JobGenerator
2020-04-08 13:50:42,620 INFO org.apache.spark.streaming.scheduler.JobScheduler: Stopped JobScheduler
2020-04-08 13:50:42,624 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@6c302a1d{/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 13:50:42,624 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@93f432e{/streaming/batch,null,UNAVAILABLE,@Spark}
2020-04-08 13:50:42,625 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@5b051a5c{/static/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 13:50:42,626 INFO org.apache.spark.streaming.StreamingContext: StreamingContext stopped successfully
2020-04-08 13:50:42,627 INFO org.apache.spark.SparkContext: Invoking stop() from shutdown hook
2020-04-08 13:50:42,633 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@5e8f9e2d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 13:50:42,635 INFO org.apache.spark.ui.SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
2020-04-08 13:50:42,643 INFO org.apache.spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2020-04-08 13:50:42,657 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2020-04-08 13:50:42,658 INFO org.apache.spark.storage.BlockManager: BlockManager stopped
2020-04-08 13:50:42,661 INFO org.apache.spark.storage.BlockManagerMaster: BlockManagerMaster stopped
2020-04-08 13:50:42,662 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2020-04-08 13:50:42,671 INFO org.apache.spark.SparkContext: Successfully stopped SparkContext
2020-04-08 13:50:42,671 INFO org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2020-04-08 13:50:42,672 INFO org.apache.spark.util.ShutdownHookManager: Deleting directory /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/spark-05e3732b-de71-4a19-bc43-86f4c751f8e0
2020-04-08 14:30:17,691 WARN org.apache.spark.util.Utils: Your hostname, MacBook-Pro-CW.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)
2020-04-08 14:30:17,694 WARN org.apache.spark.util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
2020-04-08 14:30:17,727 INFO org.apache.spark.SparkContext: Running Spark version 2.4.0.cloudera2
2020-04-08 14:30:17,983 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-04-08 14:30:18,109 INFO org.apache.spark.SparkContext: Submitted application: Kafka2SparkStreaming2Kudu-kerberos
2020-04-08 14:30:18,157 INFO org.apache.spark.SecurityManager: Changing view acls to: godiswc
2020-04-08 14:30:18,157 INFO org.apache.spark.SecurityManager: Changing modify acls to: godiswc
2020-04-08 14:30:18,158 INFO org.apache.spark.SecurityManager: Changing view acls groups to: 
2020-04-08 14:30:18,158 INFO org.apache.spark.SecurityManager: Changing modify acls groups to: 
2020-04-08 14:30:18,159 INFO org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(godiswc); groups with view permissions: Set(); users  with modify permissions: Set(godiswc); groups with modify permissions: Set()
2020-04-08 14:30:18,295 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 14:30:18,366 INFO org.apache.spark.util.Utils: Successfully started service 'sparkDriver' on port 50981.
2020-04-08 14:30:18,381 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
2020-04-08 14:30:18,392 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
2020-04-08 14:30:18,412 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-04-08 14:30:18,412 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2020-04-08 14:30:18,424 INFO org.apache.spark.storage.DiskBlockManager: Created local directory at /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/blockmgr-3fb6da43-e675-43eb-892c-7f121d07a922
2020-04-08 14:30:18,442 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 4.1 GB
2020-04-08 14:30:18,455 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
2020-04-08 14:30:18,515 INFO org.spark_project.jetty.util.log: Logging initialized @1578ms
2020-04-08 14:30:18,580 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2019-04-30T04:42:08+08:00, git hash: e1bc35120a6617ee3df052294e433f3a25ce7097
2020-04-08 14:30:18,600 INFO org.spark_project.jetty.server.Server: Started @1664ms
2020-04-08 14:30:18,603 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 14:30:18,620 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@533b266e{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 14:30:18,621 INFO org.apache.spark.util.Utils: Successfully started service 'SparkUI' on port 4040.
2020-04-08 14:30:18,650 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6e521c1e{/jobs,null,AVAILABLE,@Spark}
2020-04-08 14:30:18,654 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62dae245{/jobs/json,null,AVAILABLE,@Spark}
2020-04-08 14:30:18,655 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4b6579e8{/jobs/job,null,AVAILABLE,@Spark}
2020-04-08 14:30:18,661 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@591e58fa{/jobs/job/json,null,AVAILABLE,@Spark}
2020-04-08 14:30:18,662 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3954d008{/stages,null,AVAILABLE,@Spark}
2020-04-08 14:30:18,663 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f94c4db{/stages/json,null,AVAILABLE,@Spark}
2020-04-08 14:30:18,664 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@593e824f{/stages/stage,null,AVAILABLE,@Spark}
2020-04-08 14:30:18,666 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@64bc21ac{/stages/stage/json,null,AVAILABLE,@Spark}
2020-04-08 14:30:18,666 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@493dfb8e{/stages/pool,null,AVAILABLE,@Spark}
2020-04-08 14:30:18,667 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5d25e6bb{/stages/pool/json,null,AVAILABLE,@Spark}
2020-04-08 14:30:18,668 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@ce5a68e{/storage,null,AVAILABLE,@Spark}
2020-04-08 14:30:18,669 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@9d157ff{/storage/json,null,AVAILABLE,@Spark}
2020-04-08 14:30:18,669 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f162cc0{/storage/rdd,null,AVAILABLE,@Spark}
2020-04-08 14:30:18,670 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5df417a7{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-04-08 14:30:18,671 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7c041b41{/environment,null,AVAILABLE,@Spark}
2020-04-08 14:30:18,672 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7f69d591{/environment/json,null,AVAILABLE,@Spark}
2020-04-08 14:30:18,673 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@61078690{/executors,null,AVAILABLE,@Spark}
2020-04-08 14:30:18,674 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1cb3ec38{/executors/json,null,AVAILABLE,@Spark}
2020-04-08 14:30:18,675 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@403132fc{/executors/threadDump,null,AVAILABLE,@Spark}
2020-04-08 14:30:18,675 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@71c5b236{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-04-08 14:30:18,683 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2cab9998{/static,null,AVAILABLE,@Spark}
2020-04-08 14:30:18,684 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@332a7fce{/,null,AVAILABLE,@Spark}
2020-04-08 14:30:18,686 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@549621f3{/api,null,AVAILABLE,@Spark}
2020-04-08 14:30:18,687 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@37ebc9d8{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-04-08 14:30:18,688 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@293bb8a5{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-04-08 14:30:18,690 INFO org.apache.spark.ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
2020-04-08 14:30:18,859 INFO org.apache.spark.executor.Executor: Starting executor ID driver on host localhost
2020-04-08 14:30:18,926 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 14:30:18,927 INFO org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50986.
2020-04-08 14:30:18,927 INFO org.apache.spark.network.netty.NettyBlockTransferService: Server created on 192.168.0.102:50986
2020-04-08 14:30:18,928 INFO org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-04-08 14:30:18,954 INFO org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 50986, None)
2020-04-08 14:30:18,957 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:50986 with 4.1 GB RAM, BlockManagerId(driver, 192.168.0.102, 50986, None)
2020-04-08 14:30:18,959 INFO org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 50986, None)
2020-04-08 14:30:18,960 INFO org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 50986, None)
2020-04-08 14:30:19,137 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@54acff7d{/metrics/json,null,AVAILABLE,@Spark}
2020-04-08 14:30:19,225 WARN org.apache.spark.streaming.StreamingContext: spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
2020-04-08 14:30:19,458 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding enable.auto.commit to false for executor
2020-04-08 14:30:19,458 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding auto.offset.reset to none for executor
2020-04-08 14:30:19,459 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding executor group.id to spark-executor-testgroup1
2020-04-08 14:30:19,459 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
2020-04-08 14:30:19,963 WARN org.apache.kudu.util.NetUtil: Slow DNS lookup! Resolved IP of `cw-instances-2.vpc.cloudera.com' to cw-instances-2.vpc.cloudera.com/10.65.1.47 in 157786816ns
2020-04-08 14:30:20,178 WARN org.apache.kudu.util.NetUtil: Slow DNS lookup! Resolved IP of `cw-instances-3.vpc.cloudera.com' to cw-instances-3.vpc.cloudera.com/10.65.9.108 in 157359441ns
2020-04-08 14:30:20,339 WARN org.apache.kudu.util.NetUtil: Slow DNS lookup! Resolved IP of `cw-instances-4.vpc.cloudera.com' to cw-instances-4.vpc.cloudera.com/10.65.12.223 in 158098969ns
2020-04-08 14:30:21,651 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Slide time = 10000 ms
2020-04-08 14:30:21,651 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Storage level = Serialized 1x Replicated
2020-04-08 14:30:21,652 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Checkpoint interval = null
2020-04-08 14:30:21,652 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Remember interval = 10000 ms
2020-04-08 14:30:21,652 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@2e558c9d
2020-04-08 14:30:21,652 INFO org.apache.spark.streaming.dstream.ForEachDStream: Slide time = 10000 ms
2020-04-08 14:30:21,653 INFO org.apache.spark.streaming.dstream.ForEachDStream: Storage level = Serialized 1x Replicated
2020-04-08 14:30:21,653 INFO org.apache.spark.streaming.dstream.ForEachDStream: Checkpoint interval = null
2020-04-08 14:30:21,653 INFO org.apache.spark.streaming.dstream.ForEachDStream: Remember interval = 10000 ms
2020-04-08 14:30:21,653 INFO org.apache.spark.streaming.dstream.ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@6ebdf00a
2020-04-08 14:30:21,718 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = testgroup1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-08 14:30:21,800 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-08 14:30:21,800 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-08 14:30:21,800 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586327421798
2020-04-08 14:30:21,803 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-1, groupId=testgroup1] Subscribed to topic(s): PABUSI2
2020-04-08 14:30:22,526 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-1, groupId=testgroup1] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-08 14:30:22,527 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Discovered group coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 14:30:22,530 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Revoking previously assigned partitions []
2020-04-08 14:30:22,530 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 14:30:23,055 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 14:30:26,502 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Successfully joined group with generation 35
2020-04-08 14:30:26,512 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting newly assigned partitions: PABUSI2-2, PABUSI2-0, PABUSI2-1
2020-04-08 14:30:26,706 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-2 to the committed offset FetchPosition{offset=151, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-2.vpc.cloudera.com:9092 (id: 52 rack: null), epoch=0}}
2020-04-08 14:30:26,712 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-0 to the committed offset FetchPosition{offset=153, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-4.vpc.cloudera.com:9092 (id: 53 rack: null), epoch=2}}
2020-04-08 14:30:26,713 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-1 to the committed offset FetchPosition{offset=153, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-3.vpc.cloudera.com:9092 (id: 54 rack: null), epoch=2}}
2020-04-08 14:30:26,737 INFO org.apache.spark.streaming.util.RecurringTimer: Started timer for JobGenerator at time 1586327430000
2020-04-08 14:30:26,738 INFO org.apache.spark.streaming.scheduler.JobGenerator: Started JobGenerator at 1586327430000 ms
2020-04-08 14:30:26,739 INFO org.apache.spark.streaming.scheduler.JobScheduler: Started JobScheduler
2020-04-08 14:30:26,744 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@499ef98e{/streaming,null,AVAILABLE,@Spark}
2020-04-08 14:30:26,745 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@24934262{/streaming/json,null,AVAILABLE,@Spark}
2020-04-08 14:30:26,746 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@28501a4b{/streaming/batch,null,AVAILABLE,@Spark}
2020-04-08 14:30:26,747 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5b051a5c{/streaming/batch/json,null,AVAILABLE,@Spark}
2020-04-08 14:30:26,748 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2bebb74f{/static/streaming,null,AVAILABLE,@Spark}
2020-04-08 14:30:26,749 INFO org.apache.spark.streaming.StreamingContext: StreamingContext started
2020-04-08 14:30:30,046 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 14:30:30,047 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 14:30:30,047 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 14:30:30,227 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 152.
2020-04-08 14:30:30,228 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 14:30:30,228 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 153.
2020-04-08 14:30:30,251 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586327430000 ms
2020-04-08 14:30:30,254 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586327430000 ms.0 from job set of time 1586327430000 ms
2020-04-08 14:30:30,301 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:134
2020-04-08 14:30:30,321 INFO org.apache.spark.scheduler.DAGScheduler: Got job 0 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134) with 3 output partitions
2020-04-08 14:30:30,321 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 0 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134)
2020-04-08 14:30:30,321 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 14:30:30,322 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 14:30:30,327 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 0 (KafkaRDD[0] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113), which has no missing parents
2020-04-08 14:30:30,393 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 14:30:30,766 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 14:30:30,768 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:50986 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 14:30:30,770 INFO org.apache.spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2020-04-08 14:30:30,781 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 0 (KafkaRDD[0] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 14:30:30,782 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 0.0 with 3 tasks
2020-04-08 14:30:30,810 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:30:30,817 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
2020-04-08 14:30:30,835 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 1
2020-04-08 14:30:30,844 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 2828 bytes result sent to driver
2020-04-08 14:30:30,846 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:30:30,846 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 0.0 (TID 1)
2020-04-08 14:30:30,849 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 45 ms on localhost (executor driver) (1/3)
2020-04-08 14:30:30,850 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Computing topic PABUSI2, partition 2 offsets 151 -> 152
2020-04-08 14:30:30,853 INFO org.apache.spark.streaming.kafka010.KafkaDataConsumer: Initializing cache 16 64 0.75
2020-04-08 14:30:30,855 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-testgroup1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-08 14:30:30,860 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-08 14:30:30,861 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-08 14:30:30,861 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586327430860
2020-04-08 14:30:30,861 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Subscribed to partition(s): PABUSI2-2
2020-04-08 14:30:30,863 INFO org.apache.spark.streaming.kafka010.InternalKafkaConsumer: Initial fetch for spark-executor-testgroup1 PABUSI2-2 151
2020-04-08 14:30:30,864 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Seeking to offset 151 for partition PABUSI2-2
2020-04-08 14:30:31,389 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-08 14:30:33,185 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 0.0 (TID 1). 2871 bytes result sent to driver
2020-04-08 14:30:33,186 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:30:33,187 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 0.0 (TID 2)
2020-04-08 14:30:33,187 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 2341 ms on localhost (executor driver) (2/3)
2020-04-08 14:30:33,189 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 14:30:33,190 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 0.0 (TID 2). 2828 bytes result sent to driver
2020-04-08 14:30:33,193 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 7 ms on localhost (executor driver) (3/3)
2020-04-08 14:30:33,194 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-04-08 14:30:33,195 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 0 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134) finished in 2.850 s
2020-04-08 14:30:33,199 INFO org.apache.spark.scheduler.DAGScheduler: Job 0 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:134, took 2.897520 s
2020-04-08 14:30:33,201 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586327430000 ms.0 from job set of time 1586327430000 ms
2020-04-08 14:30:33,202 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 3.201 s for time 1586327430000 ms (execution: 2.948 s)
2020-04-08 14:30:33,206 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 14:30:33,209 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 
2020-04-08 14:30:40,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 14:30:40,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 14:30:40,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 14:30:40,180 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 153.
2020-04-08 14:30:40,180 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 14:30:40,182 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 152.
2020-04-08 14:30:40,183 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586327440000 ms
2020-04-08 14:30:40,183 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586327440000 ms.0 from job set of time 1586327440000 ms
2020-04-08 14:30:40,191 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:134
2020-04-08 14:30:40,192 INFO org.apache.spark.scheduler.DAGScheduler: Got job 1 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134) with 3 output partitions
2020-04-08 14:30:40,192 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 1 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134)
2020-04-08 14:30:40,192 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 14:30:40,192 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 14:30:40,193 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 1 (KafkaRDD[1] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113), which has no missing parents
2020-04-08 14:30:40,196 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 14:30:40,198 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 14:30:40,198 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.102:50986 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 14:30:40,199 INFO org.apache.spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2020-04-08 14:30:40,200 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 1 (KafkaRDD[1] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 14:30:40,200 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 1.0 with 3 tasks
2020-04-08 14:30:40,201 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:30:40,202 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 1.0 (TID 3)
2020-04-08 14:30:40,205 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 1
2020-04-08 14:30:40,207 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 1.0 (TID 3). 2828 bytes result sent to driver
2020-04-08 14:30:40,208 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 4, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:30:40,208 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 1.0 (TID 4)
2020-04-08 14:30:40,208 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 3) in 7 ms on localhost (executor driver) (1/3)
2020-04-08 14:30:40,211 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 152 is the same as ending offset skipping PABUSI2 2
2020-04-08 14:30:40,212 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 1.0 (TID 4). 2828 bytes result sent to driver
2020-04-08 14:30:40,213 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 1.0 (TID 5, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:30:40,213 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 1.0 (TID 5)
2020-04-08 14:30:40,213 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 4) in 6 ms on localhost (executor driver) (2/3)
2020-04-08 14:30:40,216 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 14:30:40,218 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 1.0 (TID 5). 2828 bytes result sent to driver
2020-04-08 14:30:40,219 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 1.0 (TID 5) in 6 ms on localhost (executor driver) (3/3)
2020-04-08 14:30:40,219 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
2020-04-08 14:30:40,220 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 1 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134) finished in 0.026 s
2020-04-08 14:30:40,221 INFO org.apache.spark.scheduler.DAGScheduler: Job 1 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:134, took 0.029484 s
2020-04-08 14:30:40,221 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586327440000 ms.0 from job set of time 1586327440000 ms
2020-04-08 14:30:40,221 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.221 s for time 1586327440000 ms (execution: 0.038 s)
2020-04-08 14:30:40,222 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 0 from persistence list
2020-04-08 14:30:40,228 INFO org.apache.spark.storage.BlockManager: Removing RDD 0
2020-04-08 14:30:40,228 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 14:30:40,229 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 
2020-04-08 14:30:50,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 14:30:50,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 14:30:50,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 14:30:50,181 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 153.
2020-04-08 14:30:50,181 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 152.
2020-04-08 14:30:50,182 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 14:30:50,183 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586327450000 ms
2020-04-08 14:30:50,183 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586327450000 ms.0 from job set of time 1586327450000 ms
2020-04-08 14:30:50,191 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:134
2020-04-08 14:30:50,192 INFO org.apache.spark.scheduler.DAGScheduler: Got job 2 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134) with 3 output partitions
2020-04-08 14:30:50,192 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 2 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134)
2020-04-08 14:30:50,192 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 14:30:50,192 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 14:30:50,193 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 2 (KafkaRDD[2] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113), which has no missing parents
2020-04-08 14:30:50,195 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 14:30:50,197 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 14:30:50,197 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.102:50986 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 14:30:50,198 INFO org.apache.spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2020-04-08 14:30:50,198 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 2 (KafkaRDD[2] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 14:30:50,198 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 2.0 with 3 tasks
2020-04-08 14:30:50,199 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:30:50,200 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 2.0 (TID 6)
2020-04-08 14:30:50,201 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 1
2020-04-08 14:30:50,202 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 2.0 (TID 6). 2785 bytes result sent to driver
2020-04-08 14:30:50,203 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 2.0 (TID 7, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:30:50,203 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 2.0 (TID 7)
2020-04-08 14:30:50,203 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 6) in 4 ms on localhost (executor driver) (1/3)
2020-04-08 14:30:50,205 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 152 is the same as ending offset skipping PABUSI2 2
2020-04-08 14:30:50,206 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 2.0 (TID 7). 2828 bytes result sent to driver
2020-04-08 14:30:50,207 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 2.0 (TID 8, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:30:50,207 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 2.0 (TID 8)
2020-04-08 14:30:50,207 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 2.0 (TID 7) in 4 ms on localhost (executor driver) (2/3)
2020-04-08 14:30:50,209 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 14:30:50,210 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 2.0 (TID 8). 2828 bytes result sent to driver
2020-04-08 14:30:50,211 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 2.0 (TID 8) in 5 ms on localhost (executor driver) (3/3)
2020-04-08 14:30:50,211 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
2020-04-08 14:30:50,212 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 2 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134) finished in 0.018 s
2020-04-08 14:30:50,212 INFO org.apache.spark.scheduler.DAGScheduler: Job 2 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:134, took 0.020886 s
2020-04-08 14:30:50,213 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586327450000 ms.0 from job set of time 1586327450000 ms
2020-04-08 14:30:50,213 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.213 s for time 1586327450000 ms (execution: 0.030 s)
2020-04-08 14:30:50,213 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 1 from persistence list
2020-04-08 14:30:50,213 INFO org.apache.spark.storage.BlockManager: Removing RDD 1
2020-04-08 14:30:50,214 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 14:30:50,214 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586327430000 ms
2020-04-08 14:31:00,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 14:31:00,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 14:31:00,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 14:31:00,179 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 14:31:00,180 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 152.
2020-04-08 14:31:00,180 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 153.
2020-04-08 14:31:00,181 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586327460000 ms
2020-04-08 14:31:00,182 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586327460000 ms.0 from job set of time 1586327460000 ms
2020-04-08 14:31:00,189 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:134
2020-04-08 14:31:00,189 INFO org.apache.spark.scheduler.DAGScheduler: Got job 3 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134) with 3 output partitions
2020-04-08 14:31:00,189 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 3 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134)
2020-04-08 14:31:00,189 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 14:31:00,189 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 14:31:00,190 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 3 (KafkaRDD[3] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113), which has no missing parents
2020-04-08 14:31:00,192 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 14:31:00,194 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 14:31:00,194 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.102:50986 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 14:31:00,195 INFO org.apache.spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2020-04-08 14:31:00,195 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 3 (KafkaRDD[3] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 14:31:00,195 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 3.0 with 3 tasks
2020-04-08 14:31:00,196 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 9, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:31:00,196 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 3.0 (TID 9)
2020-04-08 14:31:00,198 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 1
2020-04-08 14:31:00,199 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 3.0 (TID 9). 2828 bytes result sent to driver
2020-04-08 14:31:00,199 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 3.0 (TID 10, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:31:00,200 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 3.0 (TID 10)
2020-04-08 14:31:00,200 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 9) in 4 ms on localhost (executor driver) (1/3)
2020-04-08 14:31:00,201 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 152 is the same as ending offset skipping PABUSI2 2
2020-04-08 14:31:00,202 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 3.0 (TID 10). 2785 bytes result sent to driver
2020-04-08 14:31:00,203 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 3.0 (TID 11, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:31:00,203 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 3.0 (TID 11)
2020-04-08 14:31:00,203 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 3.0 (TID 10) in 4 ms on localhost (executor driver) (2/3)
2020-04-08 14:31:00,204 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 14:31:00,205 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 3.0 (TID 11). 2785 bytes result sent to driver
2020-04-08 14:31:00,206 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 3.0 (TID 11) in 3 ms on localhost (executor driver) (3/3)
2020-04-08 14:31:00,206 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
2020-04-08 14:31:00,206 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 3 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134) finished in 0.015 s
2020-04-08 14:31:00,207 INFO org.apache.spark.scheduler.DAGScheduler: Job 3 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:134, took 0.017785 s
2020-04-08 14:31:00,207 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586327460000 ms.0 from job set of time 1586327460000 ms
2020-04-08 14:31:00,207 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.207 s for time 1586327460000 ms (execution: 0.025 s)
2020-04-08 14:31:00,207 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 2 from persistence list
2020-04-08 14:31:00,208 INFO org.apache.spark.storage.BlockManager: Removing RDD 2
2020-04-08 14:31:00,208 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 14:31:00,208 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586327440000 ms
2020-04-08 14:31:10,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 14:31:10,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 14:31:10,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 14:31:10,182 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 14:31:10,182 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 153.
2020-04-08 14:31:10,183 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 152.
2020-04-08 14:31:10,184 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586327470000 ms
2020-04-08 14:31:10,184 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586327470000 ms.0 from job set of time 1586327470000 ms
2020-04-08 14:31:10,190 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:134
2020-04-08 14:31:10,191 INFO org.apache.spark.scheduler.DAGScheduler: Got job 4 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134) with 3 output partitions
2020-04-08 14:31:10,191 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 4 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134)
2020-04-08 14:31:10,191 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 14:31:10,192 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 14:31:10,192 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 4 (KafkaRDD[4] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113), which has no missing parents
2020-04-08 14:31:10,195 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 14:31:10,196 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 14:31:10,197 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.102:50986 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 14:31:10,197 INFO org.apache.spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1161
2020-04-08 14:31:10,198 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 4 (KafkaRDD[4] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 14:31:10,198 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 4.0 with 3 tasks
2020-04-08 14:31:10,199 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 12, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:31:10,199 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 4.0 (TID 12)
2020-04-08 14:31:10,201 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 1
2020-04-08 14:31:10,202 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 4.0 (TID 12). 2785 bytes result sent to driver
2020-04-08 14:31:10,202 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 4.0 (TID 13, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:31:10,202 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 4.0 (TID 13)
2020-04-08 14:31:10,202 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 12) in 3 ms on localhost (executor driver) (1/3)
2020-04-08 14:31:10,203 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 152 is the same as ending offset skipping PABUSI2 2
2020-04-08 14:31:10,204 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 4.0 (TID 13). 2785 bytes result sent to driver
2020-04-08 14:31:10,204 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 4.0 (TID 14, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:31:10,205 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 4.0 (TID 14)
2020-04-08 14:31:10,205 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 4.0 (TID 13) in 3 ms on localhost (executor driver) (2/3)
2020-04-08 14:31:10,206 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 14:31:10,206 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 4.0 (TID 14). 2785 bytes result sent to driver
2020-04-08 14:31:10,207 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 4.0 (TID 14) in 3 ms on localhost (executor driver) (3/3)
2020-04-08 14:31:10,207 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
2020-04-08 14:31:10,208 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 4 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134) finished in 0.014 s
2020-04-08 14:31:10,208 INFO org.apache.spark.scheduler.DAGScheduler: Job 4 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:134, took 0.017164 s
2020-04-08 14:31:10,208 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586327470000 ms.0 from job set of time 1586327470000 ms
2020-04-08 14:31:10,208 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.208 s for time 1586327470000 ms (execution: 0.024 s)
2020-04-08 14:31:10,208 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 3 from persistence list
2020-04-08 14:31:10,209 INFO org.apache.spark.storage.BlockManager: Removing RDD 3
2020-04-08 14:31:10,209 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 14:31:10,209 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586327450000 ms
2020-04-08 14:31:20,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 14:31:20,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 14:31:20,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 14:31:20,181 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 14:31:20,182 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 153.
2020-04-08 14:31:20,182 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 152.
2020-04-08 14:31:20,183 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586327480000 ms
2020-04-08 14:31:20,183 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586327480000 ms.0 from job set of time 1586327480000 ms
2020-04-08 14:31:20,189 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:134
2020-04-08 14:31:20,190 INFO org.apache.spark.scheduler.DAGScheduler: Got job 5 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134) with 3 output partitions
2020-04-08 14:31:20,190 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 5 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134)
2020-04-08 14:31:20,190 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 14:31:20,190 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 14:31:20,190 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 5 (KafkaRDD[5] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113), which has no missing parents
2020-04-08 14:31:20,192 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 14:31:20,194 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 14:31:20,194 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.102:50986 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 14:31:20,195 INFO org.apache.spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1161
2020-04-08 14:31:20,195 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 5 (KafkaRDD[5] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 14:31:20,195 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 5.0 with 3 tasks
2020-04-08 14:31:20,196 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 15, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:31:20,197 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 5.0 (TID 15)
2020-04-08 14:31:20,199 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 1
2020-04-08 14:31:20,200 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 5.0 (TID 15). 2871 bytes result sent to driver
2020-04-08 14:31:20,200 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 5.0 (TID 16, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:31:20,201 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 5.0 (TID 16)
2020-04-08 14:31:20,201 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 15) in 5 ms on localhost (executor driver) (1/3)
2020-04-08 14:31:20,202 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 152 is the same as ending offset skipping PABUSI2 2
2020-04-08 14:31:20,202 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 5.0 (TID 16). 2785 bytes result sent to driver
2020-04-08 14:31:20,203 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 5.0 (TID 17, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:31:20,203 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 5.0 (TID 17)
2020-04-08 14:31:20,203 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 5.0 (TID 16) in 3 ms on localhost (executor driver) (2/3)
2020-04-08 14:31:20,204 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 14:31:20,205 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 5.0 (TID 17). 2785 bytes result sent to driver
2020-04-08 14:31:20,205 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 5.0 (TID 17) in 2 ms on localhost (executor driver) (3/3)
2020-04-08 14:31:20,206 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
2020-04-08 14:31:20,206 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 5 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134) finished in 0.015 s
2020-04-08 14:31:20,206 INFO org.apache.spark.scheduler.DAGScheduler: Job 5 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:134, took 0.017230 s
2020-04-08 14:31:20,207 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586327480000 ms.0 from job set of time 1586327480000 ms
2020-04-08 14:31:20,207 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.207 s for time 1586327480000 ms (execution: 0.024 s)
2020-04-08 14:31:20,207 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 4 from persistence list
2020-04-08 14:31:20,207 INFO org.apache.spark.storage.BlockManager: Removing RDD 4
2020-04-08 14:31:20,208 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 14:31:20,208 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586327460000 ms
2020-04-08 14:31:30,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 14:31:30,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 14:31:30,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 14:31:30,180 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 153.
2020-04-08 14:31:30,180 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 14:31:30,181 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 152.
2020-04-08 14:31:30,182 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586327490000 ms
2020-04-08 14:31:30,182 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586327490000 ms.0 from job set of time 1586327490000 ms
2020-04-08 14:31:30,188 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:134
2020-04-08 14:31:30,188 INFO org.apache.spark.scheduler.DAGScheduler: Got job 6 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134) with 3 output partitions
2020-04-08 14:31:30,188 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 6 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134)
2020-04-08 14:31:30,189 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 14:31:30,189 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 14:31:30,189 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 6 (KafkaRDD[6] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113), which has no missing parents
2020-04-08 14:31:30,191 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 14:31:30,193 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 14:31:30,194 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.0.102:50986 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 14:31:30,194 INFO org.apache.spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1161
2020-04-08 14:31:30,194 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 6 (KafkaRDD[6] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 14:31:30,195 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 6.0 with 3 tasks
2020-04-08 14:31:30,195 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 18, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:31:30,196 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 6.0 (TID 18)
2020-04-08 14:31:30,197 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 1
2020-04-08 14:31:30,198 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 6.0 (TID 18). 2785 bytes result sent to driver
2020-04-08 14:31:30,198 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 6.0 (TID 19, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:31:30,198 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 6.0 (TID 19)
2020-04-08 14:31:30,198 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 18) in 3 ms on localhost (executor driver) (1/3)
2020-04-08 14:31:30,199 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 152 is the same as ending offset skipping PABUSI2 2
2020-04-08 14:31:30,200 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 6.0 (TID 19). 2828 bytes result sent to driver
2020-04-08 14:31:30,201 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 6.0 (TID 20, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:31:30,201 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 6.0 (TID 20)
2020-04-08 14:31:30,201 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 6.0 (TID 19) in 3 ms on localhost (executor driver) (2/3)
2020-04-08 14:31:30,202 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 14:31:30,203 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 6.0 (TID 20). 2785 bytes result sent to driver
2020-04-08 14:31:30,203 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 6.0 (TID 20) in 2 ms on localhost (executor driver) (3/3)
2020-04-08 14:31:30,204 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
2020-04-08 14:31:30,204 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 6 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134) finished in 0.014 s
2020-04-08 14:31:30,204 INFO org.apache.spark.scheduler.DAGScheduler: Job 6 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:134, took 0.016410 s
2020-04-08 14:31:30,205 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586327490000 ms.0 from job set of time 1586327490000 ms
2020-04-08 14:31:30,205 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.205 s for time 1586327490000 ms (execution: 0.023 s)
2020-04-08 14:31:30,205 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 5 from persistence list
2020-04-08 14:31:30,205 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 14:31:30,206 INFO org.apache.spark.storage.BlockManager: Removing RDD 5
2020-04-08 14:31:30,206 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586327470000 ms
2020-04-08 14:31:40,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 14:31:40,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 14:31:40,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 14:31:40,182 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 14:31:40,182 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 153.
2020-04-08 14:31:40,182 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 152.
2020-04-08 14:31:40,183 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586327500000 ms
2020-04-08 14:31:40,183 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586327500000 ms.0 from job set of time 1586327500000 ms
2020-04-08 14:31:40,189 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:134
2020-04-08 14:31:40,189 INFO org.apache.spark.scheduler.DAGScheduler: Got job 7 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134) with 3 output partitions
2020-04-08 14:31:40,189 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 7 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134)
2020-04-08 14:31:40,190 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 14:31:40,190 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 14:31:40,190 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 7 (KafkaRDD[7] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113), which has no missing parents
2020-04-08 14:31:40,194 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 14:31:40,195 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 14:31:40,196 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.0.102:50986 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 14:31:40,196 INFO org.apache.spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1161
2020-04-08 14:31:40,197 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 7 (KafkaRDD[7] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 14:31:40,197 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 7.0 with 3 tasks
2020-04-08 14:31:40,197 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 7.0 (TID 21, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:31:40,198 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 7.0 (TID 21)
2020-04-08 14:31:40,199 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 1
2020-04-08 14:31:40,200 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 7.0 (TID 21). 2785 bytes result sent to driver
2020-04-08 14:31:40,200 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 7.0 (TID 22, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:31:40,201 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 7.0 (TID 22)
2020-04-08 14:31:40,201 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 7.0 (TID 21) in 4 ms on localhost (executor driver) (1/3)
2020-04-08 14:31:40,201 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 152 is the same as ending offset skipping PABUSI2 2
2020-04-08 14:31:40,202 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 7.0 (TID 22). 2785 bytes result sent to driver
2020-04-08 14:31:40,203 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 7.0 (TID 23, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:31:40,203 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 7.0 (TID 23)
2020-04-08 14:31:40,203 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 7.0 (TID 22) in 3 ms on localhost (executor driver) (2/3)
2020-04-08 14:31:40,204 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 14:31:40,204 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 7.0 (TID 23). 2785 bytes result sent to driver
2020-04-08 14:31:40,205 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 7.0 (TID 23) in 3 ms on localhost (executor driver) (3/3)
2020-04-08 14:31:40,205 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
2020-04-08 14:31:40,205 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 7 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134) finished in 0.014 s
2020-04-08 14:31:40,206 INFO org.apache.spark.scheduler.DAGScheduler: Job 7 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:134, took 0.016675 s
2020-04-08 14:31:40,206 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586327500000 ms.0 from job set of time 1586327500000 ms
2020-04-08 14:31:40,206 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.206 s for time 1586327500000 ms (execution: 0.023 s)
2020-04-08 14:31:40,206 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 6 from persistence list
2020-04-08 14:31:40,207 INFO org.apache.spark.storage.BlockManager: Removing RDD 6
2020-04-08 14:31:40,207 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 14:31:40,207 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586327480000 ms
2020-04-08 14:31:50,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 14:31:50,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 14:31:50,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 14:31:50,178 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 152.
2020-04-08 14:31:50,178 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 153.
2020-04-08 14:31:50,178 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 14:31:50,179 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586327510000 ms
2020-04-08 14:31:50,179 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586327510000 ms.0 from job set of time 1586327510000 ms
2020-04-08 14:31:50,184 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:134
2020-04-08 14:31:50,185 INFO org.apache.spark.scheduler.DAGScheduler: Got job 8 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134) with 3 output partitions
2020-04-08 14:31:50,185 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 8 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134)
2020-04-08 14:31:50,185 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 14:31:50,185 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 14:31:50,186 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 8 (KafkaRDD[8] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113), which has no missing parents
2020-04-08 14:31:50,188 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 14:31:50,189 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 14:31:50,190 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.0.102:50986 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 14:31:50,190 INFO org.apache.spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1161
2020-04-08 14:31:50,190 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 8 (KafkaRDD[8] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 14:31:50,190 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 8.0 with 3 tasks
2020-04-08 14:31:50,191 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 24, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:31:50,191 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 8.0 (TID 24)
2020-04-08 14:31:50,192 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 1
2020-04-08 14:31:50,193 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 8.0 (TID 24). 2828 bytes result sent to driver
2020-04-08 14:31:50,193 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 8.0 (TID 25, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:31:50,194 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 8.0 (TID 25)
2020-04-08 14:31:50,194 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 24) in 3 ms on localhost (executor driver) (1/3)
2020-04-08 14:31:50,195 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 152 is the same as ending offset skipping PABUSI2 2
2020-04-08 14:31:50,195 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 8.0 (TID 25). 2785 bytes result sent to driver
2020-04-08 14:31:50,196 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 8.0 (TID 26, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:31:50,196 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 8.0 (TID 26)
2020-04-08 14:31:50,196 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 8.0 (TID 25) in 3 ms on localhost (executor driver) (2/3)
2020-04-08 14:31:50,197 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 14:31:50,197 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 8.0 (TID 26). 2785 bytes result sent to driver
2020-04-08 14:31:50,198 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 8.0 (TID 26) in 3 ms on localhost (executor driver) (3/3)
2020-04-08 14:31:50,198 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
2020-04-08 14:31:50,198 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 8 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134) finished in 0.011 s
2020-04-08 14:31:50,198 INFO org.apache.spark.scheduler.DAGScheduler: Job 8 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:134, took 0.013795 s
2020-04-08 14:31:50,199 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586327510000 ms.0 from job set of time 1586327510000 ms
2020-04-08 14:31:50,199 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 7 from persistence list
2020-04-08 14:31:50,199 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.199 s for time 1586327510000 ms (execution: 0.020 s)
2020-04-08 14:31:50,199 INFO org.apache.spark.storage.BlockManager: Removing RDD 7
2020-04-08 14:31:50,199 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 14:31:50,200 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586327490000 ms
2020-04-08 14:32:00,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 14:32:00,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 14:32:00,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 14:32:00,182 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 153.
2020-04-08 14:32:00,182 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 152.
2020-04-08 14:32:00,182 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 14:32:00,183 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586327520000 ms
2020-04-08 14:32:00,184 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586327520000 ms.0 from job set of time 1586327520000 ms
2020-04-08 14:32:00,194 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:134
2020-04-08 14:32:00,197 INFO org.apache.spark.scheduler.DAGScheduler: Got job 9 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134) with 3 output partitions
2020-04-08 14:32:00,197 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 9 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134)
2020-04-08 14:32:00,197 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 14:32:00,198 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 14:32:00,200 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 9 (KafkaRDD[9] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113), which has no missing parents
2020-04-08 14:32:00,207 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 14:32:00,213 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 14:32:00,215 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.0.102:50986 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 14:32:00,216 INFO org.apache.spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1161
2020-04-08 14:32:00,217 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 9 (KafkaRDD[9] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 14:32:00,217 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 9.0 with 3 tasks
2020-04-08 14:32:00,219 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 27, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:32:00,220 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 9.0 (TID 27)
2020-04-08 14:32:00,224 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 1
2020-04-08 14:32:00,225 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 9.0 (TID 27). 2828 bytes result sent to driver
2020-04-08 14:32:00,226 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 9.0 (TID 28, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:32:00,226 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 27) in 7 ms on localhost (executor driver) (1/3)
2020-04-08 14:32:00,226 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 9.0 (TID 28)
2020-04-08 14:32:00,229 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 152 is the same as ending offset skipping PABUSI2 2
2020-04-08 14:32:00,229 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 9.0 (TID 28). 2828 bytes result sent to driver
2020-04-08 14:32:00,230 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 9.0 (TID 29, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:32:00,230 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 9.0 (TID 28) in 4 ms on localhost (executor driver) (2/3)
2020-04-08 14:32:00,230 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 9.0 (TID 29)
2020-04-08 14:32:00,232 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 14:32:00,232 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 9.0 (TID 29). 2785 bytes result sent to driver
2020-04-08 14:32:00,233 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 9.0 (TID 29) in 3 ms on localhost (executor driver) (3/3)
2020-04-08 14:32:00,233 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
2020-04-08 14:32:00,233 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 9 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134) finished in 0.029 s
2020-04-08 14:32:00,234 INFO org.apache.spark.scheduler.DAGScheduler: Job 9 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:134, took 0.038337 s
2020-04-08 14:32:00,234 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586327520000 ms.0 from job set of time 1586327520000 ms
2020-04-08 14:32:00,234 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.234 s for time 1586327520000 ms (execution: 0.050 s)
2020-04-08 14:32:00,234 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 8 from persistence list
2020-04-08 14:32:00,235 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 14:32:00,235 INFO org.apache.spark.storage.BlockManager: Removing RDD 8
2020-04-08 14:32:00,235 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586327500000 ms
2020-04-08 14:32:10,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 14:32:10,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 14:32:10,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 14:32:10,266 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 152.
2020-04-08 14:32:10,267 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 14:32:10,267 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 153.
2020-04-08 14:32:10,268 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586327530000 ms
2020-04-08 14:32:10,268 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586327530000 ms.0 from job set of time 1586327530000 ms
2020-04-08 14:32:10,274 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:134
2020-04-08 14:32:10,276 INFO org.apache.spark.scheduler.DAGScheduler: Got job 10 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134) with 3 output partitions
2020-04-08 14:32:10,276 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 10 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134)
2020-04-08 14:32:10,276 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 14:32:10,276 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 14:32:10,277 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 10 (KafkaRDD[10] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113), which has no missing parents
2020-04-08 14:32:10,280 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 14:32:10,282 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 14:32:10,284 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 192.168.0.102:50986 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 14:32:10,285 INFO org.apache.spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1161
2020-04-08 14:32:10,286 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 10 (KafkaRDD[10] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 14:32:10,286 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 10.0 with 3 tasks
2020-04-08 14:32:10,287 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 30, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:32:10,287 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 10.0 (TID 30)
2020-04-08 14:32:10,288 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 1
2020-04-08 14:32:10,289 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 10.0 (TID 30). 2785 bytes result sent to driver
2020-04-08 14:32:10,289 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 10.0 (TID 31, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:32:10,290 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 10.0 (TID 31)
2020-04-08 14:32:10,290 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 30) in 2 ms on localhost (executor driver) (1/3)
2020-04-08 14:32:10,290 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 152 is the same as ending offset skipping PABUSI2 2
2020-04-08 14:32:10,291 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 10.0 (TID 31). 2785 bytes result sent to driver
2020-04-08 14:32:10,291 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 10.0 (TID 32, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:32:10,292 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 10.0 (TID 32)
2020-04-08 14:32:10,292 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 10.0 (TID 31) in 3 ms on localhost (executor driver) (2/3)
2020-04-08 14:32:10,294 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 14:32:10,296 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 10.0 (TID 32). 2828 bytes result sent to driver
2020-04-08 14:32:10,297 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 10.0 (TID 32) in 6 ms on localhost (executor driver) (3/3)
2020-04-08 14:32:10,297 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
2020-04-08 14:32:10,297 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 10 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134) finished in 0.019 s
2020-04-08 14:32:10,298 INFO org.apache.spark.scheduler.DAGScheduler: Job 10 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:134, took 0.023339 s
2020-04-08 14:32:10,298 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586327530000 ms.0 from job set of time 1586327530000 ms
2020-04-08 14:32:10,298 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 9 from persistence list
2020-04-08 14:32:10,298 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.298 s for time 1586327530000 ms (execution: 0.030 s)
2020-04-08 14:32:10,301 INFO org.apache.spark.storage.BlockManager: Removing RDD 9
2020-04-08 14:32:10,301 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 14:32:10,301 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586327510000 ms
2020-04-08 14:32:20,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 14:32:20,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 14:32:20,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 14:32:20,187 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 14:32:20,187 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 153.
2020-04-08 14:32:20,188 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 152.
2020-04-08 14:32:20,188 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586327540000 ms
2020-04-08 14:32:20,189 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586327540000 ms.0 from job set of time 1586327540000 ms
2020-04-08 14:32:20,194 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:134
2020-04-08 14:32:20,194 INFO org.apache.spark.scheduler.DAGScheduler: Got job 11 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134) with 3 output partitions
2020-04-08 14:32:20,194 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 11 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134)
2020-04-08 14:32:20,194 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 14:32:20,194 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 14:32:20,195 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 11 (KafkaRDD[11] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113), which has no missing parents
2020-04-08 14:32:20,197 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 14:32:20,199 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 14:32:20,199 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 192.168.0.102:50986 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 14:32:20,200 INFO org.apache.spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1161
2020-04-08 14:32:20,200 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 11 (KafkaRDD[11] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 14:32:20,200 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 11.0 with 3 tasks
2020-04-08 14:32:20,201 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 33, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:32:20,201 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 11.0 (TID 33)
2020-04-08 14:32:20,202 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 1
2020-04-08 14:32:20,203 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 11.0 (TID 33). 2785 bytes result sent to driver
2020-04-08 14:32:20,203 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 11.0 (TID 34, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:32:20,203 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 11.0 (TID 34)
2020-04-08 14:32:20,203 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 33) in 2 ms on localhost (executor driver) (1/3)
2020-04-08 14:32:20,204 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 152 is the same as ending offset skipping PABUSI2 2
2020-04-08 14:32:20,204 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 11.0 (TID 34). 2785 bytes result sent to driver
2020-04-08 14:32:20,205 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 11.0 (TID 35, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:32:20,205 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 11.0 (TID 34) in 2 ms on localhost (executor driver) (2/3)
2020-04-08 14:32:20,205 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 11.0 (TID 35)
2020-04-08 14:32:20,206 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 14:32:20,206 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 11.0 (TID 35). 2785 bytes result sent to driver
2020-04-08 14:32:20,207 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 11.0 (TID 35) in 2 ms on localhost (executor driver) (3/3)
2020-04-08 14:32:20,207 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
2020-04-08 14:32:20,207 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 11 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134) finished in 0.011 s
2020-04-08 14:32:20,207 INFO org.apache.spark.scheduler.DAGScheduler: Job 11 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:134, took 0.013500 s
2020-04-08 14:32:20,208 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586327540000 ms.0 from job set of time 1586327540000 ms
2020-04-08 14:32:20,208 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.208 s for time 1586327540000 ms (execution: 0.019 s)
2020-04-08 14:32:20,208 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 10 from persistence list
2020-04-08 14:32:20,208 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 14:32:20,209 INFO org.apache.spark.storage.BlockManager: Removing RDD 10
2020-04-08 14:32:20,209 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586327520000 ms
2020-04-08 14:32:30,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 14:32:30,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 14:32:30,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 14:32:30,186 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 153.
2020-04-08 14:32:30,186 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 152.
2020-04-08 14:32:30,186 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 14:32:30,187 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586327550000 ms
2020-04-08 14:32:30,188 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586327550000 ms.0 from job set of time 1586327550000 ms
2020-04-08 14:32:30,194 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:134
2020-04-08 14:32:30,194 INFO org.apache.spark.scheduler.DAGScheduler: Got job 12 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134) with 3 output partitions
2020-04-08 14:32:30,194 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 12 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134)
2020-04-08 14:32:30,194 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 14:32:30,194 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 14:32:30,195 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 12 (KafkaRDD[12] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113), which has no missing parents
2020-04-08 14:32:30,197 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 14:32:30,198 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 14:32:30,198 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 192.168.0.102:50986 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 14:32:30,199 INFO org.apache.spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1161
2020-04-08 14:32:30,199 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 12 (KafkaRDD[12] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 14:32:30,199 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 12.0 with 3 tasks
2020-04-08 14:32:30,200 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 12.0 (TID 36, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:32:30,200 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 12.0 (TID 36)
2020-04-08 14:32:30,201 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 1
2020-04-08 14:32:30,202 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 12.0 (TID 36). 2828 bytes result sent to driver
2020-04-08 14:32:30,202 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 12.0 (TID 37, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:32:30,202 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 12.0 (TID 37)
2020-04-08 14:32:30,202 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 12.0 (TID 36) in 2 ms on localhost (executor driver) (1/3)
2020-04-08 14:32:30,203 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 152 is the same as ending offset skipping PABUSI2 2
2020-04-08 14:32:30,204 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 12.0 (TID 37). 2785 bytes result sent to driver
2020-04-08 14:32:30,204 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 12.0 (TID 38, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:32:30,204 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 12.0 (TID 37) in 2 ms on localhost (executor driver) (2/3)
2020-04-08 14:32:30,204 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 12.0 (TID 38)
2020-04-08 14:32:30,205 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 14:32:30,206 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 12.0 (TID 38). 2785 bytes result sent to driver
2020-04-08 14:32:30,206 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 12.0 (TID 38) in 2 ms on localhost (executor driver) (3/3)
2020-04-08 14:32:30,206 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
2020-04-08 14:32:30,207 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 12 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134) finished in 0.010 s
2020-04-08 14:32:30,207 INFO org.apache.spark.scheduler.DAGScheduler: Job 12 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:134, took 0.013076 s
2020-04-08 14:32:30,207 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586327550000 ms.0 from job set of time 1586327550000 ms
2020-04-08 14:32:30,207 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.207 s for time 1586327550000 ms (execution: 0.019 s)
2020-04-08 14:32:30,207 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 11 from persistence list
2020-04-08 14:32:30,208 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 14:32:30,208 INFO org.apache.spark.storage.BlockManager: Removing RDD 11
2020-04-08 14:32:30,208 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586327530000 ms
2020-04-08 14:32:40,016 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 14:32:40,016 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 14:32:40,017 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 14:32:40,242 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 14:32:40,242 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 153.
2020-04-08 14:32:40,242 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 152.
2020-04-08 14:32:40,243 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586327560000 ms
2020-04-08 14:32:40,244 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586327560000 ms.0 from job set of time 1586327560000 ms
2020-04-08 14:32:40,252 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:134
2020-04-08 14:32:40,253 INFO org.apache.spark.scheduler.DAGScheduler: Got job 13 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134) with 3 output partitions
2020-04-08 14:32:40,253 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 13 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134)
2020-04-08 14:32:40,253 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 14:32:40,253 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 14:32:40,255 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 13 (KafkaRDD[13] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113), which has no missing parents
2020-04-08 14:32:40,258 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 14:32:40,261 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 14:32:40,263 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 192.168.0.102:50986 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 14:32:40,265 INFO org.apache.spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1161
2020-04-08 14:32:40,266 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 13 (KafkaRDD[13] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 14:32:40,266 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 13.0 with 3 tasks
2020-04-08 14:32:40,267 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 13.0 (TID 39, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:32:40,268 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 13.0 (TID 39)
2020-04-08 14:32:40,269 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 1
2020-04-08 14:32:40,270 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 13.0 (TID 39). 2785 bytes result sent to driver
2020-04-08 14:32:40,271 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 13.0 (TID 40, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:32:40,271 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 13.0 (TID 40)
2020-04-08 14:32:40,271 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 13.0 (TID 39) in 4 ms on localhost (executor driver) (1/3)
2020-04-08 14:32:40,272 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 152 is the same as ending offset skipping PABUSI2 2
2020-04-08 14:32:40,273 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 13.0 (TID 40). 2785 bytes result sent to driver
2020-04-08 14:32:40,274 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 13.0 (TID 41, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:32:40,274 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 13.0 (TID 41)
2020-04-08 14:32:40,274 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 13.0 (TID 40) in 3 ms on localhost (executor driver) (2/3)
2020-04-08 14:32:40,275 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 14:32:40,276 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 13.0 (TID 41). 2785 bytes result sent to driver
2020-04-08 14:32:40,276 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 13.0 (TID 41) in 3 ms on localhost (executor driver) (3/3)
2020-04-08 14:32:40,276 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
2020-04-08 14:32:40,277 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 13 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134) finished in 0.020 s
2020-04-08 14:32:40,277 INFO org.apache.spark.scheduler.DAGScheduler: Job 13 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:134, took 0.025169 s
2020-04-08 14:32:40,278 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586327560000 ms.0 from job set of time 1586327560000 ms
2020-04-08 14:32:40,278 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 12 from persistence list
2020-04-08 14:32:40,278 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.278 s for time 1586327560000 ms (execution: 0.034 s)
2020-04-08 14:32:40,279 INFO org.apache.spark.storage.BlockManager: Removing RDD 12
2020-04-08 14:32:40,279 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 14:32:40,279 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586327540000 ms
2020-04-08 14:32:50,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 14:32:50,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 14:32:50,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 14:32:50,181 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 14:32:50,181 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 153.
2020-04-08 14:32:50,183 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 152.
2020-04-08 14:32:50,184 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586327570000 ms
2020-04-08 14:32:50,184 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586327570000 ms.0 from job set of time 1586327570000 ms
2020-04-08 14:32:50,195 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 104
2020-04-08 14:32:50,195 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 45
2020-04-08 14:32:50,195 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 63
2020-04-08 14:32:50,195 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 315
2020-04-08 14:32:50,195 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:134
2020-04-08 14:32:50,195 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 193
2020-04-08 14:32:50,195 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 203
2020-04-08 14:32:50,195 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 106
2020-04-08 14:32:50,195 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 345
2020-04-08 14:32:50,195 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 148
2020-04-08 14:32:50,195 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 301
2020-04-08 14:32:50,195 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 147
2020-04-08 14:32:50,195 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 188
2020-04-08 14:32:50,195 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 201
2020-04-08 14:32:50,195 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 257
2020-04-08 14:32:50,195 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 91
2020-04-08 14:32:50,195 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 40
2020-04-08 14:32:50,195 INFO org.apache.spark.scheduler.DAGScheduler: Got job 14 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134) with 3 output partitions
2020-04-08 14:32:50,195 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 37
2020-04-08 14:32:50,195 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 14 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134)
2020-04-08 14:32:50,195 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 14:32:50,196 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 14:32:50,196 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 14 (KafkaRDD[14] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113), which has no missing parents
2020-04-08 14:32:50,199 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 14:32:50,200 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 14:32:50,200 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 192.168.0.102:50986 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 14:32:50,201 INFO org.apache.spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1161
2020-04-08 14:32:50,201 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 14 (KafkaRDD[14] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 14:32:50,201 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 14.0 with 3 tasks
2020-04-08 14:32:50,202 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_7_piece0 on 192.168.0.102:50986 in memory (size: 3.3 KB, free: 4.1 GB)
2020-04-08 14:32:50,202 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 42, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:32:50,202 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 14.0 (TID 42)
2020-04-08 14:32:50,203 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 1
2020-04-08 14:32:50,204 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 270
2020-04-08 14:32:50,204 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 155
2020-04-08 14:32:50,204 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 14.0 (TID 42). 2828 bytes result sent to driver
2020-04-08 14:32:50,204 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 192
2020-04-08 14:32:50,204 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 97
2020-04-08 14:32:50,204 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 27
2020-04-08 14:32:50,204 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 103
2020-04-08 14:32:50,204 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 253
2020-04-08 14:32:50,204 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 100
2020-04-08 14:32:50,204 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 328
2020-04-08 14:32:50,204 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 14.0 (TID 43, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:32:50,204 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 111
2020-04-08 14:32:50,204 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 20
2020-04-08 14:32:50,205 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 42) in 3 ms on localhost (executor driver) (1/3)
2020-04-08 14:32:50,205 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 14.0 (TID 43)
2020-04-08 14:32:50,205 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 183
2020-04-08 14:32:50,205 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 81
2020-04-08 14:32:50,205 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 277
2020-04-08 14:32:50,205 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 288
2020-04-08 14:32:50,205 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 305
2020-04-08 14:32:50,205 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 176
2020-04-08 14:32:50,205 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 173
2020-04-08 14:32:50,205 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 247
2020-04-08 14:32:50,205 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 218
2020-04-08 14:32:50,205 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 344
2020-04-08 14:32:50,205 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 60
2020-04-08 14:32:50,205 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 232
2020-04-08 14:32:50,205 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 237
2020-04-08 14:32:50,205 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 31
2020-04-08 14:32:50,205 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 323
2020-04-08 14:32:50,205 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 207
2020-04-08 14:32:50,205 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 295
2020-04-08 14:32:50,205 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 36
2020-04-08 14:32:50,206 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 235
2020-04-08 14:32:50,206 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 76
2020-04-08 14:32:50,206 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 152 is the same as ending offset skipping PABUSI2 2
2020-04-08 14:32:50,206 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 14.0 (TID 43). 2785 bytes result sent to driver
2020-04-08 14:32:50,207 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_9_piece0 on 192.168.0.102:50986 in memory (size: 3.3 KB, free: 4.1 GB)
2020-04-08 14:32:50,207 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 14.0 (TID 44, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:32:50,207 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 14.0 (TID 44)
2020-04-08 14:32:50,207 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 14.0 (TID 43) in 3 ms on localhost (executor driver) (2/3)
2020-04-08 14:32:50,208 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 14:32:50,208 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 302
2020-04-08 14:32:50,209 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 47
2020-04-08 14:32:50,209 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 230
2020-04-08 14:32:50,209 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 135
2020-04-08 14:32:50,209 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 55
2020-04-08 14:32:50,209 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 14.0 (TID 44). 2785 bytes result sent to driver
2020-04-08 14:32:50,209 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 110
2020-04-08 14:32:50,209 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 21
2020-04-08 14:32:50,209 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 304
2020-04-08 14:32:50,209 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 171
2020-04-08 14:32:50,209 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 123
2020-04-08 14:32:50,209 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 197
2020-04-08 14:32:50,209 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 341
2020-04-08 14:32:50,209 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 99
2020-04-08 14:32:50,209 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 330
2020-04-08 14:32:50,209 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 14.0 (TID 44) in 2 ms on localhost (executor driver) (3/3)
2020-04-08 14:32:50,210 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
2020-04-08 14:32:50,210 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 14 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134) finished in 0.012 s
2020-04-08 14:32:50,211 INFO org.apache.spark.scheduler.DAGScheduler: Job 14 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:134, took 0.015778 s
2020-04-08 14:32:50,211 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586327570000 ms.0 from job set of time 1586327570000 ms
2020-04-08 14:32:50,211 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.102:50986 in memory (size: 3.3 KB, free: 4.1 GB)
2020-04-08 14:32:50,211 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.211 s for time 1586327570000 ms (execution: 0.027 s)
2020-04-08 14:32:50,211 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 13 from persistence list
2020-04-08 14:32:50,212 INFO org.apache.spark.storage.BlockManager: Removing RDD 13
2020-04-08 14:32:50,212 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 14:32:50,212 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586327550000 ms
2020-04-08 14:32:50,213 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 56
2020-04-08 14:32:50,213 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 126
2020-04-08 14:32:50,213 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 227
2020-04-08 14:32:50,213 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 268
2020-04-08 14:32:50,213 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 296
2020-04-08 14:32:50,213 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 122
2020-04-08 14:32:50,213 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 92
2020-04-08 14:32:50,213 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 273
2020-04-08 14:32:50,213 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 57
2020-04-08 14:32:50,214 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 154
2020-04-08 14:32:50,214 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 281
2020-04-08 14:32:50,214 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 256
2020-04-08 14:32:50,214 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 85
2020-04-08 14:32:50,214 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 211
2020-04-08 14:32:50,214 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 59
2020-04-08 14:32:50,214 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 182
2020-04-08 14:32:50,214 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 255
2020-04-08 14:32:50,214 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 35
2020-04-08 14:32:50,214 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 117
2020-04-08 14:32:50,214 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 242
2020-04-08 14:32:50,214 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 262
2020-04-08 14:32:50,214 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 318
2020-04-08 14:32:50,214 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 49
2020-04-08 14:32:50,216 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.0.102:50986 in memory (size: 3.3 KB, free: 4.1 GB)
2020-04-08 14:32:50,218 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 245
2020-04-08 14:32:50,218 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 23
2020-04-08 14:32:50,218 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 114
2020-04-08 14:32:50,218 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 140
2020-04-08 14:32:50,218 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 186
2020-04-08 14:32:50,218 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 222
2020-04-08 14:32:50,218 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 350
2020-04-08 14:32:50,218 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 320
2020-04-08 14:32:50,219 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_13_piece0 on 192.168.0.102:50986 in memory (size: 3.3 KB, free: 4.1 GB)
2020-04-08 14:32:50,220 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 65
2020-04-08 14:32:50,220 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 116
2020-04-08 14:32:50,220 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 22
2020-04-08 14:32:50,220 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 66
2020-04-08 14:32:50,220 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 349
2020-04-08 14:32:50,220 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 194
2020-04-08 14:32:50,221 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.0.102:50986 in memory (size: 3.3 KB, free: 4.1 GB)
2020-04-08 14:32:50,223 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 175
2020-04-08 14:32:50,223 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 165
2020-04-08 14:32:50,223 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 88
2020-04-08 14:32:50,223 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 167
2020-04-08 14:32:50,223 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 236
2020-04-08 14:32:50,223 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 128
2020-04-08 14:32:50,223 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 198
2020-04-08 14:32:50,223 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 130
2020-04-08 14:32:50,223 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 70
2020-04-08 14:32:50,223 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 239
2020-04-08 14:32:50,223 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 145
2020-04-08 14:32:50,223 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 294
2020-04-08 14:32:50,223 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 62
2020-04-08 14:32:50,223 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 98
2020-04-08 14:32:50,223 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 131
2020-04-08 14:32:50,223 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 24
2020-04-08 14:32:50,223 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 279
2020-04-08 14:32:50,224 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_12_piece0 on 192.168.0.102:50986 in memory (size: 3.3 KB, free: 4.1 GB)
2020-04-08 14:32:50,225 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 234
2020-04-08 14:32:50,226 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 208
2020-04-08 14:32:50,226 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 276
2020-04-08 14:32:50,226 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 77
2020-04-08 14:32:50,226 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 313
2020-04-08 14:32:50,226 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 158
2020-04-08 14:32:50,226 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 139
2020-04-08 14:32:50,226 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 267
2020-04-08 14:32:50,226 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 298
2020-04-08 14:32:50,226 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 156
2020-04-08 14:32:50,226 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 14
2020-04-08 14:32:50,226 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 180
2020-04-08 14:32:50,226 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 166
2020-04-08 14:32:50,226 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 74
2020-04-08 14:32:50,226 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 325
2020-04-08 14:32:50,226 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 314
2020-04-08 14:32:50,226 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 133
2020-04-08 14:32:50,226 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 250
2020-04-08 14:32:50,227 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_6_piece0 on 192.168.0.102:50986 in memory (size: 3.3 KB, free: 4.1 GB)
2020-04-08 14:32:50,228 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 209
2020-04-08 14:32:50,228 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 118
2020-04-08 14:32:50,229 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 17
2020-04-08 14:32:50,229 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 67
2020-04-08 14:32:50,229 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 204
2020-04-08 14:32:50,229 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 39
2020-04-08 14:32:50,229 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 219
2020-04-08 14:32:50,229 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 29
2020-04-08 14:32:50,229 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 108
2020-04-08 14:32:50,229 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 199
2020-04-08 14:32:50,229 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 33
2020-04-08 14:32:50,229 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 107
2020-04-08 14:32:50,229 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 83
2020-04-08 14:32:50,229 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 144
2020-04-08 14:32:50,229 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 215
2020-04-08 14:32:50,229 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 53
2020-04-08 14:32:50,229 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 80
2020-04-08 14:32:50,229 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 50
2020-04-08 14:32:50,229 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 157
2020-04-08 14:32:50,229 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 326
2020-04-08 14:32:50,229 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 161
2020-04-08 14:32:50,229 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 141
2020-04-08 14:32:50,229 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 312
2020-04-08 14:32:50,230 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.102:50986 in memory (size: 3.3 KB, free: 4.1 GB)
2020-04-08 14:32:50,231 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 162
2020-04-08 14:32:50,232 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 272
2020-04-08 14:32:50,232 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 54
2020-04-08 14:32:50,232 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 225
2020-04-08 14:32:50,232 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 322
2020-04-08 14:32:50,232 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 187
2020-04-08 14:32:50,232 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 317
2020-04-08 14:32:50,232 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 13
2020-04-08 14:32:50,232 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 96
2020-04-08 14:32:50,232 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 137
2020-04-08 14:32:50,232 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 238
2020-04-08 14:32:50,232 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 221
2020-04-08 14:32:50,232 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 78
2020-04-08 14:32:50,232 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 16
2020-04-08 14:32:50,232 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 244
2020-04-08 14:32:50,232 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 264
2020-04-08 14:32:50,232 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 251
2020-04-08 14:32:50,232 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 178
2020-04-08 14:32:50,232 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 340
2020-04-08 14:32:50,232 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 163
2020-04-08 14:32:50,232 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 90
2020-04-08 14:32:50,232 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 143
2020-04-08 14:32:50,232 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 190
2020-04-08 14:32:50,233 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 152
2020-04-08 14:32:50,233 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 120
2020-04-08 14:32:50,234 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.0.102:50986 in memory (size: 3.3 KB, free: 4.1 GB)
2020-04-08 14:32:50,236 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 71
2020-04-08 14:32:50,236 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 43
2020-04-08 14:32:50,237 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 113
2020-04-08 14:32:50,237 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 346
2020-04-08 14:32:50,237 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 132
2020-04-08 14:32:50,237 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 285
2020-04-08 14:32:50,237 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 289
2020-04-08 14:32:50,237 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 51
2020-04-08 14:32:50,237 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 64
2020-04-08 14:32:50,237 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 241
2020-04-08 14:32:50,237 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 206
2020-04-08 14:32:50,239 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.0.102:50986 in memory (size: 3.3 KB, free: 4.1 GB)
2020-04-08 14:32:50,242 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 172
2020-04-08 14:32:50,242 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 105
2020-04-08 14:32:50,242 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 275
2020-04-08 14:32:50,242 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 335
2020-04-08 14:32:50,242 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 324
2020-04-08 14:32:50,242 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 121
2020-04-08 14:32:50,242 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 299
2020-04-08 14:32:50,242 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 30
2020-04-08 14:32:50,242 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 127
2020-04-08 14:32:50,242 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 185
2020-04-08 14:32:50,242 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 112
2020-04-08 14:32:50,242 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 119
2020-04-08 14:32:50,242 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 146
2020-04-08 14:32:50,242 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 269
2020-04-08 14:32:50,242 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 321
2020-04-08 14:32:50,242 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 41
2020-04-08 14:32:50,242 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 19
2020-04-08 14:32:50,242 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 287
2020-04-08 14:32:50,242 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 151
2020-04-08 14:32:50,243 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 226
2020-04-08 14:32:50,243 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 310
2020-04-08 14:32:50,243 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 125
2020-04-08 14:32:50,243 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 38
2020-04-08 14:32:50,243 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 229
2020-04-08 14:32:50,243 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 327
2020-04-08 14:32:50,243 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 261
2020-04-08 14:32:50,243 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 307
2020-04-08 14:32:50,243 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 48
2020-04-08 14:32:50,243 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 87
2020-04-08 14:32:50,243 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 72
2020-04-08 14:32:50,243 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 86
2020-04-08 14:32:50,243 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 1
2020-04-08 14:32:50,243 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 26
2020-04-08 14:32:50,243 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 136
2020-04-08 14:32:50,243 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 168
2020-04-08 14:32:50,243 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 134
2020-04-08 14:32:50,244 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 259
2020-04-08 14:32:50,244 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 4
2020-04-08 14:32:50,245 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_10_piece0 on 192.168.0.102:50986 in memory (size: 3.3 KB, free: 4.1 GB)
2020-04-08 14:32:50,247 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 44
2020-04-08 14:32:50,247 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 231
2020-04-08 14:32:50,247 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 233
2020-04-08 14:32:50,247 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 274
2020-04-08 14:32:50,247 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 93
2020-04-08 14:32:50,247 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 6
2020-04-08 14:32:50,247 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 15
2020-04-08 14:32:50,247 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 69
2020-04-08 14:32:50,247 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 286
2020-04-08 14:32:50,247 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 291
2020-04-08 14:32:50,248 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 329
2020-04-08 14:32:50,248 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 200
2020-04-08 14:32:50,248 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 220
2020-04-08 14:32:50,248 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 338
2020-04-08 14:32:50,248 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 169
2020-04-08 14:32:50,248 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 12
2020-04-08 14:32:50,248 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 153
2020-04-08 14:32:50,248 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 343
2020-04-08 14:32:50,248 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 348
2020-04-08 14:32:50,248 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 82
2020-04-08 14:32:50,248 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 258
2020-04-08 14:32:50,248 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 283
2020-04-08 14:32:50,248 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 290
2020-04-08 14:32:50,248 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 68
2020-04-08 14:32:50,248 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 3
2020-04-08 14:32:50,249 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 336
2020-04-08 14:32:50,249 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 28
2020-04-08 14:32:50,249 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 246
2020-04-08 14:32:50,249 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 124
2020-04-08 14:32:50,249 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 115
2020-04-08 14:32:50,249 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 34
2020-04-08 14:32:50,249 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 195
2020-04-08 14:32:50,249 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 196
2020-04-08 14:32:50,249 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 248
2020-04-08 14:32:50,249 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 10
2020-04-08 14:32:50,249 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 9
2020-04-08 14:32:50,249 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 142
2020-04-08 14:32:50,249 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 249
2020-04-08 14:32:50,249 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 181
2020-04-08 14:32:50,249 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 243
2020-04-08 14:32:50,249 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 282
2020-04-08 14:32:50,250 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 159
2020-04-08 14:32:50,250 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 293
2020-04-08 14:32:50,250 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 8
2020-04-08 14:32:50,250 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 252
2020-04-08 14:32:50,250 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 214
2020-04-08 14:32:50,250 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 52
2020-04-08 14:32:50,250 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 42
2020-04-08 14:32:50,250 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 32
2020-04-08 14:32:50,250 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 278
2020-04-08 14:32:50,250 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 333
2020-04-08 14:32:50,250 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 160
2020-04-08 14:32:50,250 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 331
2020-04-08 14:32:50,250 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 89
2020-04-08 14:32:50,250 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 228
2020-04-08 14:32:50,250 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 254
2020-04-08 14:32:50,250 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 280
2020-04-08 14:32:50,250 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 202
2020-04-08 14:32:50,251 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 58
2020-04-08 14:32:50,251 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 212
2020-04-08 14:32:50,251 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 339
2020-04-08 14:32:50,251 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 2
2020-04-08 14:32:50,251 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 84
2020-04-08 14:32:50,251 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 311
2020-04-08 14:32:50,251 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 170
2020-04-08 14:32:50,251 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 319
2020-04-08 14:32:50,251 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 25
2020-04-08 14:32:50,251 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 292
2020-04-08 14:32:50,251 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 308
2020-04-08 14:32:50,251 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 342
2020-04-08 14:32:50,253 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_11_piece0 on 192.168.0.102:50986 in memory (size: 3.3 KB, free: 4.1 GB)
2020-04-08 14:32:50,255 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 189
2020-04-08 14:32:50,255 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 224
2020-04-08 14:32:50,255 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 95
2020-04-08 14:32:50,255 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 303
2020-04-08 14:32:50,255 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 11
2020-04-08 14:32:50,255 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 101
2020-04-08 14:32:50,255 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 184
2020-04-08 14:32:50,255 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 164
2020-04-08 14:32:50,255 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 138
2020-04-08 14:32:50,255 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 260
2020-04-08 14:32:50,255 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 174
2020-04-08 14:32:50,256 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 191
2020-04-08 14:32:50,256 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 223
2020-04-08 14:32:50,256 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 213
2020-04-08 14:32:50,256 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 271
2020-04-08 14:32:50,256 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 216
2020-04-08 14:32:50,256 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 284
2020-04-08 14:32:50,256 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 5
2020-04-08 14:32:50,256 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 337
2020-04-08 14:32:50,256 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 94
2020-04-08 14:32:50,256 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 263
2020-04-08 14:32:50,256 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 332
2020-04-08 14:32:50,256 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 306
2020-04-08 14:32:50,256 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 150
2020-04-08 14:32:50,256 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 309
2020-04-08 14:32:50,256 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 18
2020-04-08 14:32:50,256 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 266
2020-04-08 14:32:50,258 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_8_piece0 on 192.168.0.102:50986 in memory (size: 3.3 KB, free: 4.1 GB)
2020-04-08 14:32:50,260 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 61
2020-04-08 14:32:50,260 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 205
2020-04-08 14:32:50,260 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 149
2020-04-08 14:32:50,260 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 73
2020-04-08 14:32:50,260 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 102
2020-04-08 14:32:50,260 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 179
2020-04-08 14:32:50,260 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 334
2020-04-08 14:32:50,260 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 109
2020-04-08 14:32:50,260 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 129
2020-04-08 14:32:50,260 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 79
2020-04-08 14:32:50,260 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 217
2020-04-08 14:32:50,260 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 240
2020-04-08 14:32:50,260 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 297
2020-04-08 14:32:50,260 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 347
2020-04-08 14:32:50,260 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 7
2020-04-08 14:32:50,260 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 46
2020-04-08 14:32:50,260 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 300
2020-04-08 14:32:50,260 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 75
2020-04-08 14:32:50,260 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 177
2020-04-08 14:32:50,260 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 265
2020-04-08 14:32:50,261 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 210
2020-04-08 14:32:50,261 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 316
2020-04-08 14:33:00,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 14:33:00,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 14:33:00,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 14:33:00,183 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 152.
2020-04-08 14:33:00,183 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 14:33:00,183 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 153.
2020-04-08 14:33:00,184 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586327580000 ms
2020-04-08 14:33:00,184 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586327580000 ms.0 from job set of time 1586327580000 ms
2020-04-08 14:33:00,189 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:134
2020-04-08 14:33:00,190 INFO org.apache.spark.scheduler.DAGScheduler: Got job 15 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134) with 3 output partitions
2020-04-08 14:33:00,190 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 15 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134)
2020-04-08 14:33:00,190 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 14:33:00,190 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 14:33:00,190 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 15 (KafkaRDD[15] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113), which has no missing parents
2020-04-08 14:33:00,192 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 14:33:00,193 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 14:33:00,194 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 192.168.0.102:50986 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 14:33:00,194 INFO org.apache.spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1161
2020-04-08 14:33:00,195 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 15 (KafkaRDD[15] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 14:33:00,195 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 15.0 with 3 tasks
2020-04-08 14:33:00,196 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 45, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:33:00,196 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 15.0 (TID 45)
2020-04-08 14:33:00,197 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 1
2020-04-08 14:33:00,197 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 15.0 (TID 45). 2785 bytes result sent to driver
2020-04-08 14:33:00,197 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 15.0 (TID 46, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:33:00,197 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 15.0 (TID 46)
2020-04-08 14:33:00,197 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 45) in 2 ms on localhost (executor driver) (1/3)
2020-04-08 14:33:00,198 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 152 is the same as ending offset skipping PABUSI2 2
2020-04-08 14:33:00,198 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 15.0 (TID 46). 2785 bytes result sent to driver
2020-04-08 14:33:00,200 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 15.0 (TID 47, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:33:00,200 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 15.0 (TID 46) in 3 ms on localhost (executor driver) (2/3)
2020-04-08 14:33:00,200 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 15.0 (TID 47)
2020-04-08 14:33:00,201 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 14:33:00,201 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 15.0 (TID 47). 2785 bytes result sent to driver
2020-04-08 14:33:00,201 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 15.0 (TID 47) in 1 ms on localhost (executor driver) (3/3)
2020-04-08 14:33:00,201 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
2020-04-08 14:33:00,202 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 15 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134) finished in 0.011 s
2020-04-08 14:33:00,202 INFO org.apache.spark.scheduler.DAGScheduler: Job 15 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:134, took 0.012648 s
2020-04-08 14:33:00,202 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586327580000 ms.0 from job set of time 1586327580000 ms
2020-04-08 14:33:00,202 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 14 from persistence list
2020-04-08 14:33:00,202 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.202 s for time 1586327580000 ms (execution: 0.018 s)
2020-04-08 14:33:00,203 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 14:33:00,203 INFO org.apache.spark.storage.BlockManager: Removing RDD 14
2020-04-08 14:33:00,203 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586327560000 ms
2020-04-08 14:33:10,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 14:33:10,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 14:33:10,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 14:33:10,187 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 14:33:10,188 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 153.
2020-04-08 14:33:10,188 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 152.
2020-04-08 14:33:10,188 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586327590000 ms
2020-04-08 14:33:10,189 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586327590000 ms.0 from job set of time 1586327590000 ms
2020-04-08 14:33:10,194 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:134
2020-04-08 14:33:10,194 INFO org.apache.spark.scheduler.DAGScheduler: Got job 16 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134) with 3 output partitions
2020-04-08 14:33:10,194 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 16 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134)
2020-04-08 14:33:10,194 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 14:33:10,194 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 14:33:10,195 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 16 (KafkaRDD[16] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113), which has no missing parents
2020-04-08 14:33:10,197 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 14:33:10,198 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 14:33:10,198 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 192.168.0.102:50986 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 14:33:10,198 INFO org.apache.spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1161
2020-04-08 14:33:10,199 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 16 (KafkaRDD[16] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 14:33:10,199 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 16.0 with 3 tasks
2020-04-08 14:33:10,199 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 16.0 (TID 48, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:33:10,199 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 16.0 (TID 48)
2020-04-08 14:33:10,200 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 1
2020-04-08 14:33:10,201 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 16.0 (TID 48). 2828 bytes result sent to driver
2020-04-08 14:33:10,201 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 16.0 (TID 49, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:33:10,201 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 16.0 (TID 49)
2020-04-08 14:33:10,201 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 16.0 (TID 48) in 2 ms on localhost (executor driver) (1/3)
2020-04-08 14:33:10,202 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 152 is the same as ending offset skipping PABUSI2 2
2020-04-08 14:33:10,202 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 16.0 (TID 49). 2785 bytes result sent to driver
2020-04-08 14:33:10,203 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 16.0 (TID 50, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:33:10,203 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 16.0 (TID 50)
2020-04-08 14:33:10,203 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 16.0 (TID 49) in 2 ms on localhost (executor driver) (2/3)
2020-04-08 14:33:10,204 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 14:33:10,204 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 16.0 (TID 50). 2785 bytes result sent to driver
2020-04-08 14:33:10,204 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 16.0 (TID 50) in 2 ms on localhost (executor driver) (3/3)
2020-04-08 14:33:10,204 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
2020-04-08 14:33:10,205 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 16 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134) finished in 0.009 s
2020-04-08 14:33:10,205 INFO org.apache.spark.scheduler.DAGScheduler: Job 16 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:134, took 0.010862 s
2020-04-08 14:33:10,205 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586327590000 ms.0 from job set of time 1586327590000 ms
2020-04-08 14:33:10,205 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.205 s for time 1586327590000 ms (execution: 0.016 s)
2020-04-08 14:33:10,205 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 15 from persistence list
2020-04-08 14:33:10,206 INFO org.apache.spark.storage.BlockManager: Removing RDD 15
2020-04-08 14:33:10,206 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 14:33:10,206 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586327570000 ms
2020-04-08 14:33:20,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 14:33:20,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 14:33:20,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 14:33:20,188 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 153.
2020-04-08 14:33:20,188 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 14:33:20,190 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 152.
2020-04-08 14:33:20,192 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586327600000 ms
2020-04-08 14:33:20,193 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586327600000 ms.0 from job set of time 1586327600000 ms
2020-04-08 14:33:20,205 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:134
2020-04-08 14:33:20,206 INFO org.apache.spark.scheduler.DAGScheduler: Got job 17 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134) with 3 output partitions
2020-04-08 14:33:20,206 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 17 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134)
2020-04-08 14:33:20,206 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 14:33:20,206 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 14:33:20,207 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 17 (KafkaRDD[17] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113), which has no missing parents
2020-04-08 14:33:20,211 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 14:33:20,212 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 14:33:20,213 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 192.168.0.102:50986 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 14:33:20,214 INFO org.apache.spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1161
2020-04-08 14:33:20,217 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 17 (KafkaRDD[17] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 14:33:20,217 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 17.0 with 3 tasks
2020-04-08 14:33:20,219 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 17.0 (TID 51, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:33:20,220 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 17.0 (TID 51)
2020-04-08 14:33:20,223 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 1
2020-04-08 14:33:20,224 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 17.0 (TID 51). 2828 bytes result sent to driver
2020-04-08 14:33:20,227 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 17.0 (TID 52, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:33:20,228 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 17.0 (TID 52)
2020-04-08 14:33:20,228 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 17.0 (TID 51) in 9 ms on localhost (executor driver) (1/3)
2020-04-08 14:33:20,230 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 152 is the same as ending offset skipping PABUSI2 2
2020-04-08 14:33:20,231 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 17.0 (TID 52). 2785 bytes result sent to driver
2020-04-08 14:33:20,232 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 17.0 (TID 53, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:33:20,233 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 17.0 (TID 53)
2020-04-08 14:33:20,233 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 17.0 (TID 52) in 7 ms on localhost (executor driver) (2/3)
2020-04-08 14:33:20,235 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 14:33:20,236 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 17.0 (TID 53). 2785 bytes result sent to driver
2020-04-08 14:33:20,238 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 17.0 (TID 53) in 7 ms on localhost (executor driver) (3/3)
2020-04-08 14:33:20,239 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool 
2020-04-08 14:33:20,240 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 17 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134) finished in 0.030 s
2020-04-08 14:33:20,241 INFO org.apache.spark.scheduler.DAGScheduler: Job 17 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:134, took 0.036050 s
2020-04-08 14:33:20,243 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586327600000 ms.0 from job set of time 1586327600000 ms
2020-04-08 14:33:20,244 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 16 from persistence list
2020-04-08 14:33:20,244 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.243 s for time 1586327600000 ms (execution: 0.050 s)
2020-04-08 14:33:20,246 INFO org.apache.spark.storage.BlockManager: Removing RDD 16
2020-04-08 14:33:20,246 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 14:33:20,246 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586327580000 ms
2020-04-08 14:33:30,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 14:33:30,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 14:33:30,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 14:33:30,178 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 153.
2020-04-08 14:33:30,178 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 14:33:30,179 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 152.
2020-04-08 14:33:30,179 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586327610000 ms
2020-04-08 14:33:30,179 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586327610000 ms.0 from job set of time 1586327610000 ms
2020-04-08 14:33:30,184 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:134
2020-04-08 14:33:30,185 INFO org.apache.spark.scheduler.DAGScheduler: Got job 18 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134) with 3 output partitions
2020-04-08 14:33:30,185 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 18 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134)
2020-04-08 14:33:30,185 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 14:33:30,185 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 14:33:30,185 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 18 (KafkaRDD[18] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113), which has no missing parents
2020-04-08 14:33:30,188 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 14:33:30,189 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 14:33:30,189 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 192.168.0.102:50986 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 14:33:30,190 INFO org.apache.spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1161
2020-04-08 14:33:30,191 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 18 (KafkaRDD[18] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 14:33:30,191 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 18.0 with 3 tasks
2020-04-08 14:33:30,192 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 54, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:33:30,192 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 18.0 (TID 54)
2020-04-08 14:33:30,193 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 1
2020-04-08 14:33:30,194 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 18.0 (TID 54). 2785 bytes result sent to driver
2020-04-08 14:33:30,194 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 18.0 (TID 55, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:33:30,194 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 54) in 2 ms on localhost (executor driver) (1/3)
2020-04-08 14:33:30,194 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 18.0 (TID 55)
2020-04-08 14:33:30,195 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 152 is the same as ending offset skipping PABUSI2 2
2020-04-08 14:33:30,195 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 18.0 (TID 55). 2785 bytes result sent to driver
2020-04-08 14:33:30,196 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 18.0 (TID 56, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:33:30,196 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 18.0 (TID 56)
2020-04-08 14:33:30,196 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 18.0 (TID 55) in 2 ms on localhost (executor driver) (2/3)
2020-04-08 14:33:30,197 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 14:33:30,197 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 18.0 (TID 56). 2785 bytes result sent to driver
2020-04-08 14:33:30,197 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 18.0 (TID 56) in 1 ms on localhost (executor driver) (3/3)
2020-04-08 14:33:30,197 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
2020-04-08 14:33:30,198 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 18 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:134) finished in 0.012 s
2020-04-08 14:33:30,198 INFO org.apache.spark.scheduler.DAGScheduler: Job 18 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:134, took 0.013744 s
2020-04-08 14:33:30,198 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586327610000 ms.0 from job set of time 1586327610000 ms
2020-04-08 14:33:30,198 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.198 s for time 1586327610000 ms (execution: 0.019 s)
2020-04-08 14:33:30,198 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 17 from persistence list
2020-04-08 14:33:30,199 INFO org.apache.spark.storage.BlockManager: Removing RDD 17
2020-04-08 14:33:30,199 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 14:33:30,199 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586327590000 ms
2020-04-08 14:33:30,593 INFO org.apache.spark.streaming.StreamingContext: Invoking stop(stopGracefully=false) from shutdown hook
2020-04-08 14:33:30,598 INFO org.apache.spark.streaming.scheduler.ReceiverTracker: ReceiverTracker stopped
2020-04-08 14:33:30,600 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopping JobGenerator immediately
2020-04-08 14:33:30,602 INFO org.apache.spark.streaming.util.RecurringTimer: Stopped timer for JobGenerator after time 1586327610000
2020-04-08 14:33:30,784 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Member consumer-1-fddc47fd-aee1-4ffe-a425-bff0f152aff7 sending LeaveGroup request to coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 14:33:30,968 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopped JobGenerator
2020-04-08 14:33:30,970 INFO org.apache.spark.streaming.scheduler.JobScheduler: Stopped JobScheduler
2020-04-08 14:33:30,974 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@499ef98e{/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 14:33:30,975 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@28501a4b{/streaming/batch,null,UNAVAILABLE,@Spark}
2020-04-08 14:33:30,975 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@2bebb74f{/static/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 14:33:30,976 INFO org.apache.spark.streaming.StreamingContext: StreamingContext stopped successfully
2020-04-08 14:33:30,976 INFO org.apache.spark.SparkContext: Invoking stop() from shutdown hook
2020-04-08 14:33:30,981 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@533b266e{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 14:33:30,982 INFO org.apache.spark.ui.SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
2020-04-08 14:33:30,988 INFO org.apache.spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2020-04-08 14:33:31,003 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2020-04-08 14:33:31,004 INFO org.apache.spark.storage.BlockManager: BlockManager stopped
2020-04-08 14:33:31,005 INFO org.apache.spark.storage.BlockManagerMaster: BlockManagerMaster stopped
2020-04-08 14:33:31,006 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2020-04-08 14:33:31,017 INFO org.apache.spark.SparkContext: Successfully stopped SparkContext
2020-04-08 14:33:31,017 INFO org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2020-04-08 14:33:31,018 INFO org.apache.spark.util.ShutdownHookManager: Deleting directory /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/spark-0fc39392-4061-4b84-bc3d-868307b3f98f
2020-04-08 14:33:37,820 WARN org.apache.spark.util.Utils: Your hostname, MacBook-Pro-CW.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)
2020-04-08 14:33:37,821 WARN org.apache.spark.util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
2020-04-08 14:33:37,849 INFO org.apache.spark.SparkContext: Running Spark version 2.4.0.cloudera2
2020-04-08 14:33:38,086 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-04-08 14:33:38,212 INFO org.apache.spark.SparkContext: Submitted application: Kafka2SparkStreaming2Kudu-kerberos
2020-04-08 14:33:38,262 INFO org.apache.spark.SecurityManager: Changing view acls to: godiswc
2020-04-08 14:33:38,263 INFO org.apache.spark.SecurityManager: Changing modify acls to: godiswc
2020-04-08 14:33:38,263 INFO org.apache.spark.SecurityManager: Changing view acls groups to: 
2020-04-08 14:33:38,264 INFO org.apache.spark.SecurityManager: Changing modify acls groups to: 
2020-04-08 14:33:38,264 INFO org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(godiswc); groups with view permissions: Set(); users  with modify permissions: Set(godiswc); groups with modify permissions: Set()
2020-04-08 14:33:38,389 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 14:33:38,465 INFO org.apache.spark.util.Utils: Successfully started service 'sparkDriver' on port 51737.
2020-04-08 14:33:38,481 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
2020-04-08 14:33:38,493 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
2020-04-08 14:33:38,514 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-04-08 14:33:38,514 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2020-04-08 14:33:38,526 INFO org.apache.spark.storage.DiskBlockManager: Created local directory at /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/blockmgr-c6bc84de-dd07-4c3d-af97-e4db4864033a
2020-04-08 14:33:38,540 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 4.1 GB
2020-04-08 14:33:38,548 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
2020-04-08 14:33:38,590 INFO org.spark_project.jetty.util.log: Logging initialized @1661ms
2020-04-08 14:33:38,642 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2019-04-30T04:42:08+08:00, git hash: e1bc35120a6617ee3df052294e433f3a25ce7097
2020-04-08 14:33:38,657 INFO org.spark_project.jetty.server.Server: Started @1728ms
2020-04-08 14:33:38,660 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 14:33:38,674 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@533b266e{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 14:33:38,674 INFO org.apache.spark.util.Utils: Successfully started service 'SparkUI' on port 4040.
2020-04-08 14:33:38,698 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6e521c1e{/jobs,null,AVAILABLE,@Spark}
2020-04-08 14:33:38,698 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62dae245{/jobs/json,null,AVAILABLE,@Spark}
2020-04-08 14:33:38,700 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4b6579e8{/jobs/job,null,AVAILABLE,@Spark}
2020-04-08 14:33:38,707 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@591e58fa{/jobs/job/json,null,AVAILABLE,@Spark}
2020-04-08 14:33:38,708 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3954d008{/stages,null,AVAILABLE,@Spark}
2020-04-08 14:33:38,708 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f94c4db{/stages/json,null,AVAILABLE,@Spark}
2020-04-08 14:33:38,708 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@593e824f{/stages/stage,null,AVAILABLE,@Spark}
2020-04-08 14:33:38,710 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@64bc21ac{/stages/stage/json,null,AVAILABLE,@Spark}
2020-04-08 14:33:38,710 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@493dfb8e{/stages/pool,null,AVAILABLE,@Spark}
2020-04-08 14:33:38,711 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5d25e6bb{/stages/pool/json,null,AVAILABLE,@Spark}
2020-04-08 14:33:38,712 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@ce5a68e{/storage,null,AVAILABLE,@Spark}
2020-04-08 14:33:38,712 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@9d157ff{/storage/json,null,AVAILABLE,@Spark}
2020-04-08 14:33:38,713 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f162cc0{/storage/rdd,null,AVAILABLE,@Spark}
2020-04-08 14:33:38,714 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5df417a7{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-04-08 14:33:38,714 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7c041b41{/environment,null,AVAILABLE,@Spark}
2020-04-08 14:33:38,715 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7f69d591{/environment/json,null,AVAILABLE,@Spark}
2020-04-08 14:33:38,716 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@61078690{/executors,null,AVAILABLE,@Spark}
2020-04-08 14:33:38,717 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1cb3ec38{/executors/json,null,AVAILABLE,@Spark}
2020-04-08 14:33:38,719 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@403132fc{/executors/threadDump,null,AVAILABLE,@Spark}
2020-04-08 14:33:38,720 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@71c5b236{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-04-08 14:33:38,726 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2cab9998{/static,null,AVAILABLE,@Spark}
2020-04-08 14:33:38,727 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@332a7fce{/,null,AVAILABLE,@Spark}
2020-04-08 14:33:38,728 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@549621f3{/api,null,AVAILABLE,@Spark}
2020-04-08 14:33:38,728 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@37ebc9d8{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-04-08 14:33:38,729 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@293bb8a5{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-04-08 14:33:38,730 INFO org.apache.spark.ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
2020-04-08 14:33:38,820 INFO org.apache.spark.executor.Executor: Starting executor ID driver on host localhost
2020-04-08 14:33:38,895 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 14:33:38,896 INFO org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51739.
2020-04-08 14:33:38,897 INFO org.apache.spark.network.netty.NettyBlockTransferService: Server created on 192.168.0.102:51739
2020-04-08 14:33:38,899 INFO org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-04-08 14:33:38,925 INFO org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 51739, None)
2020-04-08 14:33:38,927 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:51739 with 4.1 GB RAM, BlockManagerId(driver, 192.168.0.102, 51739, None)
2020-04-08 14:33:38,929 INFO org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 51739, None)
2020-04-08 14:33:38,929 INFO org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 51739, None)
2020-04-08 14:33:39,080 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@54acff7d{/metrics/json,null,AVAILABLE,@Spark}
2020-04-08 14:33:39,160 WARN org.apache.spark.streaming.StreamingContext: spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
2020-04-08 14:33:39,336 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding enable.auto.commit to false for executor
2020-04-08 14:33:39,337 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding auto.offset.reset to none for executor
2020-04-08 14:33:39,337 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding executor group.id to spark-executor-testgroup1
2020-04-08 14:33:39,337 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
2020-04-08 14:33:41,758 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Slide time = 10000 ms
2020-04-08 14:33:41,759 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Storage level = Serialized 1x Replicated
2020-04-08 14:33:41,760 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Checkpoint interval = null
2020-04-08 14:33:41,760 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Remember interval = 10000 ms
2020-04-08 14:33:41,761 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@3634366c
2020-04-08 14:33:41,761 INFO org.apache.spark.streaming.dstream.ForEachDStream: Slide time = 10000 ms
2020-04-08 14:33:41,761 INFO org.apache.spark.streaming.dstream.ForEachDStream: Storage level = Serialized 1x Replicated
2020-04-08 14:33:41,761 INFO org.apache.spark.streaming.dstream.ForEachDStream: Checkpoint interval = null
2020-04-08 14:33:41,761 INFO org.apache.spark.streaming.dstream.ForEachDStream: Remember interval = 10000 ms
2020-04-08 14:33:41,761 INFO org.apache.spark.streaming.dstream.ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@2f262747
2020-04-08 14:33:41,827 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = testgroup1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-08 14:33:41,899 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-08 14:33:41,900 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-08 14:33:41,900 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586327621898
2020-04-08 14:33:41,902 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-1, groupId=testgroup1] Subscribed to topic(s): PABUSI2
2020-04-08 14:33:42,732 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-1, groupId=testgroup1] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-08 14:33:42,734 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Discovered group coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 14:33:42,738 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Revoking previously assigned partitions []
2020-04-08 14:33:42,739 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 14:33:43,262 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 14:33:46,620 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Successfully joined group with generation 37
2020-04-08 14:33:46,626 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting newly assigned partitions: PABUSI2-2, PABUSI2-0, PABUSI2-1
2020-04-08 14:33:46,813 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-2 to the committed offset FetchPosition{offset=152, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-2.vpc.cloudera.com:9092 (id: 52 rack: null), epoch=0}}
2020-04-08 14:33:46,814 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-0 to the committed offset FetchPosition{offset=153, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-4.vpc.cloudera.com:9092 (id: 53 rack: null), epoch=2}}
2020-04-08 14:33:46,815 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-1 to the committed offset FetchPosition{offset=153, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-3.vpc.cloudera.com:9092 (id: 54 rack: null), epoch=2}}
2020-04-08 14:33:46,822 INFO org.apache.spark.streaming.util.RecurringTimer: Started timer for JobGenerator at time 1586327630000
2020-04-08 14:33:46,823 INFO org.apache.spark.streaming.scheduler.JobGenerator: Started JobGenerator at 1586327630000 ms
2020-04-08 14:33:46,824 INFO org.apache.spark.streaming.scheduler.JobScheduler: Started JobScheduler
2020-04-08 14:33:46,828 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@499ef98e{/streaming,null,AVAILABLE,@Spark}
2020-04-08 14:33:46,829 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@24934262{/streaming/json,null,AVAILABLE,@Spark}
2020-04-08 14:33:46,830 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@28501a4b{/streaming/batch,null,AVAILABLE,@Spark}
2020-04-08 14:33:46,830 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5b051a5c{/streaming/batch/json,null,AVAILABLE,@Spark}
2020-04-08 14:33:46,831 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2bebb74f{/static/streaming,null,AVAILABLE,@Spark}
2020-04-08 14:33:46,831 INFO org.apache.spark.streaming.StreamingContext: StreamingContext started
2020-04-08 14:33:50,044 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 14:33:50,044 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 14:33:50,045 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 14:33:50,230 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 14:33:50,230 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 154.
2020-04-08 14:33:50,230 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 152.
2020-04-08 14:33:50,260 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586327630000 ms
2020-04-08 14:33:50,264 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586327630000 ms.0 from job set of time 1586327630000 ms
2020-04-08 14:33:50,313 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:136
2020-04-08 14:33:50,338 INFO org.apache.spark.scheduler.DAGScheduler: Got job 0 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136) with 3 output partitions
2020-04-08 14:33:50,338 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 0 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136)
2020-04-08 14:33:50,339 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 14:33:50,340 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 14:33:50,347 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 0 (KafkaRDD[0] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113), which has no missing parents
2020-04-08 14:33:50,420 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 14:33:50,848 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 14:33:50,850 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:51739 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 14:33:50,853 INFO org.apache.spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2020-04-08 14:33:50,869 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 0 (KafkaRDD[0] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 14:33:50,869 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 0.0 with 3 tasks
2020-04-08 14:33:50,901 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:33:50,908 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
2020-04-08 14:33:50,931 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Computing topic PABUSI2, partition 1 offsets 153 -> 154
2020-04-08 14:33:50,934 INFO org.apache.spark.streaming.kafka010.KafkaDataConsumer: Initializing cache 16 64 0.75
2020-04-08 14:33:50,937 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-testgroup1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-08 14:33:50,942 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-08 14:33:50,942 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-08 14:33:50,942 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586327630942
2020-04-08 14:33:50,943 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Subscribed to partition(s): PABUSI2-1
2020-04-08 14:33:50,946 INFO org.apache.spark.streaming.kafka010.InternalKafkaConsumer: Initial fetch for spark-executor-testgroup1 PABUSI2-1 153
2020-04-08 14:33:50,946 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Seeking to offset 153 for partition PABUSI2-1
2020-04-08 14:33:51,482 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-08 14:33:53,313 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 2871 bytes result sent to driver
2020-04-08 14:33:53,315 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:33:53,316 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 0.0 (TID 1)
2020-04-08 14:33:53,318 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2425 ms on localhost (executor driver) (1/3)
2020-04-08 14:33:53,320 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 152 is the same as ending offset skipping PABUSI2 2
2020-04-08 14:33:53,321 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 0.0 (TID 1). 2828 bytes result sent to driver
2020-04-08 14:33:53,322 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:33:53,322 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 0.0 (TID 2)
2020-04-08 14:33:53,322 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 7 ms on localhost (executor driver) (2/3)
2020-04-08 14:33:53,325 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 14:33:53,326 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 0.0 (TID 2). 2828 bytes result sent to driver
2020-04-08 14:33:53,328 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 7 ms on localhost (executor driver) (3/3)
2020-04-08 14:33:53,329 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-04-08 14:33:53,330 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 0 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136) finished in 2.962 s
2020-04-08 14:33:53,334 INFO org.apache.spark.scheduler.DAGScheduler: Job 0 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:136, took 3.020725 s
2020-04-08 14:33:53,337 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586327630000 ms.0 from job set of time 1586327630000 ms
2020-04-08 14:33:53,338 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 3.336 s for time 1586327630000 ms (execution: 3.073 s)
2020-04-08 14:33:53,342 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 14:33:53,345 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 
2020-04-08 14:33:59,474 INFO org.apache.spark.streaming.StreamingContext: Invoking stop(stopGracefully=false) from shutdown hook
2020-04-08 14:33:59,475 INFO org.apache.spark.streaming.scheduler.ReceiverTracker: ReceiverTracker stopped
2020-04-08 14:33:59,476 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopping JobGenerator immediately
2020-04-08 14:33:59,477 INFO org.apache.spark.streaming.util.RecurringTimer: Stopped timer for JobGenerator after time 1586327630000
2020-04-08 14:33:59,655 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Member consumer-1-e6be1e44-ea48-4325-b37e-b1951c1a48f0 sending LeaveGroup request to coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 14:33:59,837 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopped JobGenerator
2020-04-08 14:33:59,840 INFO org.apache.spark.streaming.scheduler.JobScheduler: Stopped JobScheduler
2020-04-08 14:33:59,843 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@499ef98e{/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 14:33:59,844 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@28501a4b{/streaming/batch,null,UNAVAILABLE,@Spark}
2020-04-08 14:33:59,845 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@2bebb74f{/static/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 14:33:59,845 INFO org.apache.spark.streaming.StreamingContext: StreamingContext stopped successfully
2020-04-08 14:33:59,845 INFO org.apache.spark.SparkContext: Invoking stop() from shutdown hook
2020-04-08 14:33:59,850 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@533b266e{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 14:33:59,852 INFO org.apache.spark.ui.SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
2020-04-08 14:33:59,858 INFO org.apache.spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2020-04-08 14:33:59,869 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2020-04-08 14:33:59,869 INFO org.apache.spark.storage.BlockManager: BlockManager stopped
2020-04-08 14:33:59,869 INFO org.apache.spark.storage.BlockManagerMaster: BlockManagerMaster stopped
2020-04-08 14:33:59,871 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2020-04-08 14:33:59,878 INFO org.apache.spark.SparkContext: Successfully stopped SparkContext
2020-04-08 14:33:59,879 INFO org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2020-04-08 14:33:59,879 INFO org.apache.spark.util.ShutdownHookManager: Deleting directory /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/spark-147c9509-910f-40f0-987b-b19a7da6b2b7
2020-04-08 14:35:29,907 WARN org.apache.spark.util.Utils: Your hostname, MacBook-Pro-CW.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)
2020-04-08 14:35:29,908 WARN org.apache.spark.util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
2020-04-08 14:35:29,937 INFO org.apache.spark.SparkContext: Running Spark version 2.4.0.cloudera2
2020-04-08 14:35:30,153 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-04-08 14:35:30,264 INFO org.apache.spark.SparkContext: Submitted application: Kafka2SparkStreaming2Kudu-kerberos
2020-04-08 14:35:30,313 INFO org.apache.spark.SecurityManager: Changing view acls to: godiswc
2020-04-08 14:35:30,314 INFO org.apache.spark.SecurityManager: Changing modify acls to: godiswc
2020-04-08 14:35:30,314 INFO org.apache.spark.SecurityManager: Changing view acls groups to: 
2020-04-08 14:35:30,315 INFO org.apache.spark.SecurityManager: Changing modify acls groups to: 
2020-04-08 14:35:30,315 INFO org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(godiswc); groups with view permissions: Set(); users  with modify permissions: Set(godiswc); groups with modify permissions: Set()
2020-04-08 14:35:30,458 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 14:35:30,530 INFO org.apache.spark.util.Utils: Successfully started service 'sparkDriver' on port 52170.
2020-04-08 14:35:30,545 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
2020-04-08 14:35:30,556 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
2020-04-08 14:35:30,575 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-04-08 14:35:30,575 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2020-04-08 14:35:30,586 INFO org.apache.spark.storage.DiskBlockManager: Created local directory at /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/blockmgr-32329894-80d0-4a22-bf99-1c443a490460
2020-04-08 14:35:30,598 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 4.1 GB
2020-04-08 14:35:30,607 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
2020-04-08 14:35:30,654 INFO org.spark_project.jetty.util.log: Logging initialized @1416ms
2020-04-08 14:35:30,721 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2019-04-30T04:42:08+08:00, git hash: e1bc35120a6617ee3df052294e433f3a25ce7097
2020-04-08 14:35:30,737 INFO org.spark_project.jetty.server.Server: Started @1499ms
2020-04-08 14:35:30,740 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 14:35:30,756 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@533b266e{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 14:35:30,756 INFO org.apache.spark.util.Utils: Successfully started service 'SparkUI' on port 4040.
2020-04-08 14:35:30,784 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6e521c1e{/jobs,null,AVAILABLE,@Spark}
2020-04-08 14:35:30,787 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62dae245{/jobs/json,null,AVAILABLE,@Spark}
2020-04-08 14:35:30,788 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4b6579e8{/jobs/job,null,AVAILABLE,@Spark}
2020-04-08 14:35:30,792 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@591e58fa{/jobs/job/json,null,AVAILABLE,@Spark}
2020-04-08 14:35:30,793 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3954d008{/stages,null,AVAILABLE,@Spark}
2020-04-08 14:35:30,794 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f94c4db{/stages/json,null,AVAILABLE,@Spark}
2020-04-08 14:35:30,795 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@593e824f{/stages/stage,null,AVAILABLE,@Spark}
2020-04-08 14:35:30,797 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@64bc21ac{/stages/stage/json,null,AVAILABLE,@Spark}
2020-04-08 14:35:30,799 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@493dfb8e{/stages/pool,null,AVAILABLE,@Spark}
2020-04-08 14:35:30,799 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5d25e6bb{/stages/pool/json,null,AVAILABLE,@Spark}
2020-04-08 14:35:30,800 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@ce5a68e{/storage,null,AVAILABLE,@Spark}
2020-04-08 14:35:30,800 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@9d157ff{/storage/json,null,AVAILABLE,@Spark}
2020-04-08 14:35:30,801 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f162cc0{/storage/rdd,null,AVAILABLE,@Spark}
2020-04-08 14:35:30,801 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5df417a7{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-04-08 14:35:30,802 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7c041b41{/environment,null,AVAILABLE,@Spark}
2020-04-08 14:35:30,803 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7f69d591{/environment/json,null,AVAILABLE,@Spark}
2020-04-08 14:35:30,804 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@61078690{/executors,null,AVAILABLE,@Spark}
2020-04-08 14:35:30,804 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1cb3ec38{/executors/json,null,AVAILABLE,@Spark}
2020-04-08 14:35:30,805 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@403132fc{/executors/threadDump,null,AVAILABLE,@Spark}
2020-04-08 14:35:30,806 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@71c5b236{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-04-08 14:35:30,811 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2cab9998{/static,null,AVAILABLE,@Spark}
2020-04-08 14:35:30,811 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@332a7fce{/,null,AVAILABLE,@Spark}
2020-04-08 14:35:30,812 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@549621f3{/api,null,AVAILABLE,@Spark}
2020-04-08 14:35:30,813 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@37ebc9d8{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-04-08 14:35:30,813 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@293bb8a5{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-04-08 14:35:30,815 INFO org.apache.spark.ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
2020-04-08 14:35:30,910 INFO org.apache.spark.executor.Executor: Starting executor ID driver on host localhost
2020-04-08 14:35:30,980 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 14:35:30,981 INFO org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52171.
2020-04-08 14:35:30,982 INFO org.apache.spark.network.netty.NettyBlockTransferService: Server created on 192.168.0.102:52171
2020-04-08 14:35:30,984 INFO org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-04-08 14:35:31,025 INFO org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 52171, None)
2020-04-08 14:35:31,030 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:52171 with 4.1 GB RAM, BlockManagerId(driver, 192.168.0.102, 52171, None)
2020-04-08 14:35:31,033 INFO org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 52171, None)
2020-04-08 14:35:31,033 INFO org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 52171, None)
2020-04-08 14:35:31,170 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@54acff7d{/metrics/json,null,AVAILABLE,@Spark}
2020-04-08 14:35:31,269 WARN org.apache.spark.streaming.StreamingContext: spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
2020-04-08 14:35:31,442 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding enable.auto.commit to false for executor
2020-04-08 14:35:31,442 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding auto.offset.reset to none for executor
2020-04-08 14:35:31,443 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding executor group.id to spark-executor-testgroup1
2020-04-08 14:35:31,443 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
2020-04-08 14:35:33,207 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Slide time = 10000 ms
2020-04-08 14:35:33,208 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Storage level = Serialized 1x Replicated
2020-04-08 14:35:33,208 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Checkpoint interval = null
2020-04-08 14:35:33,208 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Remember interval = 10000 ms
2020-04-08 14:35:33,209 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@c5f8a3e
2020-04-08 14:35:33,209 INFO org.apache.spark.streaming.dstream.ForEachDStream: Slide time = 10000 ms
2020-04-08 14:35:33,209 INFO org.apache.spark.streaming.dstream.ForEachDStream: Storage level = Serialized 1x Replicated
2020-04-08 14:35:33,209 INFO org.apache.spark.streaming.dstream.ForEachDStream: Checkpoint interval = null
2020-04-08 14:35:33,209 INFO org.apache.spark.streaming.dstream.ForEachDStream: Remember interval = 10000 ms
2020-04-08 14:35:33,209 INFO org.apache.spark.streaming.dstream.ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@65de509c
2020-04-08 14:35:33,264 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = testgroup1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-08 14:35:33,317 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-08 14:35:33,317 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-08 14:35:33,317 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586327733316
2020-04-08 14:35:33,319 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-1, groupId=testgroup1] Subscribed to topic(s): PABUSI2
2020-04-08 14:35:34,019 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-1, groupId=testgroup1] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-08 14:35:34,020 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Discovered group coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 14:35:34,023 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Revoking previously assigned partitions []
2020-04-08 14:35:34,023 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 14:35:34,549 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 14:35:38,000 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Successfully joined group with generation 39
2020-04-08 14:35:38,004 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting newly assigned partitions: PABUSI2-2, PABUSI2-0, PABUSI2-1
2020-04-08 14:35:38,192 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-2 to the committed offset FetchPosition{offset=152, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-2.vpc.cloudera.com:9092 (id: 52 rack: null), epoch=0}}
2020-04-08 14:35:38,194 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-0 to the committed offset FetchPosition{offset=153, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-4.vpc.cloudera.com:9092 (id: 53 rack: null), epoch=2}}
2020-04-08 14:35:38,194 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-1 to the committed offset FetchPosition{offset=154, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-3.vpc.cloudera.com:9092 (id: 54 rack: null), epoch=2}}
2020-04-08 14:35:38,204 INFO org.apache.spark.streaming.util.RecurringTimer: Started timer for JobGenerator at time 1586327740000
2020-04-08 14:35:38,205 INFO org.apache.spark.streaming.scheduler.JobGenerator: Started JobGenerator at 1586327740000 ms
2020-04-08 14:35:38,206 INFO org.apache.spark.streaming.scheduler.JobScheduler: Started JobScheduler
2020-04-08 14:35:38,210 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@499ef98e{/streaming,null,AVAILABLE,@Spark}
2020-04-08 14:35:38,212 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@24934262{/streaming/json,null,AVAILABLE,@Spark}
2020-04-08 14:35:38,214 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@28501a4b{/streaming/batch,null,AVAILABLE,@Spark}
2020-04-08 14:35:38,215 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5b051a5c{/streaming/batch/json,null,AVAILABLE,@Spark}
2020-04-08 14:35:38,216 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2bebb74f{/static/streaming,null,AVAILABLE,@Spark}
2020-04-08 14:35:38,217 INFO org.apache.spark.streaming.StreamingContext: StreamingContext started
2020-04-08 14:35:40,044 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 14:35:40,045 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 14:35:40,045 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 14:35:40,228 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 152.
2020-04-08 14:35:40,229 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 154.
2020-04-08 14:35:40,229 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 14:35:40,254 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586327740000 ms
2020-04-08 14:35:40,256 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586327740000 ms.0 from job set of time 1586327740000 ms
2020-04-08 14:35:40,289 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:136
2020-04-08 14:35:40,305 INFO org.apache.spark.scheduler.DAGScheduler: Got job 0 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136) with 3 output partitions
2020-04-08 14:35:40,305 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 0 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136)
2020-04-08 14:35:40,306 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 14:35:40,307 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 14:35:40,310 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 0 (KafkaRDD[0] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113), which has no missing parents
2020-04-08 14:35:40,371 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 14:35:40,749 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 14:35:40,750 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:52171 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 14:35:40,751 INFO org.apache.spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2020-04-08 14:35:40,760 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 0 (KafkaRDD[0] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 14:35:40,760 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 0.0 with 3 tasks
2020-04-08 14:35:40,785 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:35:40,791 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
2020-04-08 14:35:40,811 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 154 is the same as ending offset skipping PABUSI2 1
2020-04-08 14:35:40,820 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 2828 bytes result sent to driver
2020-04-08 14:35:40,822 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:35:40,822 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 0.0 (TID 1)
2020-04-08 14:35:40,825 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 47 ms on localhost (executor driver) (1/3)
2020-04-08 14:35:40,826 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 152 is the same as ending offset skipping PABUSI2 2
2020-04-08 14:35:40,827 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 0.0 (TID 1). 2828 bytes result sent to driver
2020-04-08 14:35:40,828 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:35:40,828 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 0.0 (TID 2)
2020-04-08 14:35:40,828 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 7 ms on localhost (executor driver) (2/3)
2020-04-08 14:35:40,830 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 14:35:40,831 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 0.0 (TID 2). 2828 bytes result sent to driver
2020-04-08 14:35:40,834 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 6 ms on localhost (executor driver) (3/3)
2020-04-08 14:35:40,835 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-04-08 14:35:40,835 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 0 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136) finished in 0.510 s
2020-04-08 14:35:40,839 INFO org.apache.spark.scheduler.DAGScheduler: Job 0 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:136, took 0.549583 s
2020-04-08 14:35:40,842 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586327740000 ms.0 from job set of time 1586327740000 ms
2020-04-08 14:35:40,843 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.841 s for time 1586327740000 ms (execution: 0.586 s)
2020-04-08 14:35:40,847 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 14:35:40,851 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 
2020-04-08 14:35:50,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 14:35:50,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 14:35:50,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 14:35:50,183 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 153.
2020-04-08 14:35:50,183 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 154.
2020-04-08 14:35:50,184 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 14:35:50,185 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586327750000 ms
2020-04-08 14:35:50,186 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586327750000 ms.0 from job set of time 1586327750000 ms
2020-04-08 14:35:50,193 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:136
2020-04-08 14:35:50,193 INFO org.apache.spark.scheduler.DAGScheduler: Got job 1 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136) with 3 output partitions
2020-04-08 14:35:50,193 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 1 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136)
2020-04-08 14:35:50,194 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 14:35:50,194 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 14:35:50,194 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 1 (KafkaRDD[1] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113), which has no missing parents
2020-04-08 14:35:50,197 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 14:35:50,200 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 14:35:50,200 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.102:52171 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 14:35:50,201 INFO org.apache.spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2020-04-08 14:35:50,202 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 1 (KafkaRDD[1] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 14:35:50,202 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 1.0 with 3 tasks
2020-04-08 14:35:50,203 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:35:50,203 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 1.0 (TID 3)
2020-04-08 14:35:50,205 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 154 is the same as ending offset skipping PABUSI2 1
2020-04-08 14:35:50,206 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 1.0 (TID 3). 2828 bytes result sent to driver
2020-04-08 14:35:50,207 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 4, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:35:50,207 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 1.0 (TID 4)
2020-04-08 14:35:50,207 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 3) in 5 ms on localhost (executor driver) (1/3)
2020-04-08 14:35:50,210 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Computing topic PABUSI2, partition 2 offsets 152 -> 153
2020-04-08 14:35:50,213 INFO org.apache.spark.streaming.kafka010.KafkaDataConsumer: Initializing cache 16 64 0.75
2020-04-08 14:35:50,217 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-testgroup1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-08 14:35:50,223 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-08 14:35:50,223 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-08 14:35:50,223 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586327750223
2020-04-08 14:35:50,223 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Subscribed to partition(s): PABUSI2-2
2020-04-08 14:35:50,240 INFO org.apache.spark.streaming.kafka010.InternalKafkaConsumer: Initial fetch for spark-executor-testgroup1 PABUSI2-2 152
2020-04-08 14:35:50,240 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Seeking to offset 152 for partition PABUSI2-2
2020-04-08 14:35:50,241 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 17
2020-04-08 14:35:50,241 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 6
2020-04-08 14:35:50,241 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 12
2020-04-08 14:35:50,242 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 7
2020-04-08 14:35:50,242 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 21
2020-04-08 14:35:50,242 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 11
2020-04-08 14:35:50,242 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 9
2020-04-08 14:35:50,242 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 24
2020-04-08 14:35:50,242 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 3
2020-04-08 14:35:50,242 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 14
2020-04-08 14:35:50,242 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 22
2020-04-08 14:35:50,242 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 10
2020-04-08 14:35:50,242 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 18
2020-04-08 14:35:50,242 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 15
2020-04-08 14:35:50,242 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 20
2020-04-08 14:35:50,242 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 1
2020-04-08 14:35:50,243 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 8
2020-04-08 14:35:50,243 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 2
2020-04-08 14:35:50,243 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 25
2020-04-08 14:35:50,243 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 4
2020-04-08 14:35:50,243 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 19
2020-04-08 14:35:50,243 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 23
2020-04-08 14:35:50,251 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.102:52171 in memory (size: 3.3 KB, free: 4.1 GB)
2020-04-08 14:35:50,254 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 13
2020-04-08 14:35:50,254 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 5
2020-04-08 14:35:50,254 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 16
2020-04-08 14:35:50,769 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-08 14:35:52,728 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 1.0 (TID 4). 2871 bytes result sent to driver
2020-04-08 14:35:52,729 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 1.0 (TID 5, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:35:52,729 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 1.0 (TID 5)
2020-04-08 14:35:52,729 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 4) in 2522 ms on localhost (executor driver) (2/3)
2020-04-08 14:35:52,732 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 14:35:52,733 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 1.0 (TID 5). 2828 bytes result sent to driver
2020-04-08 14:35:52,733 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 1.0 (TID 5) in 4 ms on localhost (executor driver) (3/3)
2020-04-08 14:35:52,734 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
2020-04-08 14:35:52,734 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 1 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136) finished in 2.539 s
2020-04-08 14:35:52,734 INFO org.apache.spark.scheduler.DAGScheduler: Job 1 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:136, took 2.541518 s
2020-04-08 14:35:52,735 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586327750000 ms.0 from job set of time 1586327750000 ms
2020-04-08 14:35:52,735 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 2.735 s for time 1586327750000 ms (execution: 2.549 s)
2020-04-08 14:35:52,735 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 0 from persistence list
2020-04-08 14:35:52,738 INFO org.apache.spark.storage.BlockManager: Removing RDD 0
2020-04-08 14:35:52,738 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 14:35:52,738 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 
2020-04-08 14:35:56,874 INFO org.apache.spark.streaming.StreamingContext: Invoking stop(stopGracefully=false) from shutdown hook
2020-04-08 14:35:56,876 INFO org.apache.spark.streaming.scheduler.ReceiverTracker: ReceiverTracker stopped
2020-04-08 14:35:56,877 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopping JobGenerator immediately
2020-04-08 14:35:56,878 INFO org.apache.spark.streaming.util.RecurringTimer: Stopped timer for JobGenerator after time 1586327750000
2020-04-08 14:35:57,056 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Member consumer-1-d4fbd814-68a4-4f0a-9118-d456de4849fe sending LeaveGroup request to coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 14:35:57,238 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopped JobGenerator
2020-04-08 14:35:57,241 INFO org.apache.spark.streaming.scheduler.JobScheduler: Stopped JobScheduler
2020-04-08 14:35:57,245 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@499ef98e{/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 14:35:57,246 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@28501a4b{/streaming/batch,null,UNAVAILABLE,@Spark}
2020-04-08 14:35:57,246 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@2bebb74f{/static/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 14:35:57,247 INFO org.apache.spark.streaming.StreamingContext: StreamingContext stopped successfully
2020-04-08 14:35:57,248 INFO org.apache.spark.SparkContext: Invoking stop() from shutdown hook
2020-04-08 14:35:57,253 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@533b266e{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 14:35:57,254 INFO org.apache.spark.ui.SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
2020-04-08 14:35:57,262 INFO org.apache.spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2020-04-08 14:35:57,276 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2020-04-08 14:35:57,276 INFO org.apache.spark.storage.BlockManager: BlockManager stopped
2020-04-08 14:35:57,277 INFO org.apache.spark.storage.BlockManagerMaster: BlockManagerMaster stopped
2020-04-08 14:35:57,280 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2020-04-08 14:35:57,288 INFO org.apache.spark.SparkContext: Successfully stopped SparkContext
2020-04-08 14:35:57,288 INFO org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2020-04-08 14:35:57,288 INFO org.apache.spark.util.ShutdownHookManager: Deleting directory /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/spark-3f8be5cd-78cf-43a9-9e2b-504de286407d
2020-04-08 14:40:55,780 WARN org.apache.spark.util.Utils: Your hostname, MacBook-Pro-CW.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)
2020-04-08 14:40:55,781 WARN org.apache.spark.util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
2020-04-08 14:40:55,814 INFO org.apache.spark.SparkContext: Running Spark version 2.4.0.cloudera2
2020-04-08 14:40:56,003 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-04-08 14:40:56,154 INFO org.apache.spark.SparkContext: Submitted application: Kafka2SparkStreaming2Kudu-kerberos
2020-04-08 14:40:56,204 INFO org.apache.spark.SecurityManager: Changing view acls to: godiswc
2020-04-08 14:40:56,204 INFO org.apache.spark.SecurityManager: Changing modify acls to: godiswc
2020-04-08 14:40:56,204 INFO org.apache.spark.SecurityManager: Changing view acls groups to: 
2020-04-08 14:40:56,205 INFO org.apache.spark.SecurityManager: Changing modify acls groups to: 
2020-04-08 14:40:56,205 INFO org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(godiswc); groups with view permissions: Set(); users  with modify permissions: Set(godiswc); groups with modify permissions: Set()
2020-04-08 14:40:56,330 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 14:40:56,400 INFO org.apache.spark.util.Utils: Successfully started service 'sparkDriver' on port 53414.
2020-04-08 14:40:56,416 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
2020-04-08 14:40:56,431 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
2020-04-08 14:40:56,455 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-04-08 14:40:56,455 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2020-04-08 14:40:56,468 INFO org.apache.spark.storage.DiskBlockManager: Created local directory at /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/blockmgr-f885c403-8451-4190-8589-4ec31799ba8b
2020-04-08 14:40:56,482 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 4.1 GB
2020-04-08 14:40:56,491 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
2020-04-08 14:40:56,546 INFO org.spark_project.jetty.util.log: Logging initialized @1541ms
2020-04-08 14:40:56,585 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2019-04-30T04:42:08+08:00, git hash: e1bc35120a6617ee3df052294e433f3a25ce7097
2020-04-08 14:40:56,594 INFO org.spark_project.jetty.server.Server: Started @1589ms
2020-04-08 14:40:56,595 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 14:40:56,604 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@533b266e{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 14:40:56,604 INFO org.apache.spark.util.Utils: Successfully started service 'SparkUI' on port 4040.
2020-04-08 14:40:56,622 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6e521c1e{/jobs,null,AVAILABLE,@Spark}
2020-04-08 14:40:56,625 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62dae245{/jobs/json,null,AVAILABLE,@Spark}
2020-04-08 14:40:56,625 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4b6579e8{/jobs/job,null,AVAILABLE,@Spark}
2020-04-08 14:40:56,628 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@591e58fa{/jobs/job/json,null,AVAILABLE,@Spark}
2020-04-08 14:40:56,629 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3954d008{/stages,null,AVAILABLE,@Spark}
2020-04-08 14:40:56,630 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f94c4db{/stages/json,null,AVAILABLE,@Spark}
2020-04-08 14:40:56,631 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@593e824f{/stages/stage,null,AVAILABLE,@Spark}
2020-04-08 14:40:56,632 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@64bc21ac{/stages/stage/json,null,AVAILABLE,@Spark}
2020-04-08 14:40:56,633 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@493dfb8e{/stages/pool,null,AVAILABLE,@Spark}
2020-04-08 14:40:56,633 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5d25e6bb{/stages/pool/json,null,AVAILABLE,@Spark}
2020-04-08 14:40:56,634 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@ce5a68e{/storage,null,AVAILABLE,@Spark}
2020-04-08 14:40:56,634 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@9d157ff{/storage/json,null,AVAILABLE,@Spark}
2020-04-08 14:40:56,635 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f162cc0{/storage/rdd,null,AVAILABLE,@Spark}
2020-04-08 14:40:56,635 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5df417a7{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-04-08 14:40:56,636 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7c041b41{/environment,null,AVAILABLE,@Spark}
2020-04-08 14:40:56,637 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7f69d591{/environment/json,null,AVAILABLE,@Spark}
2020-04-08 14:40:56,638 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@61078690{/executors,null,AVAILABLE,@Spark}
2020-04-08 14:40:56,638 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1cb3ec38{/executors/json,null,AVAILABLE,@Spark}
2020-04-08 14:40:56,639 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@403132fc{/executors/threadDump,null,AVAILABLE,@Spark}
2020-04-08 14:40:56,640 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@71c5b236{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-04-08 14:40:56,647 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2cab9998{/static,null,AVAILABLE,@Spark}
2020-04-08 14:40:56,648 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@332a7fce{/,null,AVAILABLE,@Spark}
2020-04-08 14:40:56,649 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@549621f3{/api,null,AVAILABLE,@Spark}
2020-04-08 14:40:56,650 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@37ebc9d8{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-04-08 14:40:56,650 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@293bb8a5{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-04-08 14:40:56,652 INFO org.apache.spark.ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
2020-04-08 14:40:56,774 INFO org.apache.spark.executor.Executor: Starting executor ID driver on host localhost
2020-04-08 14:40:56,829 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 14:40:56,830 INFO org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53418.
2020-04-08 14:40:56,831 INFO org.apache.spark.network.netty.NettyBlockTransferService: Server created on 192.168.0.102:53418
2020-04-08 14:40:56,834 INFO org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-04-08 14:40:56,861 INFO org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 53418, None)
2020-04-08 14:40:56,864 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:53418 with 4.1 GB RAM, BlockManagerId(driver, 192.168.0.102, 53418, None)
2020-04-08 14:40:56,866 INFO org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 53418, None)
2020-04-08 14:40:56,866 INFO org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 53418, None)
2020-04-08 14:40:57,034 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@54acff7d{/metrics/json,null,AVAILABLE,@Spark}
2020-04-08 14:40:57,145 WARN org.apache.spark.streaming.StreamingContext: spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
2020-04-08 14:40:57,318 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding enable.auto.commit to false for executor
2020-04-08 14:40:57,318 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding auto.offset.reset to none for executor
2020-04-08 14:40:57,319 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding executor group.id to spark-executor-testgroup1
2020-04-08 14:40:57,319 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
2020-04-08 14:40:59,049 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Slide time = 10000 ms
2020-04-08 14:40:59,049 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Storage level = Serialized 1x Replicated
2020-04-08 14:40:59,050 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Checkpoint interval = null
2020-04-08 14:40:59,050 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Remember interval = 10000 ms
2020-04-08 14:40:59,050 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@6207f8b6
2020-04-08 14:40:59,050 INFO org.apache.spark.streaming.dstream.ForEachDStream: Slide time = 10000 ms
2020-04-08 14:40:59,050 INFO org.apache.spark.streaming.dstream.ForEachDStream: Storage level = Serialized 1x Replicated
2020-04-08 14:40:59,051 INFO org.apache.spark.streaming.dstream.ForEachDStream: Checkpoint interval = null
2020-04-08 14:40:59,051 INFO org.apache.spark.streaming.dstream.ForEachDStream: Remember interval = 10000 ms
2020-04-08 14:40:59,051 INFO org.apache.spark.streaming.dstream.ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@28a89d0b
2020-04-08 14:40:59,121 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = testgroup1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-08 14:40:59,210 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-08 14:40:59,210 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-08 14:40:59,211 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586328059209
2020-04-08 14:40:59,212 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-1, groupId=testgroup1] Subscribed to topic(s): PABUSI2
2020-04-08 14:40:59,891 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-1, groupId=testgroup1] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-08 14:40:59,892 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Discovered group coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 14:40:59,895 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Revoking previously assigned partitions []
2020-04-08 14:40:59,895 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 14:41:00,436 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 14:41:03,861 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Successfully joined group with generation 41
2020-04-08 14:41:03,865 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting newly assigned partitions: PABUSI2-2, PABUSI2-0, PABUSI2-1
2020-04-08 14:41:04,051 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-2 to the committed offset FetchPosition{offset=153, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-2.vpc.cloudera.com:9092 (id: 52 rack: null), epoch=0}}
2020-04-08 14:41:04,053 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-0 to the committed offset FetchPosition{offset=153, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-4.vpc.cloudera.com:9092 (id: 53 rack: null), epoch=2}}
2020-04-08 14:41:04,053 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-1 to the committed offset FetchPosition{offset=154, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-3.vpc.cloudera.com:9092 (id: 54 rack: null), epoch=2}}
2020-04-08 14:41:04,061 INFO org.apache.spark.streaming.util.RecurringTimer: Started timer for JobGenerator at time 1586328060000
2020-04-08 14:41:04,062 INFO org.apache.spark.streaming.scheduler.JobGenerator: Started JobGenerator at 1586328060000 ms
2020-04-08 14:41:04,062 INFO org.apache.spark.streaming.scheduler.JobScheduler: Started JobScheduler
2020-04-08 14:41:04,067 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@499ef98e{/streaming,null,AVAILABLE,@Spark}
2020-04-08 14:41:04,069 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@24934262{/streaming/json,null,AVAILABLE,@Spark}
2020-04-08 14:41:04,070 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@28501a4b{/streaming/batch,null,AVAILABLE,@Spark}
2020-04-08 14:41:04,071 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5b051a5c{/streaming/batch/json,null,AVAILABLE,@Spark}
2020-04-08 14:41:04,072 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2bebb74f{/static/streaming,null,AVAILABLE,@Spark}
2020-04-08 14:41:04,073 INFO org.apache.spark.streaming.StreamingContext: StreamingContext started
2020-04-08 14:41:04,115 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 14:41:04,115 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 14:41:04,115 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 14:41:05,119 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 153.
2020-04-08 14:41:05,127 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 14:41:05,127 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 154.
2020-04-08 14:41:05,151 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586328060000 ms
2020-04-08 14:41:05,153 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586328060000 ms.0 from job set of time 1586328060000 ms
2020-04-08 14:41:05,186 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:136
2020-04-08 14:41:05,201 INFO org.apache.spark.scheduler.DAGScheduler: Got job 0 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136) with 3 output partitions
2020-04-08 14:41:05,202 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 0 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136)
2020-04-08 14:41:05,202 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 14:41:05,203 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 14:41:05,207 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 0 (KafkaRDD[0] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113), which has no missing parents
2020-04-08 14:41:05,263 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 14:41:05,640 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 14:41:05,642 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:53418 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 14:41:05,644 INFO org.apache.spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2020-04-08 14:41:05,655 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 0 (KafkaRDD[0] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 14:41:05,656 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 0.0 with 3 tasks
2020-04-08 14:41:05,691 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:41:05,699 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
2020-04-08 14:41:05,726 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 154 is the same as ending offset skipping PABUSI2 1
2020-04-08 14:41:05,737 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 2828 bytes result sent to driver
2020-04-08 14:41:05,739 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:41:05,740 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 0.0 (TID 1)
2020-04-08 14:41:05,743 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 62 ms on localhost (executor driver) (1/3)
2020-04-08 14:41:05,744 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 2
2020-04-08 14:41:05,745 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 0.0 (TID 1). 2828 bytes result sent to driver
2020-04-08 14:41:05,746 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:41:05,747 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 0.0 (TID 2)
2020-04-08 14:41:05,747 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 8 ms on localhost (executor driver) (2/3)
2020-04-08 14:41:05,750 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 14:41:05,751 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 0.0 (TID 2). 2828 bytes result sent to driver
2020-04-08 14:41:05,755 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 9 ms on localhost (executor driver) (3/3)
2020-04-08 14:41:05,756 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-04-08 14:41:05,757 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 0 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136) finished in 0.534 s
2020-04-08 14:41:05,762 INFO org.apache.spark.scheduler.DAGScheduler: Job 0 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:136, took 0.575597 s
2020-04-08 14:41:05,766 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586328060000 ms.0 from job set of time 1586328060000 ms
2020-04-08 14:41:05,767 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 5.765 s for time 1586328060000 ms (execution: 0.612 s)
2020-04-08 14:41:05,772 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 14:41:05,776 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 
2020-04-08 14:41:10,009 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 14:41:10,009 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 14:41:10,009 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 14:41:10,183 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 153.
2020-04-08 14:41:10,186 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 153.
2020-04-08 14:41:10,186 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 155.
2020-04-08 14:41:10,187 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586328070000 ms
2020-04-08 14:41:10,188 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586328070000 ms.0 from job set of time 1586328070000 ms
2020-04-08 14:41:10,195 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:136
2020-04-08 14:41:10,196 INFO org.apache.spark.scheduler.DAGScheduler: Got job 1 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136) with 3 output partitions
2020-04-08 14:41:10,196 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 1 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136)
2020-04-08 14:41:10,196 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 14:41:10,196 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 14:41:10,196 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 1 (KafkaRDD[1] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113), which has no missing parents
2020-04-08 14:41:10,199 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 14:41:10,214 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 14:41:10,215 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.102:53418 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 14:41:10,216 INFO org.apache.spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2020-04-08 14:41:10,217 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 4
2020-04-08 14:41:10,217 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 1 (KafkaRDD[1] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 14:41:10,217 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 12
2020-04-08 14:41:10,218 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 1.0 with 3 tasks
2020-04-08 14:41:10,218 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 13
2020-04-08 14:41:10,218 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 20
2020-04-08 14:41:10,218 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 24
2020-04-08 14:41:10,218 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 3
2020-04-08 14:41:10,218 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 16
2020-04-08 14:41:10,218 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 23
2020-04-08 14:41:10,218 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 1
2020-04-08 14:41:10,218 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 21
2020-04-08 14:41:10,219 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 25
2020-04-08 14:41:10,219 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 6
2020-04-08 14:41:10,219 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 17
2020-04-08 14:41:10,219 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 10
2020-04-08 14:41:10,219 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 14
2020-04-08 14:41:10,219 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 8
2020-04-08 14:41:10,219 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 22
2020-04-08 14:41:10,219 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 15
2020-04-08 14:41:10,221 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:41:10,221 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 1.0 (TID 3)
2020-04-08 14:41:10,225 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 2
2020-04-08 14:41:10,227 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 1.0 (TID 3). 2828 bytes result sent to driver
2020-04-08 14:41:10,228 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 4, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:41:10,229 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 1.0 (TID 4)
2020-04-08 14:41:10,229 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 3) in 9 ms on localhost (executor driver) (1/3)
2020-04-08 14:41:10,232 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Computing topic PABUSI2, partition 1 offsets 154 -> 155
2020-04-08 14:41:10,233 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.102:53418 in memory (size: 3.3 KB, free: 4.1 GB)
2020-04-08 14:41:10,236 INFO org.apache.spark.streaming.kafka010.KafkaDataConsumer: Initializing cache 16 64 0.75
2020-04-08 14:41:10,236 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 9
2020-04-08 14:41:10,237 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 7
2020-04-08 14:41:10,237 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 5
2020-04-08 14:41:10,237 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 11
2020-04-08 14:41:10,237 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 19
2020-04-08 14:41:10,237 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 18
2020-04-08 14:41:10,237 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 2
2020-04-08 14:41:10,238 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-testgroup1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-08 14:41:10,243 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-08 14:41:10,244 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-08 14:41:10,244 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586328070243
2020-04-08 14:41:10,244 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Subscribed to partition(s): PABUSI2-1
2020-04-08 14:41:10,246 INFO org.apache.spark.streaming.kafka010.InternalKafkaConsumer: Initial fetch for spark-executor-testgroup1 PABUSI2-1 154
2020-04-08 14:41:10,247 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Seeking to offset 154 for partition PABUSI2-1
2020-04-08 14:41:10,776 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-08 14:41:13,026 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 1.0 (TID 4). 2828 bytes result sent to driver
2020-04-08 14:41:13,027 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 1.0 (TID 5, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:41:13,028 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 1.0 (TID 5)
2020-04-08 14:41:13,028 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 4) in 2800 ms on localhost (executor driver) (2/3)
2020-04-08 14:41:13,030 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 0
2020-04-08 14:41:13,031 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 1.0 (TID 5). 2828 bytes result sent to driver
2020-04-08 14:41:13,032 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 1.0 (TID 5) in 5 ms on localhost (executor driver) (3/3)
2020-04-08 14:41:13,032 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
2020-04-08 14:41:13,032 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 1 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136) finished in 2.834 s
2020-04-08 14:41:13,033 INFO org.apache.spark.scheduler.DAGScheduler: Job 1 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:136, took 2.837467 s
2020-04-08 14:41:13,033 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586328070000 ms.0 from job set of time 1586328070000 ms
2020-04-08 14:41:13,033 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 3.033 s for time 1586328070000 ms (execution: 2.845 s)
2020-04-08 14:41:13,034 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 0 from persistence list
2020-04-08 14:41:13,036 INFO org.apache.spark.storage.BlockManager: Removing RDD 0
2020-04-08 14:41:13,037 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 14:41:13,037 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 
2020-04-08 14:41:19,635 INFO org.apache.spark.streaming.StreamingContext: Invoking stop(stopGracefully=false) from shutdown hook
2020-04-08 14:41:19,637 INFO org.apache.spark.streaming.scheduler.ReceiverTracker: ReceiverTracker stopped
2020-04-08 14:41:19,638 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopping JobGenerator immediately
2020-04-08 14:41:19,639 INFO org.apache.spark.streaming.util.RecurringTimer: Stopped timer for JobGenerator after time 1586328070000
2020-04-08 14:41:19,816 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Member consumer-1-322b09be-609d-4711-be06-64dd46f2ac72 sending LeaveGroup request to coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 14:41:19,997 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopped JobGenerator
2020-04-08 14:41:19,999 INFO org.apache.spark.streaming.scheduler.JobScheduler: Stopped JobScheduler
2020-04-08 14:41:20,004 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@499ef98e{/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 14:41:20,004 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@28501a4b{/streaming/batch,null,UNAVAILABLE,@Spark}
2020-04-08 14:41:20,005 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@2bebb74f{/static/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 14:41:20,006 INFO org.apache.spark.streaming.StreamingContext: StreamingContext stopped successfully
2020-04-08 14:41:20,006 INFO org.apache.spark.SparkContext: Invoking stop() from shutdown hook
2020-04-08 14:41:20,013 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@533b266e{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 14:41:20,015 INFO org.apache.spark.ui.SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
2020-04-08 14:41:20,023 INFO org.apache.spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2020-04-08 14:41:20,036 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2020-04-08 14:41:20,036 INFO org.apache.spark.storage.BlockManager: BlockManager stopped
2020-04-08 14:41:20,037 INFO org.apache.spark.storage.BlockManagerMaster: BlockManagerMaster stopped
2020-04-08 14:41:20,039 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2020-04-08 14:41:20,047 INFO org.apache.spark.SparkContext: Successfully stopped SparkContext
2020-04-08 14:41:20,047 INFO org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2020-04-08 14:41:20,048 INFO org.apache.spark.util.ShutdownHookManager: Deleting directory /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/spark-4d5bc4b8-c297-4472-9578-506f1def7857
2020-04-08 14:42:50,875 WARN org.apache.spark.util.Utils: Your hostname, MacBook-Pro-CW.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)
2020-04-08 14:42:50,876 WARN org.apache.spark.util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
2020-04-08 14:42:50,908 INFO org.apache.spark.SparkContext: Running Spark version 2.4.0.cloudera2
2020-04-08 14:42:51,106 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-04-08 14:42:51,205 INFO org.apache.spark.SparkContext: Submitted application: Kafka2SparkStreaming2Kudu-kerberos
2020-04-08 14:42:51,249 INFO org.apache.spark.SecurityManager: Changing view acls to: godiswc
2020-04-08 14:42:51,249 INFO org.apache.spark.SecurityManager: Changing modify acls to: godiswc
2020-04-08 14:42:51,250 INFO org.apache.spark.SecurityManager: Changing view acls groups to: 
2020-04-08 14:42:51,250 INFO org.apache.spark.SecurityManager: Changing modify acls groups to: 
2020-04-08 14:42:51,250 INFO org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(godiswc); groups with view permissions: Set(); users  with modify permissions: Set(godiswc); groups with modify permissions: Set()
2020-04-08 14:42:51,401 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 14:42:51,475 INFO org.apache.spark.util.Utils: Successfully started service 'sparkDriver' on port 53854.
2020-04-08 14:42:51,489 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
2020-04-08 14:42:51,500 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
2020-04-08 14:42:51,519 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-04-08 14:42:51,519 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2020-04-08 14:42:51,531 INFO org.apache.spark.storage.DiskBlockManager: Created local directory at /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/blockmgr-03a60541-2c5e-4a3e-a528-8cfbac0eb965
2020-04-08 14:42:51,545 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 4.1 GB
2020-04-08 14:42:51,554 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
2020-04-08 14:42:51,597 INFO org.spark_project.jetty.util.log: Logging initialized @1335ms
2020-04-08 14:42:51,645 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2019-04-30T04:42:08+08:00, git hash: e1bc35120a6617ee3df052294e433f3a25ce7097
2020-04-08 14:42:51,659 INFO org.spark_project.jetty.server.Server: Started @1398ms
2020-04-08 14:42:51,662 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 14:42:51,676 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@533b266e{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 14:42:51,676 INFO org.apache.spark.util.Utils: Successfully started service 'SparkUI' on port 4040.
2020-04-08 14:42:51,702 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6e521c1e{/jobs,null,AVAILABLE,@Spark}
2020-04-08 14:42:51,707 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62dae245{/jobs/json,null,AVAILABLE,@Spark}
2020-04-08 14:42:51,707 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4b6579e8{/jobs/job,null,AVAILABLE,@Spark}
2020-04-08 14:42:51,710 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@591e58fa{/jobs/job/json,null,AVAILABLE,@Spark}
2020-04-08 14:42:51,711 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3954d008{/stages,null,AVAILABLE,@Spark}
2020-04-08 14:42:51,712 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f94c4db{/stages/json,null,AVAILABLE,@Spark}
2020-04-08 14:42:51,712 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@593e824f{/stages/stage,null,AVAILABLE,@Spark}
2020-04-08 14:42:51,714 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@64bc21ac{/stages/stage/json,null,AVAILABLE,@Spark}
2020-04-08 14:42:51,715 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@493dfb8e{/stages/pool,null,AVAILABLE,@Spark}
2020-04-08 14:42:51,716 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5d25e6bb{/stages/pool/json,null,AVAILABLE,@Spark}
2020-04-08 14:42:51,717 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@ce5a68e{/storage,null,AVAILABLE,@Spark}
2020-04-08 14:42:51,718 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@9d157ff{/storage/json,null,AVAILABLE,@Spark}
2020-04-08 14:42:51,720 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f162cc0{/storage/rdd,null,AVAILABLE,@Spark}
2020-04-08 14:42:51,720 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5df417a7{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-04-08 14:42:51,721 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7c041b41{/environment,null,AVAILABLE,@Spark}
2020-04-08 14:42:51,721 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7f69d591{/environment/json,null,AVAILABLE,@Spark}
2020-04-08 14:42:51,722 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@61078690{/executors,null,AVAILABLE,@Spark}
2020-04-08 14:42:51,722 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1cb3ec38{/executors/json,null,AVAILABLE,@Spark}
2020-04-08 14:42:51,723 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@403132fc{/executors/threadDump,null,AVAILABLE,@Spark}
2020-04-08 14:42:51,724 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@71c5b236{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-04-08 14:42:51,730 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2cab9998{/static,null,AVAILABLE,@Spark}
2020-04-08 14:42:51,730 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@332a7fce{/,null,AVAILABLE,@Spark}
2020-04-08 14:42:51,731 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@549621f3{/api,null,AVAILABLE,@Spark}
2020-04-08 14:42:51,732 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@37ebc9d8{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-04-08 14:42:51,732 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@293bb8a5{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-04-08 14:42:51,734 INFO org.apache.spark.ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
2020-04-08 14:42:51,824 INFO org.apache.spark.executor.Executor: Starting executor ID driver on host localhost
2020-04-08 14:42:51,877 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 14:42:51,878 INFO org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53856.
2020-04-08 14:42:51,879 INFO org.apache.spark.network.netty.NettyBlockTransferService: Server created on 192.168.0.102:53856
2020-04-08 14:42:51,880 INFO org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-04-08 14:42:51,906 INFO org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 53856, None)
2020-04-08 14:42:51,909 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:53856 with 4.1 GB RAM, BlockManagerId(driver, 192.168.0.102, 53856, None)
2020-04-08 14:42:51,911 INFO org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 53856, None)
2020-04-08 14:42:51,912 INFO org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 53856, None)
2020-04-08 14:42:52,073 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@54acff7d{/metrics/json,null,AVAILABLE,@Spark}
2020-04-08 14:42:52,171 WARN org.apache.spark.streaming.StreamingContext: spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
2020-04-08 14:42:52,373 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding enable.auto.commit to false for executor
2020-04-08 14:42:52,373 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding auto.offset.reset to none for executor
2020-04-08 14:42:52,374 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding executor group.id to spark-executor-testgroup1
2020-04-08 14:42:52,374 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
2020-04-08 14:42:54,167 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Slide time = 10000 ms
2020-04-08 14:42:54,168 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Storage level = Serialized 1x Replicated
2020-04-08 14:42:54,168 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Checkpoint interval = null
2020-04-08 14:42:54,169 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Remember interval = 10000 ms
2020-04-08 14:42:54,170 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@61641cd2
2020-04-08 14:42:54,170 INFO org.apache.spark.streaming.dstream.ForEachDStream: Slide time = 10000 ms
2020-04-08 14:42:54,170 INFO org.apache.spark.streaming.dstream.ForEachDStream: Storage level = Serialized 1x Replicated
2020-04-08 14:42:54,170 INFO org.apache.spark.streaming.dstream.ForEachDStream: Checkpoint interval = null
2020-04-08 14:42:54,170 INFO org.apache.spark.streaming.dstream.ForEachDStream: Remember interval = 10000 ms
2020-04-08 14:42:54,171 INFO org.apache.spark.streaming.dstream.ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@4ffd78c8
2020-04-08 14:42:54,238 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = testgroup1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-08 14:42:54,320 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-08 14:42:54,320 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-08 14:42:54,320 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586328174319
2020-04-08 14:42:54,322 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-1, groupId=testgroup1] Subscribed to topic(s): PABUSI2
2020-04-08 14:42:55,092 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-1, groupId=testgroup1] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-08 14:42:55,093 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Discovered group coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 14:42:55,098 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Revoking previously assigned partitions []
2020-04-08 14:42:55,098 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 14:42:55,631 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 14:42:58,994 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Successfully joined group with generation 43
2020-04-08 14:42:59,000 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting newly assigned partitions: PABUSI2-2, PABUSI2-0, PABUSI2-1
2020-04-08 14:42:59,187 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-2 to the committed offset FetchPosition{offset=153, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-2.vpc.cloudera.com:9092 (id: 52 rack: null), epoch=0}}
2020-04-08 14:42:59,189 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-0 to the committed offset FetchPosition{offset=153, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-4.vpc.cloudera.com:9092 (id: 53 rack: null), epoch=2}}
2020-04-08 14:42:59,189 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-1 to the committed offset FetchPosition{offset=155, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-3.vpc.cloudera.com:9092 (id: 54 rack: null), epoch=2}}
2020-04-08 14:42:59,198 INFO org.apache.spark.streaming.util.RecurringTimer: Started timer for JobGenerator at time 1586328180000
2020-04-08 14:42:59,198 INFO org.apache.spark.streaming.scheduler.JobGenerator: Started JobGenerator at 1586328180000 ms
2020-04-08 14:42:59,199 INFO org.apache.spark.streaming.scheduler.JobScheduler: Started JobScheduler
2020-04-08 14:42:59,204 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@499ef98e{/streaming,null,AVAILABLE,@Spark}
2020-04-08 14:42:59,204 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@24934262{/streaming/json,null,AVAILABLE,@Spark}
2020-04-08 14:42:59,205 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@28501a4b{/streaming/batch,null,AVAILABLE,@Spark}
2020-04-08 14:42:59,205 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5b051a5c{/streaming/batch/json,null,AVAILABLE,@Spark}
2020-04-08 14:42:59,206 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2bebb74f{/static/streaming,null,AVAILABLE,@Spark}
2020-04-08 14:42:59,207 INFO org.apache.spark.streaming.StreamingContext: StreamingContext started
2020-04-08 14:43:00,045 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 14:43:00,046 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 14:43:00,046 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 14:43:00,227 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 154.
2020-04-08 14:43:00,600 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 155.
2020-04-08 14:43:00,600 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 153.
2020-04-08 14:43:00,629 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586328180000 ms
2020-04-08 14:43:00,633 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586328180000 ms.0 from job set of time 1586328180000 ms
2020-04-08 14:43:00,682 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:136
2020-04-08 14:43:00,697 INFO org.apache.spark.scheduler.DAGScheduler: Got job 0 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136) with 3 output partitions
2020-04-08 14:43:00,699 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 0 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136)
2020-04-08 14:43:00,699 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 14:43:00,700 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 14:43:00,704 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 0 (KafkaRDD[0] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113), which has no missing parents
2020-04-08 14:43:00,785 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 14:43:01,234 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 14:43:01,236 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:53856 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 14:43:01,237 INFO org.apache.spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2020-04-08 14:43:01,256 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 0 (KafkaRDD[0] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 14:43:01,256 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 0.0 with 3 tasks
2020-04-08 14:43:01,295 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:43:01,302 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
2020-04-08 14:43:01,326 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 2
2020-04-08 14:43:01,336 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 2828 bytes result sent to driver
2020-04-08 14:43:01,338 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:43:01,339 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 0.0 (TID 1)
2020-04-08 14:43:01,343 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 57 ms on localhost (executor driver) (1/3)
2020-04-08 14:43:01,343 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 155 is the same as ending offset skipping PABUSI2 1
2020-04-08 14:43:01,344 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 0.0 (TID 1). 2871 bytes result sent to driver
2020-04-08 14:43:01,345 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:43:01,346 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 0.0 (TID 2)
2020-04-08 14:43:01,346 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 8 ms on localhost (executor driver) (2/3)
2020-04-08 14:43:01,349 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Computing topic PABUSI2, partition 0 offsets 153 -> 154
2020-04-08 14:43:01,352 INFO org.apache.spark.streaming.kafka010.KafkaDataConsumer: Initializing cache 16 64 0.75
2020-04-08 14:43:01,355 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-testgroup1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-08 14:43:01,359 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-08 14:43:01,359 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-08 14:43:01,360 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586328181359
2020-04-08 14:43:01,360 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Subscribed to partition(s): PABUSI2-0
2020-04-08 14:43:01,362 INFO org.apache.spark.streaming.kafka010.InternalKafkaConsumer: Initial fetch for spark-executor-testgroup1 PABUSI2-0 153
2020-04-08 14:43:01,363 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Seeking to offset 153 for partition PABUSI2-0
2020-04-08 14:43:01,909 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-08 14:43:03,040 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (BASE_PREMIUM, ) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:592)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:210)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:190)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:188)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:188)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:43:03,042 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (MASTER_POLICY_ID, ) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:592)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:210)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:190)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:188)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:188)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:43:03,042 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (VAT, 220.75) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: "220.75"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:210)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:190)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:188)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:188)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:43:03,043 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (PRODUCT_ID, 22222222222) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: "22222222222"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:583)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:210)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:190)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:188)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:188)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:43:03,043 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (START_REVISION_ID, 7900000000000038) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: "7900000000000038"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:583)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:210)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:190)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:188)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:188)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:43:03,044 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (PRD_PKG_DISCOUNT_RATE, ) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:592)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:210)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:190)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:188)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:188)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:43:03,044 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (BELONG_TO_HANDLER_ID, 26004873143) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: "26004873143"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:583)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:210)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:190)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:188)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:188)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:43:03,044 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (C1_FEE_RATE, ) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:592)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:210)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:190)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:188)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:188)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:43:03,045 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (REGULAR_ENDO_DAYS_COUNT, ) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:592)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:210)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:190)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:188)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:188)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:43:03,045 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (STD_PREMIUM_LOCAL, ) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:592)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:210)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:190)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:188)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:188)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:43:03,046 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (UPDATE_BY, 260064029580813) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: "260064029580813"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:583)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:210)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:190)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:188)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:188)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:43:03,046 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (POLICY_ID0, 3003664443882041) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: "3003664443882041"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:583)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:210)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:190)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:188)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:188)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:43:03,046 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (BEFORE_VAT_PREMIUM_LOCAL, 3679.25) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: "3679.25"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:210)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:190)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:188)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:188)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:43:03,047 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (COMPY_VEH_TAX_PREMIUM, ) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:592)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:210)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:190)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:188)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:188)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:43:03,047 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (BASE_PREMIUM_LOCAL, ) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:592)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:210)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:190)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:188)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:188)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:43:03,047 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (APP_ID, ) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:592)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:210)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:190)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:188)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:188)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:43:03,048 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (TOTAL_DISCOUNT_RATE, ) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:592)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:210)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:190)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:188)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:188)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:43:03,048 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (GROSS_PREMIUM_LOCAL, 3899.97) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: "3899.97"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:210)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:190)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:188)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:188)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:43:03,048 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (PRD_PKG_DISCOUNT, ) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:592)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:210)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:190)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:188)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:188)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:43:03,049 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (C2_FEE_RATE, ) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:592)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:210)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:190)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:188)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:188)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:43:03,049 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (EXPECTED_ADJUSTED_RATE, ) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:592)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:210)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:190)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:188)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:188)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:43:03,050 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (STD_PREMIUM, ) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:592)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:210)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:190)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:188)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:188)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:43:03,050 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (ADJUSTED_PREMIUM_LOCAL, ) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:592)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:210)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:190)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:188)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:188)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:43:03,050 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (POLICY_ID, 9900000000000008) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: "9900000000000008"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:583)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:210)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:190)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:188)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:188)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:43:03,051 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (ISSUE_USER_ID, 26004917171) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: "26004917171"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:583)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:210)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:190)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:188)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:188)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:43:03,051 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (REVISION_TYPE, ) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:592)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:210)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:190)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:188)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:188)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:43:03,051 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (AGENT_ID, 26006983857) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: "26006983857"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:583)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:210)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:190)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:188)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:188)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:43:03,051 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (GROSS_PREMIUM, 3899.97) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: "3899.97"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:210)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:190)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:188)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:188)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:43:03,052 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (ORG_ID, 260060047158204) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: "260060047158204"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:583)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:210)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:190)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:188)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:188)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:43:03,052 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (VAT_RATE, ) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:592)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:210)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:190)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:188)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:188)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:43:03,052 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (VAT_LOCAL, 220.75) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: "220.75"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:210)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:190)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:188)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:188)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:43:03,053 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (EXPECTED_PREMIUM, ) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:592)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:210)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:190)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:188)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:188)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:43:03,053 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (INSERT_BY, 26004917171) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: "26004917171"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:583)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:210)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:190)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:188)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:188)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:43:03,053 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (SMS_TEMPLATE_ID, ) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:592)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:210)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:190)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:188)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:188)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:43:03,054 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (C_FEE_RATE, ) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:592)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:210)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:190)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:188)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:188)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:43:03,054 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (BEFORE_VAT_PREMIUM, 3679.25) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: "3679.25"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:210)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:190)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:188)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:188)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:43:03,054 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (MP_TOTAL_PREPAY_PREMIUM, ) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:592)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:210)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:190)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:188)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:188)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:43:03,054 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (CESSION_BASIS_CODE, ) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:592)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:210)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:190)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:188)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:188)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:43:03,055 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (ISSUE_ORG_ID, 260059653500187) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: "260059653500187"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:583)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:210)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:190)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:188)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:188)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:43:03,055 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (SUBMISSION_VER_ID, ) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:592)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:210)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:190)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:188)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:188)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:43:03,056 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (MP_TOTAL_SUM_INSURED, ) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:592)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:210)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:190)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:188)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:188)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:43:03,056 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (SUBMISSION_ID, ) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:592)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:210)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:190)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:188)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:188)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:43:03,062 ERROR org.apache.spark.executor.Executor: Exception in task 2.0 in stage 0.0 (TID 2)
org.apache.kudu.client.NonRecoverableException: Primary key column policy_id is not set
	at org.apache.kudu.client.KuduException.transformException(KuduException.java:132)
	at org.apache.kudu.client.KuduSession.apply(KuduSession.java:94)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:198)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalStateException: Primary key column policy_id is not set
	at org.apache.kudu.client.KeyEncoder.encodeColumn(KeyEncoder.java:143)
	at org.apache.kudu.client.KeyEncoder.encodeColumns(KeyEncoder.java:126)
	at org.apache.kudu.client.KeyEncoder.getHashBucket(KeyEncoder.java:80)
	at org.apache.kudu.client.KeyEncoder.encodePartitionKey(KeyEncoder.java:96)
	at org.apache.kudu.client.PartitionSchema.encodePartitionKey(PartitionSchema.java:84)
	at org.apache.kudu.client.Operation.partitionKey(Operation.java:164)
	at org.apache.kudu.client.AsyncKuduSession.apply(AsyncKuduSession.java:560)
	at org.apache.kudu.client.KuduSession.apply(KuduSession.java:80)
	... 19 more
2020-04-08 14:43:03,089 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 2.0 in stage 0.0 (TID 2, localhost, executor driver): org.apache.kudu.client.NonRecoverableException: Primary key column policy_id is not set
	at org.apache.kudu.client.KuduException.transformException(KuduException.java:132)
	at org.apache.kudu.client.KuduSession.apply(KuduSession.java:94)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:198)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalStateException: Primary key column policy_id is not set
	at org.apache.kudu.client.KeyEncoder.encodeColumn(KeyEncoder.java:143)
	at org.apache.kudu.client.KeyEncoder.encodeColumns(KeyEncoder.java:126)
	at org.apache.kudu.client.KeyEncoder.getHashBucket(KeyEncoder.java:80)
	at org.apache.kudu.client.KeyEncoder.encodePartitionKey(KeyEncoder.java:96)
	at org.apache.kudu.client.PartitionSchema.encodePartitionKey(PartitionSchema.java:84)
	at org.apache.kudu.client.Operation.partitionKey(Operation.java:164)
	at org.apache.kudu.client.AsyncKuduSession.apply(AsyncKuduSession.java:560)
	at org.apache.kudu.client.KuduSession.apply(KuduSession.java:80)
	... 19 more

2020-04-08 14:43:03,090 ERROR org.apache.spark.scheduler.TaskSetManager: Task 2 in stage 0.0 failed 1 times; aborting job
2020-04-08 14:43:03,091 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-04-08 14:43:03,093 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Cancelling stage 0
2020-04-08 14:43:03,093 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Killing all running tasks in stage 0: Stage cancelled
2020-04-08 14:43:03,094 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 0 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136) failed in 2.376 s due to Job aborted due to stage failure: Task 2 in stage 0.0 failed 1 times, most recent failure: Lost task 2.0 in stage 0.0 (TID 2, localhost, executor driver): org.apache.kudu.client.NonRecoverableException: Primary key column policy_id is not set
	at org.apache.kudu.client.KuduException.transformException(KuduException.java:132)
	at org.apache.kudu.client.KuduSession.apply(KuduSession.java:94)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:198)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalStateException: Primary key column policy_id is not set
	at org.apache.kudu.client.KeyEncoder.encodeColumn(KeyEncoder.java:143)
	at org.apache.kudu.client.KeyEncoder.encodeColumns(KeyEncoder.java:126)
	at org.apache.kudu.client.KeyEncoder.getHashBucket(KeyEncoder.java:80)
	at org.apache.kudu.client.KeyEncoder.encodePartitionKey(KeyEncoder.java:96)
	at org.apache.kudu.client.PartitionSchema.encodePartitionKey(PartitionSchema.java:84)
	at org.apache.kudu.client.Operation.partitionKey(Operation.java:164)
	at org.apache.kudu.client.AsyncKuduSession.apply(AsyncKuduSession.java:560)
	at org.apache.kudu.client.KuduSession.apply(KuduSession.java:80)
	... 19 more

Driver stacktrace:
2020-04-08 14:43:03,098 INFO org.apache.spark.scheduler.DAGScheduler: Job 0 failed: foreachPartition at Kafka2SparkStreaming2Kudu.scala:136, took 2.415831 s
2020-04-08 14:43:03,100 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586328180000 ms.0 from job set of time 1586328180000 ms
2020-04-08 14:43:03,101 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1586328180000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 0.0 failed 1 times, most recent failure: Lost task 2.0 in stage 0.0 (TID 2, localhost, executor driver): org.apache.kudu.client.NonRecoverableException: Primary key column policy_id is not set
	at org.apache.kudu.client.KuduException.transformException(KuduException.java:132)
	at org.apache.kudu.client.KuduSession.apply(KuduSession.java:94)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:198)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalStateException: Primary key column policy_id is not set
	at org.apache.kudu.client.KeyEncoder.encodeColumn(KeyEncoder.java:143)
	at org.apache.kudu.client.KeyEncoder.encodeColumns(KeyEncoder.java:126)
	at org.apache.kudu.client.KeyEncoder.getHashBucket(KeyEncoder.java:80)
	at org.apache.kudu.client.KeyEncoder.encodePartitionKey(KeyEncoder.java:96)
	at org.apache.kudu.client.PartitionSchema.encodePartitionKey(PartitionSchema.java:84)
	at org.apache.kudu.client.Operation.partitionKey(Operation.java:164)
	at org.apache.kudu.client.AsyncKuduSession.apply(AsyncKuduSession.java:560)
	at org.apache.kudu.client.KuduSession.apply(KuduSession.java:80)
	... 19 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:245)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:933)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:933)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1.apply(Kafka2SparkStreaming2Kudu.scala:131)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:191)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:43:03,112 INFO org.apache.spark.streaming.StreamingContext: Invoking stop(stopGracefully=false) from shutdown hook
2020-04-08 14:43:03,113 INFO org.apache.spark.streaming.scheduler.ReceiverTracker: ReceiverTracker stopped
2020-04-08 14:43:03,114 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopping JobGenerator immediately
2020-04-08 14:43:03,114 INFO org.apache.spark.streaming.util.RecurringTimer: Stopped timer for JobGenerator after time 1586328180000
2020-04-08 14:43:03,293 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Member consumer-1-0a931400-aece-460c-8076-607b2e9c5da3 sending LeaveGroup request to coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 14:43:03,474 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopped JobGenerator
2020-04-08 14:43:03,477 INFO org.apache.spark.streaming.scheduler.JobScheduler: Stopped JobScheduler
2020-04-08 14:43:03,481 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@499ef98e{/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 14:43:03,481 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@28501a4b{/streaming/batch,null,UNAVAILABLE,@Spark}
2020-04-08 14:43:03,482 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@2bebb74f{/static/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 14:43:03,483 INFO org.apache.spark.streaming.StreamingContext: StreamingContext stopped successfully
2020-04-08 14:43:03,483 INFO org.apache.spark.SparkContext: Invoking stop() from shutdown hook
2020-04-08 14:43:03,487 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@533b266e{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 14:43:03,488 INFO org.apache.spark.ui.SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
2020-04-08 14:43:03,493 INFO org.apache.spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2020-04-08 14:43:03,504 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2020-04-08 14:43:03,504 INFO org.apache.spark.storage.BlockManager: BlockManager stopped
2020-04-08 14:43:03,505 INFO org.apache.spark.storage.BlockManagerMaster: BlockManagerMaster stopped
2020-04-08 14:43:03,507 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2020-04-08 14:43:03,517 INFO org.apache.spark.SparkContext: Successfully stopped SparkContext
2020-04-08 14:43:03,518 INFO org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2020-04-08 14:43:03,518 INFO org.apache.spark.util.ShutdownHookManager: Deleting directory /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/spark-f55de697-e3b0-4da4-bf41-9156e8c643af
2020-04-08 14:52:20,788 WARN org.apache.spark.util.Utils: Your hostname, MacBook-Pro-CW.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)
2020-04-08 14:52:20,791 WARN org.apache.spark.util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
2020-04-08 14:52:20,826 INFO org.apache.spark.SparkContext: Running Spark version 2.4.0.cloudera2
2020-04-08 14:52:21,027 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-04-08 14:52:21,170 INFO org.apache.spark.SparkContext: Submitted application: Kafka2-kerberos
2020-04-08 14:52:21,210 INFO org.apache.spark.SecurityManager: Changing view acls to: godiswc
2020-04-08 14:52:21,211 INFO org.apache.spark.SecurityManager: Changing modify acls to: godiswc
2020-04-08 14:52:21,211 INFO org.apache.spark.SecurityManager: Changing view acls groups to: 
2020-04-08 14:52:21,211 INFO org.apache.spark.SecurityManager: Changing modify acls groups to: 
2020-04-08 14:52:21,212 INFO org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(godiswc); groups with view permissions: Set(); users  with modify permissions: Set(godiswc); groups with modify permissions: Set()
2020-04-08 14:52:21,352 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 14:52:21,442 INFO org.apache.spark.util.Utils: Successfully started service 'sparkDriver' on port 55996.
2020-04-08 14:52:21,460 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
2020-04-08 14:52:21,471 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
2020-04-08 14:52:21,472 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-04-08 14:52:21,473 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2020-04-08 14:52:21,483 INFO org.apache.spark.storage.DiskBlockManager: Created local directory at /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/blockmgr-584a3f6e-2f97-462c-a9e1-9350efa09952
2020-04-08 14:52:21,515 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 4.1 GB
2020-04-08 14:52:21,527 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
2020-04-08 14:52:21,590 INFO org.spark_project.jetty.util.log: Logging initialized @1564ms
2020-04-08 14:52:21,644 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2019-04-30T04:42:08+08:00, git hash: e1bc35120a6617ee3df052294e433f3a25ce7097
2020-04-08 14:52:21,659 INFO org.spark_project.jetty.server.Server: Started @1634ms
2020-04-08 14:52:21,661 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 14:52:21,673 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@743cb8e0{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 14:52:21,673 INFO org.apache.spark.util.Utils: Successfully started service 'SparkUI' on port 4040.
2020-04-08 14:52:21,695 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@41a90fa8{/jobs,null,AVAILABLE,@Spark}
2020-04-08 14:52:21,696 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c67e137{/jobs/json,null,AVAILABLE,@Spark}
2020-04-08 14:52:21,696 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2c9399a4{/jobs/job,null,AVAILABLE,@Spark}
2020-04-08 14:52:21,699 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@53ab0286{/jobs/job/json,null,AVAILABLE,@Spark}
2020-04-08 14:52:21,699 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@63c5efee{/stages,null,AVAILABLE,@Spark}
2020-04-08 14:52:21,700 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2d10e0b1{/stages/json,null,AVAILABLE,@Spark}
2020-04-08 14:52:21,700 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1c98290c{/stages/stage,null,AVAILABLE,@Spark}
2020-04-08 14:52:21,701 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@71e5f61d{/stages/stage/json,null,AVAILABLE,@Spark}
2020-04-08 14:52:21,702 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2ce86164{/stages/pool,null,AVAILABLE,@Spark}
2020-04-08 14:52:21,702 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5e8f9e2d{/stages/pool/json,null,AVAILABLE,@Spark}
2020-04-08 14:52:21,703 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@51df223b{/storage,null,AVAILABLE,@Spark}
2020-04-08 14:52:21,703 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@fd46303{/storage/json,null,AVAILABLE,@Spark}
2020-04-08 14:52:21,704 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60d8c0dc{/storage/rdd,null,AVAILABLE,@Spark}
2020-04-08 14:52:21,704 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4204541c{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-04-08 14:52:21,705 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a62689d{/environment,null,AVAILABLE,@Spark}
2020-04-08 14:52:21,705 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4602c2a9{/environment/json,null,AVAILABLE,@Spark}
2020-04-08 14:52:21,706 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60fa3495{/executors,null,AVAILABLE,@Spark}
2020-04-08 14:52:21,706 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3e2822{/executors/json,null,AVAILABLE,@Spark}
2020-04-08 14:52:21,707 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@79e18e38{/executors/threadDump,null,AVAILABLE,@Spark}
2020-04-08 14:52:21,707 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@29a60c27{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-04-08 14:52:21,712 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1849db1a{/static,null,AVAILABLE,@Spark}
2020-04-08 14:52:21,713 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2cf23c81{/,null,AVAILABLE,@Spark}
2020-04-08 14:52:21,713 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3624da92{/api,null,AVAILABLE,@Spark}
2020-04-08 14:52:21,714 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2484f433{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-04-08 14:52:21,714 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60b71e8f{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-04-08 14:52:21,716 INFO org.apache.spark.ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
2020-04-08 14:52:21,787 INFO org.apache.spark.executor.Executor: Starting executor ID driver on host localhost
2020-04-08 14:52:21,858 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 14:52:21,859 INFO org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55997.
2020-04-08 14:52:21,860 INFO org.apache.spark.network.netty.NettyBlockTransferService: Server created on 192.168.0.102:55997
2020-04-08 14:52:21,862 INFO org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-04-08 14:52:21,894 INFO org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 55997, None)
2020-04-08 14:52:21,898 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:55997 with 4.1 GB RAM, BlockManagerId(driver, 192.168.0.102, 55997, None)
2020-04-08 14:52:21,901 INFO org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 55997, None)
2020-04-08 14:52:21,901 INFO org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 55997, None)
2020-04-08 14:52:22,052 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@502f8b57{/metrics/json,null,AVAILABLE,@Spark}
2020-04-08 14:52:22,147 INFO org.apache.spark.SparkContext: Invoking stop() from shutdown hook
2020-04-08 14:52:22,165 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@743cb8e0{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 14:52:22,167 INFO org.apache.spark.ui.SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
2020-04-08 14:52:22,177 INFO org.apache.spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2020-04-08 14:52:22,189 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2020-04-08 14:52:22,189 INFO org.apache.spark.storage.BlockManager: BlockManager stopped
2020-04-08 14:52:22,193 INFO org.apache.spark.storage.BlockManagerMaster: BlockManagerMaster stopped
2020-04-08 14:52:22,195 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2020-04-08 14:52:22,204 INFO org.apache.spark.SparkContext: Successfully stopped SparkContext
2020-04-08 14:52:22,205 INFO org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2020-04-08 14:52:22,205 INFO org.apache.spark.util.ShutdownHookManager: Deleting directory /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/spark-8ee780fd-235e-45a1-8b4a-910f57e99972
2020-04-08 14:52:59,167 WARN org.apache.spark.util.Utils: Your hostname, MacBook-Pro-CW.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)
2020-04-08 14:52:59,168 WARN org.apache.spark.util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
2020-04-08 14:52:59,194 INFO org.apache.spark.SparkContext: Running Spark version 2.4.0.cloudera2
2020-04-08 14:52:59,416 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-04-08 14:52:59,527 INFO org.apache.spark.SparkContext: Submitted application: Kafka2-kerberos
2020-04-08 14:52:59,568 INFO org.apache.spark.SecurityManager: Changing view acls to: godiswc
2020-04-08 14:52:59,568 INFO org.apache.spark.SecurityManager: Changing modify acls to: godiswc
2020-04-08 14:52:59,569 INFO org.apache.spark.SecurityManager: Changing view acls groups to: 
2020-04-08 14:52:59,569 INFO org.apache.spark.SecurityManager: Changing modify acls groups to: 
2020-04-08 14:52:59,569 INFO org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(godiswc); groups with view permissions: Set(); users  with modify permissions: Set(godiswc); groups with modify permissions: Set()
2020-04-08 14:52:59,710 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 14:52:59,815 INFO org.apache.spark.util.Utils: Successfully started service 'sparkDriver' on port 56138.
2020-04-08 14:52:59,835 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
2020-04-08 14:52:59,849 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
2020-04-08 14:52:59,851 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-04-08 14:52:59,851 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2020-04-08 14:52:59,864 INFO org.apache.spark.storage.DiskBlockManager: Created local directory at /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/blockmgr-136454d8-bcc1-41ad-aa23-1c12f4897ff4
2020-04-08 14:52:59,898 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 4.1 GB
2020-04-08 14:52:59,908 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
2020-04-08 14:52:59,967 INFO org.spark_project.jetty.util.log: Logging initialized @1362ms
2020-04-08 14:53:00,014 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2019-04-30T04:42:08+08:00, git hash: e1bc35120a6617ee3df052294e433f3a25ce7097
2020-04-08 14:53:00,027 INFO org.spark_project.jetty.server.Server: Started @1422ms
2020-04-08 14:53:00,029 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 14:53:00,040 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@743cb8e0{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 14:53:00,040 INFO org.apache.spark.util.Utils: Successfully started service 'SparkUI' on port 4040.
2020-04-08 14:53:00,059 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@41a90fa8{/jobs,null,AVAILABLE,@Spark}
2020-04-08 14:53:00,059 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c67e137{/jobs/json,null,AVAILABLE,@Spark}
2020-04-08 14:53:00,060 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2c9399a4{/jobs/job,null,AVAILABLE,@Spark}
2020-04-08 14:53:00,062 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@53ab0286{/jobs/job/json,null,AVAILABLE,@Spark}
2020-04-08 14:53:00,067 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@63c5efee{/stages,null,AVAILABLE,@Spark}
2020-04-08 14:53:00,068 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2d10e0b1{/stages/json,null,AVAILABLE,@Spark}
2020-04-08 14:53:00,069 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1c98290c{/stages/stage,null,AVAILABLE,@Spark}
2020-04-08 14:53:00,070 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@71e5f61d{/stages/stage/json,null,AVAILABLE,@Spark}
2020-04-08 14:53:00,070 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2ce86164{/stages/pool,null,AVAILABLE,@Spark}
2020-04-08 14:53:00,071 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5e8f9e2d{/stages/pool/json,null,AVAILABLE,@Spark}
2020-04-08 14:53:00,071 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@51df223b{/storage,null,AVAILABLE,@Spark}
2020-04-08 14:53:00,072 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@fd46303{/storage/json,null,AVAILABLE,@Spark}
2020-04-08 14:53:00,072 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60d8c0dc{/storage/rdd,null,AVAILABLE,@Spark}
2020-04-08 14:53:00,073 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4204541c{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-04-08 14:53:00,073 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a62689d{/environment,null,AVAILABLE,@Spark}
2020-04-08 14:53:00,073 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4602c2a9{/environment/json,null,AVAILABLE,@Spark}
2020-04-08 14:53:00,074 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60fa3495{/executors,null,AVAILABLE,@Spark}
2020-04-08 14:53:00,074 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3e2822{/executors/json,null,AVAILABLE,@Spark}
2020-04-08 14:53:00,075 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@79e18e38{/executors/threadDump,null,AVAILABLE,@Spark}
2020-04-08 14:53:00,075 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@29a60c27{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-04-08 14:53:00,080 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1849db1a{/static,null,AVAILABLE,@Spark}
2020-04-08 14:53:00,081 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2cf23c81{/,null,AVAILABLE,@Spark}
2020-04-08 14:53:00,082 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3624da92{/api,null,AVAILABLE,@Spark}
2020-04-08 14:53:00,082 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2484f433{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-04-08 14:53:00,082 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60b71e8f{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-04-08 14:53:00,084 INFO org.apache.spark.ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
2020-04-08 14:53:00,183 INFO org.apache.spark.executor.Executor: Starting executor ID driver on host localhost
2020-04-08 14:53:00,279 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 14:53:00,281 INFO org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 56142.
2020-04-08 14:53:00,282 INFO org.apache.spark.network.netty.NettyBlockTransferService: Server created on 192.168.0.102:56142
2020-04-08 14:53:00,285 INFO org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-04-08 14:53:00,324 INFO org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 56142, None)
2020-04-08 14:53:00,327 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:56142 with 4.1 GB RAM, BlockManagerId(driver, 192.168.0.102, 56142, None)
2020-04-08 14:53:00,329 INFO org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 56142, None)
2020-04-08 14:53:00,330 INFO org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 56142, None)
2020-04-08 14:53:00,517 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@502f8b57{/metrics/json,null,AVAILABLE,@Spark}
2020-04-08 14:53:00,586 INFO org.apache.spark.SparkContext: Invoking stop() from shutdown hook
2020-04-08 14:53:00,596 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@743cb8e0{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 14:53:00,597 INFO org.apache.spark.ui.SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
2020-04-08 14:53:00,608 INFO org.apache.spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2020-04-08 14:53:00,621 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2020-04-08 14:53:00,622 INFO org.apache.spark.storage.BlockManager: BlockManager stopped
2020-04-08 14:53:00,626 INFO org.apache.spark.storage.BlockManagerMaster: BlockManagerMaster stopped
2020-04-08 14:53:00,629 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2020-04-08 14:53:00,638 INFO org.apache.spark.SparkContext: Successfully stopped SparkContext
2020-04-08 14:53:00,638 INFO org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2020-04-08 14:53:00,639 INFO org.apache.spark.util.ShutdownHookManager: Deleting directory /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/spark-ec5d8eeb-1c34-4a59-92f9-544b06b02468
2020-04-08 14:56:48,783 WARN org.apache.spark.util.Utils: Your hostname, MacBook-Pro-CW.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)
2020-04-08 14:56:48,784 WARN org.apache.spark.util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
2020-04-08 14:56:48,811 INFO org.apache.spark.SparkContext: Running Spark version 2.4.0.cloudera2
2020-04-08 14:56:49,007 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-04-08 14:56:49,125 INFO org.apache.spark.SparkContext: Submitted application: Kafka2-kerberos
2020-04-08 14:56:49,173 INFO org.apache.spark.SecurityManager: Changing view acls to: godiswc
2020-04-08 14:56:49,173 INFO org.apache.spark.SecurityManager: Changing modify acls to: godiswc
2020-04-08 14:56:49,174 INFO org.apache.spark.SecurityManager: Changing view acls groups to: 
2020-04-08 14:56:49,174 INFO org.apache.spark.SecurityManager: Changing modify acls groups to: 
2020-04-08 14:56:49,175 INFO org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(godiswc); groups with view permissions: Set(); users  with modify permissions: Set(godiswc); groups with modify permissions: Set()
2020-04-08 14:56:49,307 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 14:56:49,376 INFO org.apache.spark.util.Utils: Successfully started service 'sparkDriver' on port 57000.
2020-04-08 14:56:49,391 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
2020-04-08 14:56:49,402 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
2020-04-08 14:56:49,403 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-04-08 14:56:49,404 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2020-04-08 14:56:49,415 INFO org.apache.spark.storage.DiskBlockManager: Created local directory at /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/blockmgr-86c378c7-bef1-4c97-92e0-f41dc4edeaaa
2020-04-08 14:56:49,445 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 4.1 GB
2020-04-08 14:56:49,453 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
2020-04-08 14:56:49,499 INFO org.spark_project.jetty.util.log: Logging initialized @1432ms
2020-04-08 14:56:49,542 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2019-04-30T04:42:08+08:00, git hash: e1bc35120a6617ee3df052294e433f3a25ce7097
2020-04-08 14:56:49,555 INFO org.spark_project.jetty.server.Server: Started @1489ms
2020-04-08 14:56:49,557 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 14:56:49,573 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@743cb8e0{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 14:56:49,576 INFO org.apache.spark.util.Utils: Successfully started service 'SparkUI' on port 4040.
2020-04-08 14:56:49,600 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@41a90fa8{/jobs,null,AVAILABLE,@Spark}
2020-04-08 14:56:49,601 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c67e137{/jobs/json,null,AVAILABLE,@Spark}
2020-04-08 14:56:49,601 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2c9399a4{/jobs/job,null,AVAILABLE,@Spark}
2020-04-08 14:56:49,604 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@53ab0286{/jobs/job/json,null,AVAILABLE,@Spark}
2020-04-08 14:56:49,605 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@63c5efee{/stages,null,AVAILABLE,@Spark}
2020-04-08 14:56:49,605 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2d10e0b1{/stages/json,null,AVAILABLE,@Spark}
2020-04-08 14:56:49,606 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1c98290c{/stages/stage,null,AVAILABLE,@Spark}
2020-04-08 14:56:49,607 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@71e5f61d{/stages/stage/json,null,AVAILABLE,@Spark}
2020-04-08 14:56:49,608 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2ce86164{/stages/pool,null,AVAILABLE,@Spark}
2020-04-08 14:56:49,608 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5e8f9e2d{/stages/pool/json,null,AVAILABLE,@Spark}
2020-04-08 14:56:49,609 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@51df223b{/storage,null,AVAILABLE,@Spark}
2020-04-08 14:56:49,609 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@fd46303{/storage/json,null,AVAILABLE,@Spark}
2020-04-08 14:56:49,610 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60d8c0dc{/storage/rdd,null,AVAILABLE,@Spark}
2020-04-08 14:56:49,610 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4204541c{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-04-08 14:56:49,611 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a62689d{/environment,null,AVAILABLE,@Spark}
2020-04-08 14:56:49,611 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4602c2a9{/environment/json,null,AVAILABLE,@Spark}
2020-04-08 14:56:49,612 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60fa3495{/executors,null,AVAILABLE,@Spark}
2020-04-08 14:56:49,612 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3e2822{/executors/json,null,AVAILABLE,@Spark}
2020-04-08 14:56:49,613 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@79e18e38{/executors/threadDump,null,AVAILABLE,@Spark}
2020-04-08 14:56:49,613 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@29a60c27{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-04-08 14:56:49,618 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1849db1a{/static,null,AVAILABLE,@Spark}
2020-04-08 14:56:49,619 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2cf23c81{/,null,AVAILABLE,@Spark}
2020-04-08 14:56:49,620 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3624da92{/api,null,AVAILABLE,@Spark}
2020-04-08 14:56:49,620 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2484f433{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-04-08 14:56:49,621 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60b71e8f{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-04-08 14:56:49,622 INFO org.apache.spark.ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
2020-04-08 14:56:49,696 INFO org.apache.spark.executor.Executor: Starting executor ID driver on host localhost
2020-04-08 14:56:49,742 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 14:56:49,743 INFO org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 57001.
2020-04-08 14:56:49,743 INFO org.apache.spark.network.netty.NettyBlockTransferService: Server created on 192.168.0.102:57001
2020-04-08 14:56:49,744 INFO org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-04-08 14:56:49,769 INFO org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 57001, None)
2020-04-08 14:56:49,773 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:57001 with 4.1 GB RAM, BlockManagerId(driver, 192.168.0.102, 57001, None)
2020-04-08 14:56:49,775 INFO org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 57001, None)
2020-04-08 14:56:49,775 INFO org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 57001, None)
2020-04-08 14:56:49,964 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@502f8b57{/metrics/json,null,AVAILABLE,@Spark}
2020-04-08 14:56:50,065 INFO org.apache.spark.SparkContext: Invoking stop() from shutdown hook
2020-04-08 14:56:50,080 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@743cb8e0{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 14:56:50,082 INFO org.apache.spark.ui.SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
2020-04-08 14:56:50,090 INFO org.apache.spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2020-04-08 14:56:50,102 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2020-04-08 14:56:50,103 INFO org.apache.spark.storage.BlockManager: BlockManager stopped
2020-04-08 14:56:50,107 INFO org.apache.spark.storage.BlockManagerMaster: BlockManagerMaster stopped
2020-04-08 14:56:50,109 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2020-04-08 14:56:50,119 INFO org.apache.spark.SparkContext: Successfully stopped SparkContext
2020-04-08 14:56:50,119 INFO org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2020-04-08 14:56:50,120 INFO org.apache.spark.util.ShutdownHookManager: Deleting directory /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/spark-14b7cc5b-ccdc-42d1-9bfe-5e80617d3963
2020-04-08 14:56:55,848 WARN org.apache.spark.util.Utils: Your hostname, MacBook-Pro-CW.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)
2020-04-08 14:56:55,848 WARN org.apache.spark.util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
2020-04-08 14:56:55,877 INFO org.apache.spark.SparkContext: Running Spark version 2.4.0.cloudera2
2020-04-08 14:56:56,068 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-04-08 14:56:56,181 INFO org.apache.spark.SparkContext: Submitted application: Kafka2SparkStreaming2Kudu-kerberos
2020-04-08 14:56:56,221 INFO org.apache.spark.SecurityManager: Changing view acls to: godiswc
2020-04-08 14:56:56,221 INFO org.apache.spark.SecurityManager: Changing modify acls to: godiswc
2020-04-08 14:56:56,222 INFO org.apache.spark.SecurityManager: Changing view acls groups to: 
2020-04-08 14:56:56,222 INFO org.apache.spark.SecurityManager: Changing modify acls groups to: 
2020-04-08 14:56:56,222 INFO org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(godiswc); groups with view permissions: Set(); users  with modify permissions: Set(godiswc); groups with modify permissions: Set()
2020-04-08 14:56:56,365 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 14:56:56,446 INFO org.apache.spark.util.Utils: Successfully started service 'sparkDriver' on port 57030.
2020-04-08 14:56:56,462 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
2020-04-08 14:56:56,473 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
2020-04-08 14:56:56,492 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-04-08 14:56:56,493 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2020-04-08 14:56:56,504 INFO org.apache.spark.storage.DiskBlockManager: Created local directory at /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/blockmgr-cff4b0a1-6c2c-4394-b3e4-86d3e42f364f
2020-04-08 14:56:56,516 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 4.1 GB
2020-04-08 14:56:56,525 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
2020-04-08 14:56:56,580 INFO org.spark_project.jetty.util.log: Logging initialized @1314ms
2020-04-08 14:56:56,641 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2019-04-30T04:42:08+08:00, git hash: e1bc35120a6617ee3df052294e433f3a25ce7097
2020-04-08 14:56:56,659 INFO org.spark_project.jetty.server.Server: Started @1394ms
2020-04-08 14:56:56,662 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 14:56:56,681 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@533b266e{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 14:56:56,681 INFO org.apache.spark.util.Utils: Successfully started service 'SparkUI' on port 4040.
2020-04-08 14:56:56,703 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6e521c1e{/jobs,null,AVAILABLE,@Spark}
2020-04-08 14:56:56,705 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62dae245{/jobs/json,null,AVAILABLE,@Spark}
2020-04-08 14:56:56,706 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4b6579e8{/jobs/job,null,AVAILABLE,@Spark}
2020-04-08 14:56:56,709 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@591e58fa{/jobs/job/json,null,AVAILABLE,@Spark}
2020-04-08 14:56:56,709 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3954d008{/stages,null,AVAILABLE,@Spark}
2020-04-08 14:56:56,710 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f94c4db{/stages/json,null,AVAILABLE,@Spark}
2020-04-08 14:56:56,710 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@593e824f{/stages/stage,null,AVAILABLE,@Spark}
2020-04-08 14:56:56,711 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@64bc21ac{/stages/stage/json,null,AVAILABLE,@Spark}
2020-04-08 14:56:56,712 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@493dfb8e{/stages/pool,null,AVAILABLE,@Spark}
2020-04-08 14:56:56,713 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5d25e6bb{/stages/pool/json,null,AVAILABLE,@Spark}
2020-04-08 14:56:56,713 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@ce5a68e{/storage,null,AVAILABLE,@Spark}
2020-04-08 14:56:56,714 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@9d157ff{/storage/json,null,AVAILABLE,@Spark}
2020-04-08 14:56:56,714 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f162cc0{/storage/rdd,null,AVAILABLE,@Spark}
2020-04-08 14:56:56,715 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5df417a7{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-04-08 14:56:56,715 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7c041b41{/environment,null,AVAILABLE,@Spark}
2020-04-08 14:56:56,716 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7f69d591{/environment/json,null,AVAILABLE,@Spark}
2020-04-08 14:56:56,716 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@61078690{/executors,null,AVAILABLE,@Spark}
2020-04-08 14:56:56,717 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1cb3ec38{/executors/json,null,AVAILABLE,@Spark}
2020-04-08 14:56:56,718 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@403132fc{/executors/threadDump,null,AVAILABLE,@Spark}
2020-04-08 14:56:56,718 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@71c5b236{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-04-08 14:56:56,724 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2cab9998{/static,null,AVAILABLE,@Spark}
2020-04-08 14:56:56,725 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@332a7fce{/,null,AVAILABLE,@Spark}
2020-04-08 14:56:56,726 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@549621f3{/api,null,AVAILABLE,@Spark}
2020-04-08 14:56:56,726 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@37ebc9d8{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-04-08 14:56:56,727 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@293bb8a5{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-04-08 14:56:56,729 INFO org.apache.spark.ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
2020-04-08 14:56:56,854 INFO org.apache.spark.executor.Executor: Starting executor ID driver on host localhost
2020-04-08 14:56:56,938 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 14:56:56,939 INFO org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 57031.
2020-04-08 14:56:56,939 INFO org.apache.spark.network.netty.NettyBlockTransferService: Server created on 192.168.0.102:57031
2020-04-08 14:56:56,941 INFO org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-04-08 14:56:56,962 INFO org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 57031, None)
2020-04-08 14:56:56,964 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:57031 with 4.1 GB RAM, BlockManagerId(driver, 192.168.0.102, 57031, None)
2020-04-08 14:56:56,966 INFO org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 57031, None)
2020-04-08 14:56:56,966 INFO org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 57031, None)
2020-04-08 14:56:57,134 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@54acff7d{/metrics/json,null,AVAILABLE,@Spark}
2020-04-08 14:56:57,208 WARN org.apache.spark.streaming.StreamingContext: spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
2020-04-08 14:56:57,398 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding enable.auto.commit to false for executor
2020-04-08 14:56:57,399 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding auto.offset.reset to none for executor
2020-04-08 14:56:57,399 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding executor group.id to spark-executor-testgroup1
2020-04-08 14:56:57,400 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
2020-04-08 14:56:59,176 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Slide time = 10000 ms
2020-04-08 14:56:59,177 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Storage level = Serialized 1x Replicated
2020-04-08 14:56:59,178 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Checkpoint interval = null
2020-04-08 14:56:59,179 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Remember interval = 10000 ms
2020-04-08 14:56:59,180 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@5f34274a
2020-04-08 14:56:59,180 INFO org.apache.spark.streaming.dstream.ForEachDStream: Slide time = 10000 ms
2020-04-08 14:56:59,180 INFO org.apache.spark.streaming.dstream.ForEachDStream: Storage level = Serialized 1x Replicated
2020-04-08 14:56:59,180 INFO org.apache.spark.streaming.dstream.ForEachDStream: Checkpoint interval = null
2020-04-08 14:56:59,180 INFO org.apache.spark.streaming.dstream.ForEachDStream: Remember interval = 10000 ms
2020-04-08 14:56:59,181 INFO org.apache.spark.streaming.dstream.ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@61eb12c6
2020-04-08 14:56:59,255 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = testgroup1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-08 14:56:59,344 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-08 14:56:59,345 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-08 14:56:59,345 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586329019343
2020-04-08 14:56:59,347 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-1, groupId=testgroup1] Subscribed to topic(s): PABUSI2
2020-04-08 14:57:00,121 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-1, groupId=testgroup1] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-08 14:57:00,123 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Discovered group coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 14:57:00,128 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Revoking previously assigned partitions []
2020-04-08 14:57:00,128 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 14:57:01,114 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 14:57:04,473 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Successfully joined group with generation 45
2020-04-08 14:57:04,477 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting newly assigned partitions: PABUSI2-2, PABUSI2-0, PABUSI2-1
2020-04-08 14:57:04,663 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-2 to the committed offset FetchPosition{offset=153, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-2.vpc.cloudera.com:9092 (id: 52 rack: null), epoch=0}}
2020-04-08 14:57:04,664 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-0 to the committed offset FetchPosition{offset=154, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-4.vpc.cloudera.com:9092 (id: 53 rack: null), epoch=2}}
2020-04-08 14:57:04,664 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-1 to the committed offset FetchPosition{offset=155, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-3.vpc.cloudera.com:9092 (id: 54 rack: null), epoch=2}}
2020-04-08 14:57:04,673 INFO org.apache.spark.streaming.util.RecurringTimer: Started timer for JobGenerator at time 1586329020000
2020-04-08 14:57:04,673 INFO org.apache.spark.streaming.scheduler.JobGenerator: Started JobGenerator at 1586329020000 ms
2020-04-08 14:57:04,674 INFO org.apache.spark.streaming.scheduler.JobScheduler: Started JobScheduler
2020-04-08 14:57:04,679 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@499ef98e{/streaming,null,AVAILABLE,@Spark}
2020-04-08 14:57:04,681 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@24934262{/streaming/json,null,AVAILABLE,@Spark}
2020-04-08 14:57:04,682 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@28501a4b{/streaming/batch,null,AVAILABLE,@Spark}
2020-04-08 14:57:04,682 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5b051a5c{/streaming/batch/json,null,AVAILABLE,@Spark}
2020-04-08 14:57:04,683 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2bebb74f{/static/streaming,null,AVAILABLE,@Spark}
2020-04-08 14:57:04,684 INFO org.apache.spark.streaming.StreamingContext: StreamingContext started
2020-04-08 14:57:04,718 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 14:57:04,718 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 14:57:04,718 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 14:57:05,713 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 155.
2020-04-08 14:57:05,713 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 153.
2020-04-08 14:57:05,714 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 154.
2020-04-08 14:57:05,737 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586329020000 ms
2020-04-08 14:57:05,740 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586329020000 ms.0 from job set of time 1586329020000 ms
2020-04-08 14:57:05,773 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:136
2020-04-08 14:57:05,790 INFO org.apache.spark.scheduler.DAGScheduler: Got job 0 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136) with 3 output partitions
2020-04-08 14:57:05,790 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 0 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136)
2020-04-08 14:57:05,790 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 14:57:05,791 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 14:57:05,795 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 0 (KafkaRDD[0] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113), which has no missing parents
2020-04-08 14:57:05,857 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 14:57:06,253 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.4 KB, free 4.1 GB)
2020-04-08 14:57:06,255 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:57031 (size: 3.4 KB, free: 4.1 GB)
2020-04-08 14:57:06,257 INFO org.apache.spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2020-04-08 14:57:06,268 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 0 (KafkaRDD[0] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 14:57:06,268 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 0.0 with 3 tasks
2020-04-08 14:57:06,304 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:57:06,312 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
2020-04-08 14:57:06,337 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 153 is the same as ending offset skipping PABUSI2 2
2020-04-08 14:57:06,348 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 2871 bytes result sent to driver
2020-04-08 14:57:06,350 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:57:06,351 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 0.0 (TID 1)
2020-04-08 14:57:06,354 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 60 ms on localhost (executor driver) (1/3)
2020-04-08 14:57:06,355 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 155 is the same as ending offset skipping PABUSI2 1
2020-04-08 14:57:06,356 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 0.0 (TID 1). 2828 bytes result sent to driver
2020-04-08 14:57:06,357 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:57:06,357 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 0.0 (TID 2)
2020-04-08 14:57:06,358 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 8 ms on localhost (executor driver) (2/3)
2020-04-08 14:57:06,361 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 154 is the same as ending offset skipping PABUSI2 0
2020-04-08 14:57:06,362 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 0.0 (TID 2). 2828 bytes result sent to driver
2020-04-08 14:57:06,365 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 8 ms on localhost (executor driver) (3/3)
2020-04-08 14:57:06,366 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-04-08 14:57:06,367 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 0 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136) finished in 0.555 s
2020-04-08 14:57:06,371 INFO org.apache.spark.scheduler.DAGScheduler: Job 0 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:136, took 0.598175 s
2020-04-08 14:57:06,375 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586329020000 ms.0 from job set of time 1586329020000 ms
2020-04-08 14:57:06,376 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 6.374 s for time 1586329020000 ms (execution: 0.635 s)
2020-04-08 14:57:06,380 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 14:57:06,384 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 
2020-04-08 14:57:10,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 14:57:10,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 14:57:10,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 14:57:10,178 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 155.
2020-04-08 14:57:10,179 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 154.
2020-04-08 14:57:10,179 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 154.
2020-04-08 14:57:10,180 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586329030000 ms
2020-04-08 14:57:10,180 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586329030000 ms.0 from job set of time 1586329030000 ms
2020-04-08 14:57:10,186 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:136
2020-04-08 14:57:10,186 INFO org.apache.spark.scheduler.DAGScheduler: Got job 1 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136) with 3 output partitions
2020-04-08 14:57:10,186 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 1 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136)
2020-04-08 14:57:10,186 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 14:57:10,187 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 14:57:10,187 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 1 (KafkaRDD[1] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113), which has no missing parents
2020-04-08 14:57:10,189 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 14:57:10,192 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 14:57:10,192 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.102:57031 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 14:57:10,193 INFO org.apache.spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2020-04-08 14:57:10,193 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 1 (KafkaRDD[1] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 14:57:10,193 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 1.0 with 3 tasks
2020-04-08 14:57:10,194 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:57:10,195 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 1.0 (TID 3)
2020-04-08 14:57:10,197 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Computing topic PABUSI2, partition 2 offsets 153 -> 154
2020-04-08 14:57:10,200 INFO org.apache.spark.streaming.kafka010.KafkaDataConsumer: Initializing cache 16 64 0.75
2020-04-08 14:57:10,202 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-testgroup1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-08 14:57:10,206 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-08 14:57:10,206 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-08 14:57:10,206 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586329030206
2020-04-08 14:57:10,206 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Subscribed to partition(s): PABUSI2-2
2020-04-08 14:57:10,209 INFO org.apache.spark.streaming.kafka010.InternalKafkaConsumer: Initial fetch for spark-executor-testgroup1 PABUSI2-2 153
2020-04-08 14:57:10,209 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Seeking to offset 153 for partition PABUSI2-2
2020-04-08 14:57:10,736 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-08 14:57:11,302 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.102:57031 in memory (size: 3.4 KB, free: 4.1 GB)
2020-04-08 14:57:11,307 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 8
2020-04-08 14:57:11,307 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 6
2020-04-08 14:57:11,307 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 14
2020-04-08 14:57:11,307 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 18
2020-04-08 14:57:11,307 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 21
2020-04-08 14:57:11,307 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 20
2020-04-08 14:57:11,307 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 19
2020-04-08 14:57:11,307 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 24
2020-04-08 14:57:11,307 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 15
2020-04-08 14:57:11,307 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 12
2020-04-08 14:57:11,308 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 13
2020-04-08 14:57:11,308 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 16
2020-04-08 14:57:11,308 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 23
2020-04-08 14:57:11,308 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 2
2020-04-08 14:57:11,308 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 17
2020-04-08 14:57:11,308 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 10
2020-04-08 14:57:11,308 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 9
2020-04-08 14:57:11,308 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 22
2020-04-08 14:57:11,308 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 4
2020-04-08 14:57:11,308 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 25
2020-04-08 14:57:11,308 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 1
2020-04-08 14:57:11,308 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 3
2020-04-08 14:57:11,309 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 7
2020-04-08 14:57:11,309 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 11
2020-04-08 14:57:11,309 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 5
2020-04-08 14:57:12,754 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (BASE_PREMIUM, ) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:592)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:57:12,756 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (MASTER_POLICY_ID, ) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:592)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:57:12,757 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (VAT, 220.75) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: "220.75"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:57:12,757 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (PRODUCT_ID, 22222222222) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: "22222222222"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:583)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:57:12,758 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (START_REVISION_ID, 7900000000000038) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: "7900000000000038"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:583)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:57:12,758 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (PRD_PKG_DISCOUNT_RATE, ) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:592)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:57:12,758 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (BELONG_TO_HANDLER_ID, 26004873143) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: "26004873143"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:583)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:57:12,759 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (C1_FEE_RATE, ) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:592)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:57:12,759 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (REGULAR_ENDO_DAYS_COUNT, ) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:592)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:57:12,759 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (STD_PREMIUM_LOCAL, ) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:592)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:57:12,760 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (UPDATE_BY, 260064029580813) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: "260064029580813"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:583)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:57:12,760 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (POLICY_ID0, 3003664443882041) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: "3003664443882041"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:583)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:57:12,760 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (BEFORE_VAT_PREMIUM_LOCAL, 3679.25) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: "3679.25"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:57:12,761 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (COMPY_VEH_TAX_PREMIUM, ) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:592)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:57:12,761 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (BASE_PREMIUM_LOCAL, ) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:592)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:57:12,761 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (APP_ID, ) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:592)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:57:12,762 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (TOTAL_DISCOUNT_RATE, ) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:592)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:57:12,762 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (GROSS_PREMIUM_LOCAL, 3899.97) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: "3899.97"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:57:12,762 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (PRD_PKG_DISCOUNT, ) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:592)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:57:12,763 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (C2_FEE_RATE, ) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:592)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:57:12,763 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (EXPECTED_ADJUSTED_RATE, ) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:592)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:57:12,764 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (STD_PREMIUM, ) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:592)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:57:12,764 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (ADJUSTED_PREMIUM_LOCAL, ) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:592)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:57:12,764 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (POLICY_ID, 9900000000000008) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: "9900000000000008"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:583)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:57:12,765 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (ISSUE_USER_ID, 26004917171) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: "26004917171"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:583)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:57:12,765 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (REVISION_TYPE, ) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:592)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:57:12,765 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (AGENT_ID, 26006983857) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: "26006983857"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:583)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:57:12,765 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (GROSS_PREMIUM, 3899.97) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: "3899.97"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:57:12,766 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (ORG_ID, 260060047158204) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: "260060047158204"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:583)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:57:12,766 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (VAT_RATE, ) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:592)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:57:12,766 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (VAT_LOCAL, 220.75) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: "220.75"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:57:12,767 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (EXPECTED_PREMIUM, ) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:592)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:57:12,767 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (INSERT_BY, 26004917171) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: "26004917171"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:583)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:57:12,767 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (SMS_TEMPLATE_ID, ) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:592)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:57:12,767 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (C_FEE_RATE, ) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:592)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:57:12,768 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (BEFORE_VAT_PREMIUM, 3679.25) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: "3679.25"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:57:12,768 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (MP_TOTAL_PREPAY_PREMIUM, ) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:592)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:57:12,768 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (CESSION_BASIS_CODE, ) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:592)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:57:12,768 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (ISSUE_ORG_ID, 260059653500187) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: "260059653500187"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:583)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:57:12,769 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (SUBMISSION_VER_ID, ) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:592)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:57:12,769 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (MP_TOTAL_SUM_INSURED, ) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:592)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:57:12,770 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (SUBMISSION_ID, ) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:592)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:57:12,774 ERROR org.apache.spark.executor.Executor: Exception in task 0.0 in stage 1.0 (TID 3)
org.apache.kudu.client.NonRecoverableException: Primary key column policy_id is not set
	at org.apache.kudu.client.KuduException.transformException(KuduException.java:132)
	at org.apache.kudu.client.KuduSession.apply(KuduSession.java:94)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:199)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalStateException: Primary key column policy_id is not set
	at org.apache.kudu.client.KeyEncoder.encodeColumn(KeyEncoder.java:143)
	at org.apache.kudu.client.KeyEncoder.encodeColumns(KeyEncoder.java:126)
	at org.apache.kudu.client.KeyEncoder.getHashBucket(KeyEncoder.java:80)
	at org.apache.kudu.client.KeyEncoder.encodePartitionKey(KeyEncoder.java:96)
	at org.apache.kudu.client.PartitionSchema.encodePartitionKey(PartitionSchema.java:84)
	at org.apache.kudu.client.Operation.partitionKey(Operation.java:164)
	at org.apache.kudu.client.AsyncKuduSession.apply(AsyncKuduSession.java:560)
	at org.apache.kudu.client.KuduSession.apply(KuduSession.java:80)
	... 19 more
2020-04-08 14:57:12,797 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 4, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:57:12,797 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 1.0 (TID 4)
2020-04-08 14:57:12,799 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 1.0 (TID 3, localhost, executor driver): org.apache.kudu.client.NonRecoverableException: Primary key column policy_id is not set
	at org.apache.kudu.client.KuduException.transformException(KuduException.java:132)
	at org.apache.kudu.client.KuduSession.apply(KuduSession.java:94)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:199)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalStateException: Primary key column policy_id is not set
	at org.apache.kudu.client.KeyEncoder.encodeColumn(KeyEncoder.java:143)
	at org.apache.kudu.client.KeyEncoder.encodeColumns(KeyEncoder.java:126)
	at org.apache.kudu.client.KeyEncoder.getHashBucket(KeyEncoder.java:80)
	at org.apache.kudu.client.KeyEncoder.encodePartitionKey(KeyEncoder.java:96)
	at org.apache.kudu.client.PartitionSchema.encodePartitionKey(PartitionSchema.java:84)
	at org.apache.kudu.client.Operation.partitionKey(Operation.java:164)
	at org.apache.kudu.client.AsyncKuduSession.apply(AsyncKuduSession.java:560)
	at org.apache.kudu.client.KuduSession.apply(KuduSession.java:80)
	... 19 more

2020-04-08 14:57:12,799 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 155 is the same as ending offset skipping PABUSI2 1
2020-04-08 14:57:12,800 ERROR org.apache.spark.scheduler.TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job
2020-04-08 14:57:12,801 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 1.0 (TID 4). 2828 bytes result sent to driver
2020-04-08 14:57:12,802 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 4) in 5 ms on localhost (executor driver) (1/3)
2020-04-08 14:57:12,802 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
2020-04-08 14:57:12,803 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Cancelling stage 1
2020-04-08 14:57:12,803 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Killing all running tasks in stage 1: Stage cancelled
2020-04-08 14:57:12,804 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 1 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136) failed in 2.616 s due to Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 3, localhost, executor driver): org.apache.kudu.client.NonRecoverableException: Primary key column policy_id is not set
	at org.apache.kudu.client.KuduException.transformException(KuduException.java:132)
	at org.apache.kudu.client.KuduSession.apply(KuduSession.java:94)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:199)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalStateException: Primary key column policy_id is not set
	at org.apache.kudu.client.KeyEncoder.encodeColumn(KeyEncoder.java:143)
	at org.apache.kudu.client.KeyEncoder.encodeColumns(KeyEncoder.java:126)
	at org.apache.kudu.client.KeyEncoder.getHashBucket(KeyEncoder.java:80)
	at org.apache.kudu.client.KeyEncoder.encodePartitionKey(KeyEncoder.java:96)
	at org.apache.kudu.client.PartitionSchema.encodePartitionKey(PartitionSchema.java:84)
	at org.apache.kudu.client.Operation.partitionKey(Operation.java:164)
	at org.apache.kudu.client.AsyncKuduSession.apply(AsyncKuduSession.java:560)
	at org.apache.kudu.client.KuduSession.apply(KuduSession.java:80)
	... 19 more

Driver stacktrace:
2020-04-08 14:57:12,805 INFO org.apache.spark.scheduler.DAGScheduler: Job 1 failed: foreachPartition at Kafka2SparkStreaming2Kudu.scala:136, took 2.618651 s
2020-04-08 14:57:12,805 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586329030000 ms.0 from job set of time 1586329030000 ms
2020-04-08 14:57:12,807 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1586329030000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 3, localhost, executor driver): org.apache.kudu.client.NonRecoverableException: Primary key column policy_id is not set
	at org.apache.kudu.client.KuduException.transformException(KuduException.java:132)
	at org.apache.kudu.client.KuduSession.apply(KuduSession.java:94)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:199)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalStateException: Primary key column policy_id is not set
	at org.apache.kudu.client.KeyEncoder.encodeColumn(KeyEncoder.java:143)
	at org.apache.kudu.client.KeyEncoder.encodeColumns(KeyEncoder.java:126)
	at org.apache.kudu.client.KeyEncoder.getHashBucket(KeyEncoder.java:80)
	at org.apache.kudu.client.KeyEncoder.encodePartitionKey(KeyEncoder.java:96)
	at org.apache.kudu.client.PartitionSchema.encodePartitionKey(PartitionSchema.java:84)
	at org.apache.kudu.client.Operation.partitionKey(Operation.java:164)
	at org.apache.kudu.client.AsyncKuduSession.apply(AsyncKuduSession.java:560)
	at org.apache.kudu.client.KuduSession.apply(KuduSession.java:80)
	... 19 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:245)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:933)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:933)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1.apply(Kafka2SparkStreaming2Kudu.scala:131)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:191)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:57:12,815 INFO org.apache.spark.streaming.StreamingContext: Invoking stop(stopGracefully=false) from shutdown hook
2020-04-08 14:57:12,817 INFO org.apache.spark.streaming.scheduler.ReceiverTracker: ReceiverTracker stopped
2020-04-08 14:57:12,817 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopping JobGenerator immediately
2020-04-08 14:57:12,818 INFO org.apache.spark.streaming.util.RecurringTimer: Stopped timer for JobGenerator after time 1586329030000
2020-04-08 14:57:12,994 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Member consumer-1-debc093e-c942-49d5-99ed-082db3554fa8 sending LeaveGroup request to coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 14:57:13,175 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopped JobGenerator
2020-04-08 14:57:13,178 INFO org.apache.spark.streaming.scheduler.JobScheduler: Stopped JobScheduler
2020-04-08 14:57:13,182 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@499ef98e{/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 14:57:13,182 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@28501a4b{/streaming/batch,null,UNAVAILABLE,@Spark}
2020-04-08 14:57:13,183 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@2bebb74f{/static/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 14:57:13,184 INFO org.apache.spark.streaming.StreamingContext: StreamingContext stopped successfully
2020-04-08 14:57:13,184 INFO org.apache.spark.SparkContext: Invoking stop() from shutdown hook
2020-04-08 14:57:13,189 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@533b266e{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 14:57:13,191 INFO org.apache.spark.ui.SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
2020-04-08 14:57:13,198 INFO org.apache.spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2020-04-08 14:57:13,222 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2020-04-08 14:57:13,224 INFO org.apache.spark.storage.BlockManager: BlockManager stopped
2020-04-08 14:57:13,225 INFO org.apache.spark.storage.BlockManagerMaster: BlockManagerMaster stopped
2020-04-08 14:57:13,227 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2020-04-08 14:57:13,244 INFO org.apache.spark.SparkContext: Successfully stopped SparkContext
2020-04-08 14:57:13,245 INFO org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2020-04-08 14:57:13,246 INFO org.apache.spark.util.ShutdownHookManager: Deleting directory /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/spark-48c9ab4e-a6e0-4d0d-a3e0-9db97e3118f0
2020-04-08 14:58:52,336 WARN org.apache.spark.util.Utils: Your hostname, MacBook-Pro-CW.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)
2020-04-08 14:58:52,337 WARN org.apache.spark.util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
2020-04-08 14:58:52,371 INFO org.apache.spark.SparkContext: Running Spark version 2.4.0.cloudera2
2020-04-08 14:58:52,568 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-04-08 14:58:52,679 INFO org.apache.spark.SparkContext: Submitted application: Kafka2SparkStreaming2Kudu-kerberos
2020-04-08 14:58:52,721 INFO org.apache.spark.SecurityManager: Changing view acls to: godiswc
2020-04-08 14:58:52,721 INFO org.apache.spark.SecurityManager: Changing modify acls to: godiswc
2020-04-08 14:58:52,721 INFO org.apache.spark.SecurityManager: Changing view acls groups to: 
2020-04-08 14:58:52,722 INFO org.apache.spark.SecurityManager: Changing modify acls groups to: 
2020-04-08 14:58:52,722 INFO org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(godiswc); groups with view permissions: Set(); users  with modify permissions: Set(godiswc); groups with modify permissions: Set()
2020-04-08 14:58:52,853 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 14:58:52,948 INFO org.apache.spark.util.Utils: Successfully started service 'sparkDriver' on port 57471.
2020-04-08 14:58:52,965 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
2020-04-08 14:58:52,976 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
2020-04-08 14:58:52,998 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-04-08 14:58:52,998 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2020-04-08 14:58:53,011 INFO org.apache.spark.storage.DiskBlockManager: Created local directory at /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/blockmgr-464efa8b-d434-4620-9595-bc875000931a
2020-04-08 14:58:53,025 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 4.1 GB
2020-04-08 14:58:53,034 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
2020-04-08 14:58:53,099 INFO org.spark_project.jetty.util.log: Logging initialized @1503ms
2020-04-08 14:58:53,168 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2019-04-30T04:42:08+08:00, git hash: e1bc35120a6617ee3df052294e433f3a25ce7097
2020-04-08 14:58:53,185 INFO org.spark_project.jetty.server.Server: Started @1590ms
2020-04-08 14:58:53,188 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 14:58:53,213 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@533b266e{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 14:58:53,214 INFO org.apache.spark.util.Utils: Successfully started service 'SparkUI' on port 4040.
2020-04-08 14:58:53,248 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6e521c1e{/jobs,null,AVAILABLE,@Spark}
2020-04-08 14:58:53,249 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62dae245{/jobs/json,null,AVAILABLE,@Spark}
2020-04-08 14:58:53,249 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4b6579e8{/jobs/job,null,AVAILABLE,@Spark}
2020-04-08 14:58:53,254 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@591e58fa{/jobs/job/json,null,AVAILABLE,@Spark}
2020-04-08 14:58:53,255 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3954d008{/stages,null,AVAILABLE,@Spark}
2020-04-08 14:58:53,256 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f94c4db{/stages/json,null,AVAILABLE,@Spark}
2020-04-08 14:58:53,256 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@593e824f{/stages/stage,null,AVAILABLE,@Spark}
2020-04-08 14:58:53,260 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@64bc21ac{/stages/stage/json,null,AVAILABLE,@Spark}
2020-04-08 14:58:53,262 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@493dfb8e{/stages/pool,null,AVAILABLE,@Spark}
2020-04-08 14:58:53,262 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5d25e6bb{/stages/pool/json,null,AVAILABLE,@Spark}
2020-04-08 14:58:53,263 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@ce5a68e{/storage,null,AVAILABLE,@Spark}
2020-04-08 14:58:53,264 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@9d157ff{/storage/json,null,AVAILABLE,@Spark}
2020-04-08 14:58:53,265 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f162cc0{/storage/rdd,null,AVAILABLE,@Spark}
2020-04-08 14:58:53,266 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5df417a7{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-04-08 14:58:53,267 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7c041b41{/environment,null,AVAILABLE,@Spark}
2020-04-08 14:58:53,268 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7f69d591{/environment/json,null,AVAILABLE,@Spark}
2020-04-08 14:58:53,269 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@61078690{/executors,null,AVAILABLE,@Spark}
2020-04-08 14:58:53,270 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1cb3ec38{/executors/json,null,AVAILABLE,@Spark}
2020-04-08 14:58:53,271 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@403132fc{/executors/threadDump,null,AVAILABLE,@Spark}
2020-04-08 14:58:53,272 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@71c5b236{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-04-08 14:58:53,279 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2cab9998{/static,null,AVAILABLE,@Spark}
2020-04-08 14:58:53,280 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@332a7fce{/,null,AVAILABLE,@Spark}
2020-04-08 14:58:53,281 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@549621f3{/api,null,AVAILABLE,@Spark}
2020-04-08 14:58:53,281 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@37ebc9d8{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-04-08 14:58:53,282 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@293bb8a5{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-04-08 14:58:53,283 INFO org.apache.spark.ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
2020-04-08 14:58:53,459 INFO org.apache.spark.executor.Executor: Starting executor ID driver on host localhost
2020-04-08 14:58:53,529 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 14:58:53,530 INFO org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 57476.
2020-04-08 14:58:53,531 INFO org.apache.spark.network.netty.NettyBlockTransferService: Server created on 192.168.0.102:57476
2020-04-08 14:58:53,532 INFO org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-04-08 14:58:53,561 INFO org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 57476, None)
2020-04-08 14:58:53,564 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:57476 with 4.1 GB RAM, BlockManagerId(driver, 192.168.0.102, 57476, None)
2020-04-08 14:58:53,566 INFO org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 57476, None)
2020-04-08 14:58:53,567 INFO org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 57476, None)
2020-04-08 14:58:53,888 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@54acff7d{/metrics/json,null,AVAILABLE,@Spark}
2020-04-08 14:58:53,963 WARN org.apache.spark.streaming.StreamingContext: spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
2020-04-08 14:58:54,226 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding enable.auto.commit to false for executor
2020-04-08 14:58:54,227 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding auto.offset.reset to none for executor
2020-04-08 14:58:54,228 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding executor group.id to spark-executor-testgroup1
2020-04-08 14:58:54,228 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
2020-04-08 14:58:56,053 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Slide time = 10000 ms
2020-04-08 14:58:56,053 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Storage level = Serialized 1x Replicated
2020-04-08 14:58:56,054 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Checkpoint interval = null
2020-04-08 14:58:56,054 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Remember interval = 10000 ms
2020-04-08 14:58:56,054 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@54202661
2020-04-08 14:58:56,054 INFO org.apache.spark.streaming.dstream.ForEachDStream: Slide time = 10000 ms
2020-04-08 14:58:56,054 INFO org.apache.spark.streaming.dstream.ForEachDStream: Storage level = Serialized 1x Replicated
2020-04-08 14:58:56,054 INFO org.apache.spark.streaming.dstream.ForEachDStream: Checkpoint interval = null
2020-04-08 14:58:56,055 INFO org.apache.spark.streaming.dstream.ForEachDStream: Remember interval = 10000 ms
2020-04-08 14:58:56,055 INFO org.apache.spark.streaming.dstream.ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@104b8677
2020-04-08 14:58:56,105 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = testgroup1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-08 14:58:56,155 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-08 14:58:56,156 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-08 14:58:56,156 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586329136154
2020-04-08 14:58:56,157 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-1, groupId=testgroup1] Subscribed to topic(s): PABUSI2
2020-04-08 14:58:56,851 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-1, groupId=testgroup1] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-08 14:58:56,852 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Discovered group coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 14:58:56,856 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Revoking previously assigned partitions []
2020-04-08 14:58:56,856 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 14:58:57,383 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 14:59:00,785 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Successfully joined group with generation 47
2020-04-08 14:59:00,790 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting newly assigned partitions: PABUSI2-2, PABUSI2-0, PABUSI2-1
2020-04-08 14:59:00,979 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-2 to the committed offset FetchPosition{offset=154, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-2.vpc.cloudera.com:9092 (id: 52 rack: null), epoch=0}}
2020-04-08 14:59:00,981 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-0 to the committed offset FetchPosition{offset=154, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-4.vpc.cloudera.com:9092 (id: 53 rack: null), epoch=2}}
2020-04-08 14:59:00,981 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-1 to the committed offset FetchPosition{offset=155, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-3.vpc.cloudera.com:9092 (id: 54 rack: null), epoch=2}}
2020-04-08 14:59:00,989 INFO org.apache.spark.streaming.util.RecurringTimer: Started timer for JobGenerator at time 1586329140000
2020-04-08 14:59:00,990 INFO org.apache.spark.streaming.scheduler.JobGenerator: Started JobGenerator at 1586329140000 ms
2020-04-08 14:59:00,991 INFO org.apache.spark.streaming.scheduler.JobScheduler: Started JobScheduler
2020-04-08 14:59:00,997 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@499ef98e{/streaming,null,AVAILABLE,@Spark}
2020-04-08 14:59:00,998 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@24934262{/streaming/json,null,AVAILABLE,@Spark}
2020-04-08 14:59:01,000 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@28501a4b{/streaming/batch,null,AVAILABLE,@Spark}
2020-04-08 14:59:01,001 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5b051a5c{/streaming/batch/json,null,AVAILABLE,@Spark}
2020-04-08 14:59:01,001 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2bebb74f{/static/streaming,null,AVAILABLE,@Spark}
2020-04-08 14:59:01,002 INFO org.apache.spark.streaming.StreamingContext: StreamingContext started
2020-04-08 14:59:01,041 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 14:59:01,041 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 14:59:01,041 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 14:59:01,701 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 155.
2020-04-08 14:59:02,037 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 154.
2020-04-08 14:59:02,037 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 155.
2020-04-08 14:59:02,059 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586329140000 ms
2020-04-08 14:59:02,062 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586329140000 ms.0 from job set of time 1586329140000 ms
2020-04-08 14:59:02,092 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:136
2020-04-08 14:59:02,106 INFO org.apache.spark.scheduler.DAGScheduler: Got job 0 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136) with 3 output partitions
2020-04-08 14:59:02,106 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 0 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136)
2020-04-08 14:59:02,106 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 14:59:02,107 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 14:59:02,110 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 0 (KafkaRDD[0] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113), which has no missing parents
2020-04-08 14:59:02,169 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 14:59:02,989 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 14:59:02,990 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:57476 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 14:59:02,992 INFO org.apache.spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2020-04-08 14:59:03,002 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 0 (KafkaRDD[0] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 14:59:03,003 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 0.0 with 3 tasks
2020-04-08 14:59:03,037 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:59:03,044 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
2020-04-08 14:59:03,066 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Computing topic PABUSI2, partition 2 offsets 154 -> 155
2020-04-08 14:59:03,069 INFO org.apache.spark.streaming.kafka010.KafkaDataConsumer: Initializing cache 16 64 0.75
2020-04-08 14:59:03,071 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-testgroup1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-08 14:59:03,076 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-08 14:59:03,076 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-08 14:59:03,076 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586329143076
2020-04-08 14:59:03,077 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Subscribed to partition(s): PABUSI2-2
2020-04-08 14:59:03,079 INFO org.apache.spark.streaming.kafka010.InternalKafkaConsumer: Initial fetch for spark-executor-testgroup1 PABUSI2-2 154
2020-04-08 14:59:03,080 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Seeking to offset 154 for partition PABUSI2-2
2020-04-08 14:59:03,610 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-08 14:59:04,935 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (VAT, 220.75) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: "220.75"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:59:04,937 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (PRODUCT_ID, 22222222222) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: "22222222222"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:583)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:59:04,937 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (START_REVISION_ID, 7900000000000038) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: "7900000000000038"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:583)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:59:04,938 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (BELONG_TO_HANDLER_ID, 26004873143) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: "26004873143"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:583)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:59:04,939 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (UPDATE_BY, 260064029580813) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: "260064029580813"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:583)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:59:04,939 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (POLICY_ID0, 3003664443882041) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: "3003664443882041"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:583)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:59:04,940 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (BEFORE_VAT_PREMIUM_LOCAL, 3679.25) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: "3679.25"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:59:04,940 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (GROSS_PREMIUM_LOCAL, 3899.97) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: "3899.97"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:59:04,941 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (POLICY_ID, 9900000000000008) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: "9900000000000008"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:583)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:59:04,941 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (ISSUE_USER_ID, 26004917171) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: "26004917171"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:583)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:59:04,942 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (AGENT_ID, 26006983857) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: "26006983857"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:583)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:59:04,942 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (GROSS_PREMIUM, 3899.97) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: "3899.97"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:59:04,942 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (ORG_ID, 260060047158204) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: "260060047158204"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:583)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:59:04,943 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (VAT_LOCAL, 220.75) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: "220.75"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:59:04,943 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (INSERT_BY, 26004917171) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: "26004917171"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:583)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:59:04,944 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (BEFORE_VAT_PREMIUM, 3679.25) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: "3679.25"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:59:04,944 WARN com.cloudera.streaming.Kafka2SparkStreaming2Kudu$: Will not add (ISSUE_ORG_ID, 260059653500187) for impala::ccic_kudu_sj_dev.T1_PA_PL_POLICY_L
java.lang.NumberFormatException: For input string: "260059653500187"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:583)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:241)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.addColumnData(Kafka2SparkStreaming2Kudu.scala:211)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:191)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$insert$1.apply(Kafka2SparkStreaming2Kudu.scala:189)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:189)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:59:04,952 ERROR org.apache.spark.executor.Executor: Exception in task 0.0 in stage 0.0 (TID 0)
org.apache.kudu.client.NonRecoverableException: Primary key column policy_id is not set
	at org.apache.kudu.client.KuduException.transformException(KuduException.java:132)
	at org.apache.kudu.client.KuduSession.apply(KuduSession.java:94)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:199)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalStateException: Primary key column policy_id is not set
	at org.apache.kudu.client.KeyEncoder.encodeColumn(KeyEncoder.java:143)
	at org.apache.kudu.client.KeyEncoder.encodeColumns(KeyEncoder.java:126)
	at org.apache.kudu.client.KeyEncoder.getHashBucket(KeyEncoder.java:80)
	at org.apache.kudu.client.KeyEncoder.encodePartitionKey(KeyEncoder.java:96)
	at org.apache.kudu.client.PartitionSchema.encodePartitionKey(PartitionSchema.java:84)
	at org.apache.kudu.client.Operation.partitionKey(Operation.java:164)
	at org.apache.kudu.client.AsyncKuduSession.apply(AsyncKuduSession.java:560)
	at org.apache.kudu.client.KuduSession.apply(KuduSession.java:80)
	... 19 more
2020-04-08 14:59:04,974 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 14:59:04,974 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 0.0 (TID 1)
2020-04-08 14:59:04,976 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): org.apache.kudu.client.NonRecoverableException: Primary key column policy_id is not set
	at org.apache.kudu.client.KuduException.transformException(KuduException.java:132)
	at org.apache.kudu.client.KuduSession.apply(KuduSession.java:94)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:199)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalStateException: Primary key column policy_id is not set
	at org.apache.kudu.client.KeyEncoder.encodeColumn(KeyEncoder.java:143)
	at org.apache.kudu.client.KeyEncoder.encodeColumns(KeyEncoder.java:126)
	at org.apache.kudu.client.KeyEncoder.getHashBucket(KeyEncoder.java:80)
	at org.apache.kudu.client.KeyEncoder.encodePartitionKey(KeyEncoder.java:96)
	at org.apache.kudu.client.PartitionSchema.encodePartitionKey(PartitionSchema.java:84)
	at org.apache.kudu.client.Operation.partitionKey(Operation.java:164)
	at org.apache.kudu.client.AsyncKuduSession.apply(AsyncKuduSession.java:560)
	at org.apache.kudu.client.KuduSession.apply(KuduSession.java:80)
	... 19 more

2020-04-08 14:59:04,978 ERROR org.apache.spark.scheduler.TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job
2020-04-08 14:59:04,978 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 155 is the same as ending offset skipping PABUSI2 1
2020-04-08 14:59:04,981 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 0.0 (TID 1). 2828 bytes result sent to driver
2020-04-08 14:59:04,983 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Cancelling stage 0
2020-04-08 14:59:04,983 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Killing all running tasks in stage 0: Stage cancelled
2020-04-08 14:59:04,984 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-04-08 14:59:04,985 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Stage 0 was cancelled
2020-04-08 14:59:04,986 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 0 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136) failed in 2.862 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): org.apache.kudu.client.NonRecoverableException: Primary key column policy_id is not set
	at org.apache.kudu.client.KuduException.transformException(KuduException.java:132)
	at org.apache.kudu.client.KuduSession.apply(KuduSession.java:94)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:199)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalStateException: Primary key column policy_id is not set
	at org.apache.kudu.client.KeyEncoder.encodeColumn(KeyEncoder.java:143)
	at org.apache.kudu.client.KeyEncoder.encodeColumns(KeyEncoder.java:126)
	at org.apache.kudu.client.KeyEncoder.getHashBucket(KeyEncoder.java:80)
	at org.apache.kudu.client.KeyEncoder.encodePartitionKey(KeyEncoder.java:96)
	at org.apache.kudu.client.PartitionSchema.encodePartitionKey(PartitionSchema.java:84)
	at org.apache.kudu.client.Operation.partitionKey(Operation.java:164)
	at org.apache.kudu.client.AsyncKuduSession.apply(AsyncKuduSession.java:560)
	at org.apache.kudu.client.KuduSession.apply(KuduSession.java:80)
	... 19 more

Driver stacktrace:
2020-04-08 14:59:04,986 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 12 ms on localhost (executor driver) (1/3)
2020-04-08 14:59:04,987 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-04-08 14:59:04,989 INFO org.apache.spark.scheduler.DAGScheduler: Job 0 failed: foreachPartition at Kafka2SparkStreaming2Kudu.scala:136, took 2.897295 s
2020-04-08 14:59:04,991 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586329140000 ms.0 from job set of time 1586329140000 ms
2020-04-08 14:59:04,992 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1586329140000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): org.apache.kudu.client.NonRecoverableException: Primary key column policy_id is not set
	at org.apache.kudu.client.KuduException.transformException(KuduException.java:132)
	at org.apache.kudu.client.KuduSession.apply(KuduSession.java:94)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$.insert(Kafka2SparkStreaming2Kudu.scala:199)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:156)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1$$anonfun$apply$2.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at scala.collection.Iterator$class.foreach(Iterator.scala:743)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.foreach(KafkaRDD.scala:229)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:138)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1$$anonfun$apply$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalStateException: Primary key column policy_id is not set
	at org.apache.kudu.client.KeyEncoder.encodeColumn(KeyEncoder.java:143)
	at org.apache.kudu.client.KeyEncoder.encodeColumns(KeyEncoder.java:126)
	at org.apache.kudu.client.KeyEncoder.getHashBucket(KeyEncoder.java:80)
	at org.apache.kudu.client.KeyEncoder.encodePartitionKey(KeyEncoder.java:96)
	at org.apache.kudu.client.PartitionSchema.encodePartitionKey(PartitionSchema.java:84)
	at org.apache.kudu.client.Operation.partitionKey(Operation.java:164)
	at org.apache.kudu.client.AsyncKuduSession.apply(AsyncKuduSession.java:560)
	at org.apache.kudu.client.KuduSession.apply(KuduSession.java:80)
	... 19 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:245)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:933)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:933)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1.apply(Kafka2SparkStreaming2Kudu.scala:136)
	at com.cloudera.streaming.Kafka2SparkStreaming2Kudu$$anonfun$main$1.apply(Kafka2SparkStreaming2Kudu.scala:131)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:191)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-04-08 14:59:05,004 INFO org.apache.spark.streaming.StreamingContext: Invoking stop(stopGracefully=false) from shutdown hook
2020-04-08 14:59:05,005 INFO org.apache.spark.streaming.scheduler.ReceiverTracker: ReceiverTracker stopped
2020-04-08 14:59:05,006 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopping JobGenerator immediately
2020-04-08 14:59:05,007 INFO org.apache.spark.streaming.util.RecurringTimer: Stopped timer for JobGenerator after time 1586329140000
2020-04-08 14:59:05,186 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Member consumer-1-0d1a1adf-37ad-4891-946b-31be65958d93 sending LeaveGroup request to coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 14:59:05,368 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopped JobGenerator
2020-04-08 14:59:05,371 INFO org.apache.spark.streaming.scheduler.JobScheduler: Stopped JobScheduler
2020-04-08 14:59:05,375 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@499ef98e{/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 14:59:05,375 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@28501a4b{/streaming/batch,null,UNAVAILABLE,@Spark}
2020-04-08 14:59:05,376 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@2bebb74f{/static/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 14:59:05,376 INFO org.apache.spark.streaming.StreamingContext: StreamingContext stopped successfully
2020-04-08 14:59:05,377 INFO org.apache.spark.SparkContext: Invoking stop() from shutdown hook
2020-04-08 14:59:05,380 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@533b266e{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 14:59:05,381 INFO org.apache.spark.ui.SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
2020-04-08 14:59:05,386 INFO org.apache.spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2020-04-08 14:59:05,399 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2020-04-08 14:59:05,399 INFO org.apache.spark.storage.BlockManager: BlockManager stopped
2020-04-08 14:59:05,399 INFO org.apache.spark.storage.BlockManagerMaster: BlockManagerMaster stopped
2020-04-08 14:59:05,401 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2020-04-08 14:59:05,411 INFO org.apache.spark.SparkContext: Successfully stopped SparkContext
2020-04-08 14:59:05,412 INFO org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2020-04-08 14:59:05,413 INFO org.apache.spark.util.ShutdownHookManager: Deleting directory /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/spark-585c9d8e-c5c9-43fe-b91f-032494e2d213
2020-04-08 15:08:05,952 WARN org.apache.spark.util.Utils: Your hostname, MacBook-Pro-CW.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)
2020-04-08 15:08:05,955 WARN org.apache.spark.util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
2020-04-08 15:08:06,147 INFO org.apache.spark.SparkContext: Running Spark version 2.4.0.cloudera2
2020-04-08 15:08:06,402 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-04-08 15:08:06,530 INFO org.apache.spark.SparkContext: Submitted application: Kafka2-kerberos
2020-04-08 15:08:06,580 INFO org.apache.spark.SecurityManager: Changing view acls to: godiswc
2020-04-08 15:08:06,581 INFO org.apache.spark.SecurityManager: Changing modify acls to: godiswc
2020-04-08 15:08:06,581 INFO org.apache.spark.SecurityManager: Changing view acls groups to: 
2020-04-08 15:08:06,581 INFO org.apache.spark.SecurityManager: Changing modify acls groups to: 
2020-04-08 15:08:06,582 INFO org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(godiswc); groups with view permissions: Set(); users  with modify permissions: Set(godiswc); groups with modify permissions: Set()
2020-04-08 15:08:06,741 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 15:08:06,828 INFO org.apache.spark.util.Utils: Successfully started service 'sparkDriver' on port 59782.
2020-04-08 15:08:06,846 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
2020-04-08 15:08:06,859 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
2020-04-08 15:08:06,861 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-04-08 15:08:06,861 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2020-04-08 15:08:06,873 INFO org.apache.spark.storage.DiskBlockManager: Created local directory at /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/blockmgr-a0f81d66-df49-4fac-b26c-89ea31628a29
2020-04-08 15:08:06,912 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 4.1 GB
2020-04-08 15:08:06,922 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
2020-04-08 15:08:06,991 INFO org.spark_project.jetty.util.log: Logging initialized @1800ms
2020-04-08 15:08:07,040 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2019-04-30T04:42:08+08:00, git hash: e1bc35120a6617ee3df052294e433f3a25ce7097
2020-04-08 15:08:07,052 INFO org.spark_project.jetty.server.Server: Started @1863ms
2020-04-08 15:08:07,054 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 15:08:07,065 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@743cb8e0{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 15:08:07,065 INFO org.apache.spark.util.Utils: Successfully started service 'SparkUI' on port 4040.
2020-04-08 15:08:07,085 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@41a90fa8{/jobs,null,AVAILABLE,@Spark}
2020-04-08 15:08:07,086 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c67e137{/jobs/json,null,AVAILABLE,@Spark}
2020-04-08 15:08:07,086 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2c9399a4{/jobs/job,null,AVAILABLE,@Spark}
2020-04-08 15:08:07,088 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@53ab0286{/jobs/job/json,null,AVAILABLE,@Spark}
2020-04-08 15:08:07,089 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@63c5efee{/stages,null,AVAILABLE,@Spark}
2020-04-08 15:08:07,089 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2d10e0b1{/stages/json,null,AVAILABLE,@Spark}
2020-04-08 15:08:07,090 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1c98290c{/stages/stage,null,AVAILABLE,@Spark}
2020-04-08 15:08:07,091 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@71e5f61d{/stages/stage/json,null,AVAILABLE,@Spark}
2020-04-08 15:08:07,091 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2ce86164{/stages/pool,null,AVAILABLE,@Spark}
2020-04-08 15:08:07,091 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5e8f9e2d{/stages/pool/json,null,AVAILABLE,@Spark}
2020-04-08 15:08:07,092 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@51df223b{/storage,null,AVAILABLE,@Spark}
2020-04-08 15:08:07,092 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@fd46303{/storage/json,null,AVAILABLE,@Spark}
2020-04-08 15:08:07,093 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60d8c0dc{/storage/rdd,null,AVAILABLE,@Spark}
2020-04-08 15:08:07,093 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4204541c{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-04-08 15:08:07,094 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a62689d{/environment,null,AVAILABLE,@Spark}
2020-04-08 15:08:07,094 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4602c2a9{/environment/json,null,AVAILABLE,@Spark}
2020-04-08 15:08:07,095 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60fa3495{/executors,null,AVAILABLE,@Spark}
2020-04-08 15:08:07,095 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3e2822{/executors/json,null,AVAILABLE,@Spark}
2020-04-08 15:08:07,096 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@79e18e38{/executors/threadDump,null,AVAILABLE,@Spark}
2020-04-08 15:08:07,096 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@29a60c27{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-04-08 15:08:07,101 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1849db1a{/static,null,AVAILABLE,@Spark}
2020-04-08 15:08:07,102 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2cf23c81{/,null,AVAILABLE,@Spark}
2020-04-08 15:08:07,103 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3624da92{/api,null,AVAILABLE,@Spark}
2020-04-08 15:08:07,103 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2484f433{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-04-08 15:08:07,103 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60b71e8f{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-04-08 15:08:07,105 INFO org.apache.spark.ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
2020-04-08 15:08:07,178 INFO org.apache.spark.executor.Executor: Starting executor ID driver on host localhost
2020-04-08 15:08:07,245 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 15:08:07,246 INFO org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59784.
2020-04-08 15:08:07,247 INFO org.apache.spark.network.netty.NettyBlockTransferService: Server created on 192.168.0.102:59784
2020-04-08 15:08:07,248 INFO org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-04-08 15:08:07,278 INFO org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 59784, None)
2020-04-08 15:08:07,281 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:59784 with 4.1 GB RAM, BlockManagerId(driver, 192.168.0.102, 59784, None)
2020-04-08 15:08:07,284 INFO org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 59784, None)
2020-04-08 15:08:07,285 INFO org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 59784, None)
2020-04-08 15:08:07,452 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@502f8b57{/metrics/json,null,AVAILABLE,@Spark}
2020-04-08 15:08:07,533 INFO org.apache.spark.SparkContext: Invoking stop() from shutdown hook
2020-04-08 15:08:07,550 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@743cb8e0{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 15:08:07,553 INFO org.apache.spark.ui.SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
2020-04-08 15:08:07,563 INFO org.apache.spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2020-04-08 15:08:07,578 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2020-04-08 15:08:07,579 INFO org.apache.spark.storage.BlockManager: BlockManager stopped
2020-04-08 15:08:07,583 INFO org.apache.spark.storage.BlockManagerMaster: BlockManagerMaster stopped
2020-04-08 15:08:07,586 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2020-04-08 15:08:07,595 INFO org.apache.spark.SparkContext: Successfully stopped SparkContext
2020-04-08 15:08:07,595 INFO org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2020-04-08 15:08:07,596 INFO org.apache.spark.util.ShutdownHookManager: Deleting directory /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/spark-42b791bf-e4c2-45a2-b6c0-3152743f354d
2020-04-08 15:08:18,072 WARN org.apache.spark.util.Utils: Your hostname, MacBook-Pro-CW.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)
2020-04-08 15:08:18,073 WARN org.apache.spark.util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
2020-04-08 15:08:18,101 INFO org.apache.spark.SparkContext: Running Spark version 2.4.0.cloudera2
2020-04-08 15:08:18,290 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-04-08 15:08:18,407 INFO org.apache.spark.SparkContext: Submitted application: Kafka2-kerberos
2020-04-08 15:08:18,450 INFO org.apache.spark.SecurityManager: Changing view acls to: godiswc
2020-04-08 15:08:18,451 INFO org.apache.spark.SecurityManager: Changing modify acls to: godiswc
2020-04-08 15:08:18,451 INFO org.apache.spark.SecurityManager: Changing view acls groups to: 
2020-04-08 15:08:18,452 INFO org.apache.spark.SecurityManager: Changing modify acls groups to: 
2020-04-08 15:08:18,452 INFO org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(godiswc); groups with view permissions: Set(); users  with modify permissions: Set(godiswc); groups with modify permissions: Set()
2020-04-08 15:08:18,609 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 15:08:18,698 INFO org.apache.spark.util.Utils: Successfully started service 'sparkDriver' on port 59834.
2020-04-08 15:08:18,717 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
2020-04-08 15:08:18,728 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
2020-04-08 15:08:18,730 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-04-08 15:08:18,730 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2020-04-08 15:08:18,742 INFO org.apache.spark.storage.DiskBlockManager: Created local directory at /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/blockmgr-d21ac8dd-25ff-4c0f-9694-5aea08aecd93
2020-04-08 15:08:18,773 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 4.1 GB
2020-04-08 15:08:18,783 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
2020-04-08 15:08:18,848 INFO org.spark_project.jetty.util.log: Logging initialized @1344ms
2020-04-08 15:08:18,900 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2019-04-30T04:42:08+08:00, git hash: e1bc35120a6617ee3df052294e433f3a25ce7097
2020-04-08 15:08:18,913 INFO org.spark_project.jetty.server.Server: Started @1409ms
2020-04-08 15:08:18,915 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 15:08:18,926 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@743cb8e0{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 15:08:18,926 INFO org.apache.spark.util.Utils: Successfully started service 'SparkUI' on port 4040.
2020-04-08 15:08:18,945 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@41a90fa8{/jobs,null,AVAILABLE,@Spark}
2020-04-08 15:08:18,946 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c67e137{/jobs/json,null,AVAILABLE,@Spark}
2020-04-08 15:08:18,946 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2c9399a4{/jobs/job,null,AVAILABLE,@Spark}
2020-04-08 15:08:18,948 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@53ab0286{/jobs/job/json,null,AVAILABLE,@Spark}
2020-04-08 15:08:18,949 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@63c5efee{/stages,null,AVAILABLE,@Spark}
2020-04-08 15:08:18,949 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2d10e0b1{/stages/json,null,AVAILABLE,@Spark}
2020-04-08 15:08:18,950 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1c98290c{/stages/stage,null,AVAILABLE,@Spark}
2020-04-08 15:08:18,951 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@71e5f61d{/stages/stage/json,null,AVAILABLE,@Spark}
2020-04-08 15:08:18,951 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2ce86164{/stages/pool,null,AVAILABLE,@Spark}
2020-04-08 15:08:18,952 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5e8f9e2d{/stages/pool/json,null,AVAILABLE,@Spark}
2020-04-08 15:08:18,952 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@51df223b{/storage,null,AVAILABLE,@Spark}
2020-04-08 15:08:18,952 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@fd46303{/storage/json,null,AVAILABLE,@Spark}
2020-04-08 15:08:18,953 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60d8c0dc{/storage/rdd,null,AVAILABLE,@Spark}
2020-04-08 15:08:18,953 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4204541c{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-04-08 15:08:18,954 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a62689d{/environment,null,AVAILABLE,@Spark}
2020-04-08 15:08:18,954 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4602c2a9{/environment/json,null,AVAILABLE,@Spark}
2020-04-08 15:08:18,955 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60fa3495{/executors,null,AVAILABLE,@Spark}
2020-04-08 15:08:18,955 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3e2822{/executors/json,null,AVAILABLE,@Spark}
2020-04-08 15:08:18,956 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@79e18e38{/executors/threadDump,null,AVAILABLE,@Spark}
2020-04-08 15:08:18,956 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@29a60c27{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-04-08 15:08:18,960 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1849db1a{/static,null,AVAILABLE,@Spark}
2020-04-08 15:08:18,961 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2cf23c81{/,null,AVAILABLE,@Spark}
2020-04-08 15:08:18,962 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3624da92{/api,null,AVAILABLE,@Spark}
2020-04-08 15:08:18,962 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2484f433{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-04-08 15:08:18,963 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60b71e8f{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-04-08 15:08:18,964 INFO org.apache.spark.ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
2020-04-08 15:08:19,066 INFO org.apache.spark.executor.Executor: Starting executor ID driver on host localhost
2020-04-08 15:08:19,160 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 15:08:19,161 INFO org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59835.
2020-04-08 15:08:19,162 INFO org.apache.spark.network.netty.NettyBlockTransferService: Server created on 192.168.0.102:59835
2020-04-08 15:08:19,164 INFO org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-04-08 15:08:19,203 INFO org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 59835, None)
2020-04-08 15:08:19,206 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:59835 with 4.1 GB RAM, BlockManagerId(driver, 192.168.0.102, 59835, None)
2020-04-08 15:08:19,209 INFO org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 59835, None)
2020-04-08 15:08:19,209 INFO org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 59835, None)
2020-04-08 15:08:19,402 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@502f8b57{/metrics/json,null,AVAILABLE,@Spark}
2020-04-08 15:08:19,463 INFO org.apache.spark.SparkContext: Invoking stop() from shutdown hook
2020-04-08 15:08:19,473 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@743cb8e0{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 15:08:19,475 INFO org.apache.spark.ui.SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
2020-04-08 15:08:19,482 INFO org.apache.spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2020-04-08 15:08:19,493 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2020-04-08 15:08:19,493 INFO org.apache.spark.storage.BlockManager: BlockManager stopped
2020-04-08 15:08:19,497 INFO org.apache.spark.storage.BlockManagerMaster: BlockManagerMaster stopped
2020-04-08 15:08:19,502 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2020-04-08 15:08:19,510 INFO org.apache.spark.SparkContext: Successfully stopped SparkContext
2020-04-08 15:08:19,510 INFO org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2020-04-08 15:08:19,511 INFO org.apache.spark.util.ShutdownHookManager: Deleting directory /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/spark-96bc6cdf-e256-41a8-af7f-8159dd2fde5d
2020-04-08 15:08:30,808 WARN org.apache.spark.util.Utils: Your hostname, MacBook-Pro-CW.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)
2020-04-08 15:08:30,809 WARN org.apache.spark.util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
2020-04-08 15:08:30,836 INFO org.apache.spark.SparkContext: Running Spark version 2.4.0.cloudera2
2020-04-08 15:08:31,024 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-04-08 15:08:31,143 INFO org.apache.spark.SparkContext: Submitted application: Kafka2-kerberos
2020-04-08 15:08:31,184 INFO org.apache.spark.SecurityManager: Changing view acls to: godiswc
2020-04-08 15:08:31,184 INFO org.apache.spark.SecurityManager: Changing modify acls to: godiswc
2020-04-08 15:08:31,184 INFO org.apache.spark.SecurityManager: Changing view acls groups to: 
2020-04-08 15:08:31,185 INFO org.apache.spark.SecurityManager: Changing modify acls groups to: 
2020-04-08 15:08:31,185 INFO org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(godiswc); groups with view permissions: Set(); users  with modify permissions: Set(godiswc); groups with modify permissions: Set()
2020-04-08 15:08:31,336 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 15:08:31,438 INFO org.apache.spark.util.Utils: Successfully started service 'sparkDriver' on port 59885.
2020-04-08 15:08:31,458 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
2020-04-08 15:08:31,471 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
2020-04-08 15:08:31,473 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-04-08 15:08:31,474 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2020-04-08 15:08:31,485 INFO org.apache.spark.storage.DiskBlockManager: Created local directory at /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/blockmgr-f4b44a2a-ffb1-4201-933a-1660c17fea54
2020-04-08 15:08:31,521 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 4.1 GB
2020-04-08 15:08:31,530 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
2020-04-08 15:08:31,592 INFO org.spark_project.jetty.util.log: Logging initialized @1344ms
2020-04-08 15:08:31,638 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2019-04-30T04:42:08+08:00, git hash: e1bc35120a6617ee3df052294e433f3a25ce7097
2020-04-08 15:08:31,650 INFO org.spark_project.jetty.server.Server: Started @1404ms
2020-04-08 15:08:31,652 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 15:08:31,663 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@743cb8e0{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 15:08:31,665 INFO org.apache.spark.util.Utils: Successfully started service 'SparkUI' on port 4040.
2020-04-08 15:08:31,683 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@41a90fa8{/jobs,null,AVAILABLE,@Spark}
2020-04-08 15:08:31,684 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c67e137{/jobs/json,null,AVAILABLE,@Spark}
2020-04-08 15:08:31,684 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2c9399a4{/jobs/job,null,AVAILABLE,@Spark}
2020-04-08 15:08:31,686 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@53ab0286{/jobs/job/json,null,AVAILABLE,@Spark}
2020-04-08 15:08:31,687 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@63c5efee{/stages,null,AVAILABLE,@Spark}
2020-04-08 15:08:31,687 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2d10e0b1{/stages/json,null,AVAILABLE,@Spark}
2020-04-08 15:08:31,688 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1c98290c{/stages/stage,null,AVAILABLE,@Spark}
2020-04-08 15:08:31,689 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@71e5f61d{/stages/stage/json,null,AVAILABLE,@Spark}
2020-04-08 15:08:31,689 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2ce86164{/stages/pool,null,AVAILABLE,@Spark}
2020-04-08 15:08:31,690 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5e8f9e2d{/stages/pool/json,null,AVAILABLE,@Spark}
2020-04-08 15:08:31,690 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@51df223b{/storage,null,AVAILABLE,@Spark}
2020-04-08 15:08:31,691 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@fd46303{/storage/json,null,AVAILABLE,@Spark}
2020-04-08 15:08:31,691 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60d8c0dc{/storage/rdd,null,AVAILABLE,@Spark}
2020-04-08 15:08:31,691 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4204541c{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-04-08 15:08:31,692 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a62689d{/environment,null,AVAILABLE,@Spark}
2020-04-08 15:08:31,692 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4602c2a9{/environment/json,null,AVAILABLE,@Spark}
2020-04-08 15:08:31,693 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60fa3495{/executors,null,AVAILABLE,@Spark}
2020-04-08 15:08:31,693 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3e2822{/executors/json,null,AVAILABLE,@Spark}
2020-04-08 15:08:31,693 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@79e18e38{/executors/threadDump,null,AVAILABLE,@Spark}
2020-04-08 15:08:31,694 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@29a60c27{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-04-08 15:08:31,698 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1849db1a{/static,null,AVAILABLE,@Spark}
2020-04-08 15:08:31,699 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2cf23c81{/,null,AVAILABLE,@Spark}
2020-04-08 15:08:31,699 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3624da92{/api,null,AVAILABLE,@Spark}
2020-04-08 15:08:31,700 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2484f433{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-04-08 15:08:31,701 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60b71e8f{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-04-08 15:08:31,702 INFO org.apache.spark.ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
2020-04-08 15:08:31,790 INFO org.apache.spark.executor.Executor: Starting executor ID driver on host localhost
2020-04-08 15:08:31,866 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 15:08:31,867 INFO org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59887.
2020-04-08 15:08:31,867 INFO org.apache.spark.network.netty.NettyBlockTransferService: Server created on 192.168.0.102:59887
2020-04-08 15:08:31,869 INFO org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-04-08 15:08:31,911 INFO org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 59887, None)
2020-04-08 15:08:31,917 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:59887 with 4.1 GB RAM, BlockManagerId(driver, 192.168.0.102, 59887, None)
2020-04-08 15:08:31,920 INFO org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 59887, None)
2020-04-08 15:08:31,920 INFO org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 59887, None)
2020-04-08 15:08:32,090 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@502f8b57{/metrics/json,null,AVAILABLE,@Spark}
2020-04-08 15:08:32,187 INFO org.apache.spark.SparkContext: Invoking stop() from shutdown hook
2020-04-08 15:08:32,199 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@743cb8e0{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 15:08:32,200 INFO org.apache.spark.ui.SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
2020-04-08 15:08:32,207 INFO org.apache.spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2020-04-08 15:08:32,218 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2020-04-08 15:08:32,218 INFO org.apache.spark.storage.BlockManager: BlockManager stopped
2020-04-08 15:08:32,222 INFO org.apache.spark.storage.BlockManagerMaster: BlockManagerMaster stopped
2020-04-08 15:08:32,224 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2020-04-08 15:08:32,232 INFO org.apache.spark.SparkContext: Successfully stopped SparkContext
2020-04-08 15:08:32,232 INFO org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2020-04-08 15:08:32,233 INFO org.apache.spark.util.ShutdownHookManager: Deleting directory /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/spark-a686f638-99dc-49c8-afdf-6d83eb962534
2020-04-08 15:15:26,415 WARN org.apache.spark.util.Utils: Your hostname, MacBook-Pro-CW.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)
2020-04-08 15:15:26,417 WARN org.apache.spark.util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
2020-04-08 15:15:26,612 INFO org.apache.spark.SparkContext: Running Spark version 2.4.0.cloudera2
2020-04-08 15:15:26,819 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-04-08 15:15:26,924 INFO org.apache.spark.SparkContext: Submitted application: Kafka2SparkStreaming2Kudu-kerberos
2020-04-08 15:15:26,961 INFO org.apache.spark.SecurityManager: Changing view acls to: godiswc
2020-04-08 15:15:26,961 INFO org.apache.spark.SecurityManager: Changing modify acls to: godiswc
2020-04-08 15:15:26,961 INFO org.apache.spark.SecurityManager: Changing view acls groups to: 
2020-04-08 15:15:26,962 INFO org.apache.spark.SecurityManager: Changing modify acls groups to: 
2020-04-08 15:15:26,962 INFO org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(godiswc); groups with view permissions: Set(); users  with modify permissions: Set(godiswc); groups with modify permissions: Set()
2020-04-08 15:15:27,100 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 15:15:27,179 INFO org.apache.spark.util.Utils: Successfully started service 'sparkDriver' on port 61512.
2020-04-08 15:15:27,194 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
2020-04-08 15:15:27,204 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
2020-04-08 15:15:27,224 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-04-08 15:15:27,224 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2020-04-08 15:15:27,237 INFO org.apache.spark.storage.DiskBlockManager: Created local directory at /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/blockmgr-f2a4e0b4-d25d-4a5d-8611-4162f810b65d
2020-04-08 15:15:27,251 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 4.1 GB
2020-04-08 15:15:27,260 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
2020-04-08 15:15:27,304 INFO org.spark_project.jetty.util.log: Logging initialized @1575ms
2020-04-08 15:15:27,352 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2019-04-30T04:42:08+08:00, git hash: e1bc35120a6617ee3df052294e433f3a25ce7097
2020-04-08 15:15:27,363 INFO org.spark_project.jetty.server.Server: Started @1634ms
2020-04-08 15:15:27,364 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 15:15:27,374 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@533b266e{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 15:15:27,375 INFO org.apache.spark.util.Utils: Successfully started service 'SparkUI' on port 4040.
2020-04-08 15:15:27,390 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6e521c1e{/jobs,null,AVAILABLE,@Spark}
2020-04-08 15:15:27,392 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62dae245{/jobs/json,null,AVAILABLE,@Spark}
2020-04-08 15:15:27,393 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4b6579e8{/jobs/job,null,AVAILABLE,@Spark}
2020-04-08 15:15:27,395 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@591e58fa{/jobs/job/json,null,AVAILABLE,@Spark}
2020-04-08 15:15:27,396 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3954d008{/stages,null,AVAILABLE,@Spark}
2020-04-08 15:15:27,396 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f94c4db{/stages/json,null,AVAILABLE,@Spark}
2020-04-08 15:15:27,396 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@593e824f{/stages/stage,null,AVAILABLE,@Spark}
2020-04-08 15:15:27,397 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@64bc21ac{/stages/stage/json,null,AVAILABLE,@Spark}
2020-04-08 15:15:27,398 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@493dfb8e{/stages/pool,null,AVAILABLE,@Spark}
2020-04-08 15:15:27,398 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5d25e6bb{/stages/pool/json,null,AVAILABLE,@Spark}
2020-04-08 15:15:27,398 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@ce5a68e{/storage,null,AVAILABLE,@Spark}
2020-04-08 15:15:27,399 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@9d157ff{/storage/json,null,AVAILABLE,@Spark}
2020-04-08 15:15:27,399 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f162cc0{/storage/rdd,null,AVAILABLE,@Spark}
2020-04-08 15:15:27,400 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5df417a7{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-04-08 15:15:27,400 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7c041b41{/environment,null,AVAILABLE,@Spark}
2020-04-08 15:15:27,400 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7f69d591{/environment/json,null,AVAILABLE,@Spark}
2020-04-08 15:15:27,401 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@61078690{/executors,null,AVAILABLE,@Spark}
2020-04-08 15:15:27,401 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1cb3ec38{/executors/json,null,AVAILABLE,@Spark}
2020-04-08 15:15:27,402 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@403132fc{/executors/threadDump,null,AVAILABLE,@Spark}
2020-04-08 15:15:27,402 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@71c5b236{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-04-08 15:15:27,406 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2cab9998{/static,null,AVAILABLE,@Spark}
2020-04-08 15:15:27,406 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@332a7fce{/,null,AVAILABLE,@Spark}
2020-04-08 15:15:27,407 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@549621f3{/api,null,AVAILABLE,@Spark}
2020-04-08 15:15:27,407 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@37ebc9d8{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-04-08 15:15:27,408 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@293bb8a5{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-04-08 15:15:27,409 INFO org.apache.spark.ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
2020-04-08 15:15:27,479 INFO org.apache.spark.executor.Executor: Starting executor ID driver on host localhost
2020-04-08 15:15:27,526 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 15:15:27,528 INFO org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 61516.
2020-04-08 15:15:27,529 INFO org.apache.spark.network.netty.NettyBlockTransferService: Server created on 192.168.0.102:61516
2020-04-08 15:15:27,530 INFO org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-04-08 15:15:27,555 INFO org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 61516, None)
2020-04-08 15:15:27,558 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:61516 with 4.1 GB RAM, BlockManagerId(driver, 192.168.0.102, 61516, None)
2020-04-08 15:15:27,560 INFO org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 61516, None)
2020-04-08 15:15:27,561 INFO org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 61516, None)
2020-04-08 15:15:27,728 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@54acff7d{/metrics/json,null,AVAILABLE,@Spark}
2020-04-08 15:15:27,827 WARN org.apache.spark.streaming.StreamingContext: spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
2020-04-08 15:15:28,036 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding enable.auto.commit to false for executor
2020-04-08 15:15:28,037 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding auto.offset.reset to none for executor
2020-04-08 15:15:28,037 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding executor group.id to spark-executor-testgroup1
2020-04-08 15:15:28,038 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
2020-04-08 15:15:28,708 WARN org.apache.kudu.util.NetUtil: Slow DNS lookup! Resolved IP of `cw-instances-2.vpc.cloudera.com' to cw-instances-2.vpc.cloudera.com/10.65.1.47 in 160058753ns
2020-04-08 15:15:28,971 WARN org.apache.kudu.util.NetUtil: Slow DNS lookup! Resolved IP of `cw-instances-3.vpc.cloudera.com' to cw-instances-3.vpc.cloudera.com/10.65.9.108 in 159502471ns
2020-04-08 15:15:29,133 WARN org.apache.kudu.util.NetUtil: Slow DNS lookup! Resolved IP of `cw-instances-4.vpc.cloudera.com' to cw-instances-4.vpc.cloudera.com/10.65.12.223 in 157615115ns
2020-04-08 15:15:30,881 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Slide time = 10000 ms
2020-04-08 15:15:30,882 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Storage level = Serialized 1x Replicated
2020-04-08 15:15:30,883 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Checkpoint interval = null
2020-04-08 15:15:30,884 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Remember interval = 10000 ms
2020-04-08 15:15:30,884 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@7b306ae5
2020-04-08 15:15:30,884 INFO org.apache.spark.streaming.dstream.ForEachDStream: Slide time = 10000 ms
2020-04-08 15:15:30,885 INFO org.apache.spark.streaming.dstream.ForEachDStream: Storage level = Serialized 1x Replicated
2020-04-08 15:15:30,885 INFO org.apache.spark.streaming.dstream.ForEachDStream: Checkpoint interval = null
2020-04-08 15:15:30,885 INFO org.apache.spark.streaming.dstream.ForEachDStream: Remember interval = 10000 ms
2020-04-08 15:15:30,885 INFO org.apache.spark.streaming.dstream.ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@25b34eb8
2020-04-08 15:15:30,958 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = testgroup1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-08 15:15:31,030 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-08 15:15:31,030 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-08 15:15:31,030 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586330131028
2020-04-08 15:15:31,032 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-1, groupId=testgroup1] Subscribed to topic(s): PABUSI2
2020-04-08 15:15:31,761 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-1, groupId=testgroup1] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-08 15:15:31,762 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Discovered group coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 15:15:31,765 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Revoking previously assigned partitions []
2020-04-08 15:15:31,766 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 15:15:32,288 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 15:15:35,701 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Successfully joined group with generation 49
2020-04-08 15:15:35,707 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting newly assigned partitions: PABUSI2-2, PABUSI2-0, PABUSI2-1
2020-04-08 15:15:35,892 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-2 to the committed offset FetchPosition{offset=155, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-2.vpc.cloudera.com:9092 (id: 52 rack: null), epoch=0}}
2020-04-08 15:15:35,894 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-0 to the committed offset FetchPosition{offset=154, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-4.vpc.cloudera.com:9092 (id: 53 rack: null), epoch=2}}
2020-04-08 15:15:35,894 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-1 to the committed offset FetchPosition{offset=155, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-3.vpc.cloudera.com:9092 (id: 54 rack: null), epoch=2}}
2020-04-08 15:15:35,902 INFO org.apache.spark.streaming.util.RecurringTimer: Started timer for JobGenerator at time 1586330140000
2020-04-08 15:15:35,903 INFO org.apache.spark.streaming.scheduler.JobGenerator: Started JobGenerator at 1586330140000 ms
2020-04-08 15:15:35,903 INFO org.apache.spark.streaming.scheduler.JobScheduler: Started JobScheduler
2020-04-08 15:15:35,909 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@499ef98e{/streaming,null,AVAILABLE,@Spark}
2020-04-08 15:15:35,910 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@24934262{/streaming/json,null,AVAILABLE,@Spark}
2020-04-08 15:15:35,912 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@28501a4b{/streaming/batch,null,AVAILABLE,@Spark}
2020-04-08 15:15:35,912 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5b051a5c{/streaming/batch/json,null,AVAILABLE,@Spark}
2020-04-08 15:15:35,913 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2bebb74f{/static/streaming,null,AVAILABLE,@Spark}
2020-04-08 15:15:35,914 INFO org.apache.spark.streaming.StreamingContext: StreamingContext started
2020-04-08 15:15:40,038 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 15:15:40,038 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 15:15:40,038 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 15:15:40,221 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 155.
2020-04-08 15:15:40,221 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 156.
2020-04-08 15:15:40,222 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 154.
2020-04-08 15:15:40,244 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586330140000 ms
2020-04-08 15:15:40,248 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586330140000 ms.0 from job set of time 1586330140000 ms
2020-04-08 15:15:40,290 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:136
2020-04-08 15:15:40,308 INFO org.apache.spark.scheduler.DAGScheduler: Got job 0 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136) with 3 output partitions
2020-04-08 15:15:40,308 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 0 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136)
2020-04-08 15:15:40,309 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 15:15:40,310 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 15:15:40,314 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 0 (KafkaRDD[0] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113), which has no missing parents
2020-04-08 15:15:40,377 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 15:15:40,751 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.4 KB, free 4.1 GB)
2020-04-08 15:15:40,752 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:61516 (size: 3.4 KB, free: 4.1 GB)
2020-04-08 15:15:40,754 INFO org.apache.spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2020-04-08 15:15:40,764 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 0 (KafkaRDD[0] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 15:15:40,765 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 0.0 with 3 tasks
2020-04-08 15:15:40,794 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:15:40,800 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
2020-04-08 15:15:40,820 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Computing topic PABUSI2, partition 2 offsets 155 -> 156
2020-04-08 15:15:40,823 INFO org.apache.spark.streaming.kafka010.KafkaDataConsumer: Initializing cache 16 64 0.75
2020-04-08 15:15:40,826 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-testgroup1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-08 15:15:40,831 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-08 15:15:40,831 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-08 15:15:40,831 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586330140830
2020-04-08 15:15:40,831 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Subscribed to partition(s): PABUSI2-2
2020-04-08 15:15:40,834 INFO org.apache.spark.streaming.kafka010.InternalKafkaConsumer: Initial fetch for spark-executor-testgroup1 PABUSI2-2 155
2020-04-08 15:15:40,834 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Seeking to offset 155 for partition PABUSI2-2
2020-04-08 15:15:41,372 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-08 15:15:43,744 WARN org.apache.kudu.util.NetUtil: Slow DNS lookup! Resolved IP of `cw-instances-1.vpc.cloudera.com' to cw-instances-1.vpc.cloudera.com/10.65.11.227 in 155611994ns
2020-04-08 15:15:45,044 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 2880 bytes result sent to driver
2020-04-08 15:15:45,046 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:15:45,046 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 0.0 (TID 1)
2020-04-08 15:15:45,049 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 4262 ms on localhost (executor driver) (1/3)
2020-04-08 15:15:45,050 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 155 is the same as ending offset skipping PABUSI2 1
2020-04-08 15:15:45,051 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 0.0 (TID 1). 2828 bytes result sent to driver
2020-04-08 15:15:45,052 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:15:45,052 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 0.0 (TID 2)
2020-04-08 15:15:45,052 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 7 ms on localhost (executor driver) (2/3)
2020-04-08 15:15:45,055 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 154 is the same as ending offset skipping PABUSI2 0
2020-04-08 15:15:45,056 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 0.0 (TID 2). 2871 bytes result sent to driver
2020-04-08 15:15:45,059 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 7 ms on localhost (executor driver) (3/3)
2020-04-08 15:15:45,060 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-04-08 15:15:45,061 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 0 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136) finished in 4.727 s
2020-04-08 15:15:45,065 INFO org.apache.spark.scheduler.DAGScheduler: Job 0 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:136, took 4.774454 s
2020-04-08 15:15:45,067 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586330140000 ms.0 from job set of time 1586330140000 ms
2020-04-08 15:15:45,068 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 5.067 s for time 1586330140000 ms (execution: 4.821 s)
2020-04-08 15:15:45,072 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 15:15:45,075 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 
2020-04-08 15:15:50,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 15:15:50,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 15:15:50,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 15:15:50,182 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 155.
2020-04-08 15:15:50,183 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 156.
2020-04-08 15:15:50,183 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 154.
2020-04-08 15:15:50,185 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586330150000 ms
2020-04-08 15:15:50,185 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586330150000 ms.0 from job set of time 1586330150000 ms
2020-04-08 15:15:50,198 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:136
2020-04-08 15:15:50,199 INFO org.apache.spark.scheduler.DAGScheduler: Got job 1 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136) with 3 output partitions
2020-04-08 15:15:50,199 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 1 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136)
2020-04-08 15:15:50,199 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 15:15:50,200 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 15:15:50,200 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 1 (KafkaRDD[1] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113), which has no missing parents
2020-04-08 15:15:50,203 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 15:15:50,207 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 15:15:50,208 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.102:61516 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 15:15:50,210 INFO org.apache.spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2020-04-08 15:15:50,212 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 1 (KafkaRDD[1] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 15:15:50,212 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 1.0 with 3 tasks
2020-04-08 15:15:50,215 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:15:50,216 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 1.0 (TID 3)
2020-04-08 15:15:50,222 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 156 is the same as ending offset skipping PABUSI2 2
2020-04-08 15:15:50,225 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 1.0 (TID 3). 2828 bytes result sent to driver
2020-04-08 15:15:50,227 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 4, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:15:50,229 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 1.0 (TID 4)
2020-04-08 15:15:50,229 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 3) in 15 ms on localhost (executor driver) (1/3)
2020-04-08 15:15:50,232 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 155 is the same as ending offset skipping PABUSI2 1
2020-04-08 15:15:50,234 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 1.0 (TID 4). 2828 bytes result sent to driver
2020-04-08 15:15:50,235 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 1.0 (TID 5, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:15:50,235 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 1.0 (TID 5)
2020-04-08 15:15:50,236 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 4) in 9 ms on localhost (executor driver) (2/3)
2020-04-08 15:15:50,242 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 154 is the same as ending offset skipping PABUSI2 0
2020-04-08 15:15:50,246 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 1.0 (TID 5). 2871 bytes result sent to driver
2020-04-08 15:15:50,249 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 1.0 (TID 5) in 15 ms on localhost (executor driver) (3/3)
2020-04-08 15:15:50,250 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
2020-04-08 15:15:50,252 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 1 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136) finished in 0.050 s
2020-04-08 15:15:50,254 INFO org.apache.spark.scheduler.DAGScheduler: Job 1 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:136, took 0.055003 s
2020-04-08 15:15:50,255 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586330150000 ms.0 from job set of time 1586330150000 ms
2020-04-08 15:15:50,256 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.255 s for time 1586330150000 ms (execution: 0.070 s)
2020-04-08 15:15:50,257 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 0 from persistence list
2020-04-08 15:15:50,264 INFO org.apache.spark.storage.BlockManager: Removing RDD 0
2020-04-08 15:15:50,265 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 15:15:50,265 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 
2020-04-08 15:16:00,001 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 15:16:00,001 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 15:16:00,001 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 15:16:00,178 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 155.
2020-04-08 15:16:00,178 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 156.
2020-04-08 15:16:00,178 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 154.
2020-04-08 15:16:00,180 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586330160000 ms
2020-04-08 15:16:00,180 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586330160000 ms.0 from job set of time 1586330160000 ms
2020-04-08 15:16:00,186 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:136
2020-04-08 15:16:00,187 INFO org.apache.spark.scheduler.DAGScheduler: Got job 2 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136) with 3 output partitions
2020-04-08 15:16:00,187 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 2 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136)
2020-04-08 15:16:00,187 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 15:16:00,188 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 15:16:00,188 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 2 (KafkaRDD[2] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113), which has no missing parents
2020-04-08 15:16:00,190 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 15:16:00,192 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 15:16:00,192 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.102:61516 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 15:16:00,193 INFO org.apache.spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2020-04-08 15:16:00,193 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 2 (KafkaRDD[2] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 15:16:00,193 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 2.0 with 3 tasks
2020-04-08 15:16:00,194 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:16:00,194 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 2.0 (TID 6)
2020-04-08 15:16:00,196 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 156 is the same as ending offset skipping PABUSI2 2
2020-04-08 15:16:00,197 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 2.0 (TID 6). 2785 bytes result sent to driver
2020-04-08 15:16:00,197 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 2.0 (TID 7, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:16:00,197 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 2.0 (TID 7)
2020-04-08 15:16:00,197 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 6) in 3 ms on localhost (executor driver) (1/3)
2020-04-08 15:16:00,199 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 155 is the same as ending offset skipping PABUSI2 1
2020-04-08 15:16:00,200 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 2.0 (TID 7). 2785 bytes result sent to driver
2020-04-08 15:16:00,200 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 2.0 (TID 8, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:16:00,201 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 2.0 (TID 8)
2020-04-08 15:16:00,201 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 2.0 (TID 7) in 4 ms on localhost (executor driver) (2/3)
2020-04-08 15:16:00,202 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 154 is the same as ending offset skipping PABUSI2 0
2020-04-08 15:16:00,203 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 2.0 (TID 8). 2828 bytes result sent to driver
2020-04-08 15:16:00,204 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 2.0 (TID 8) in 3 ms on localhost (executor driver) (3/3)
2020-04-08 15:16:00,204 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
2020-04-08 15:16:00,204 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 2 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136) finished in 0.015 s
2020-04-08 15:16:00,204 INFO org.apache.spark.scheduler.DAGScheduler: Job 2 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:136, took 0.017774 s
2020-04-08 15:16:00,205 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586330160000 ms.0 from job set of time 1586330160000 ms
2020-04-08 15:16:00,205 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.205 s for time 1586330160000 ms (execution: 0.025 s)
2020-04-08 15:16:00,205 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 1 from persistence list
2020-04-08 15:16:00,206 INFO org.apache.spark.storage.BlockManager: Removing RDD 1
2020-04-08 15:16:00,206 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 15:16:00,206 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586330140000 ms
2020-04-08 15:16:10,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 15:16:10,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 15:16:10,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 15:16:10,177 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 155.
2020-04-08 15:16:10,180 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 156.
2020-04-08 15:16:10,180 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 154.
2020-04-08 15:16:10,181 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586330170000 ms
2020-04-08 15:16:10,181 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586330170000 ms.0 from job set of time 1586330170000 ms
2020-04-08 15:16:10,187 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:136
2020-04-08 15:16:10,187 INFO org.apache.spark.scheduler.DAGScheduler: Got job 3 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136) with 3 output partitions
2020-04-08 15:16:10,187 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 3 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136)
2020-04-08 15:16:10,187 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 15:16:10,187 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 15:16:10,188 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 3 (KafkaRDD[3] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113), which has no missing parents
2020-04-08 15:16:10,190 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 15:16:10,191 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 15:16:10,191 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.102:61516 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 15:16:10,192 INFO org.apache.spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2020-04-08 15:16:10,192 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 3 (KafkaRDD[3] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 15:16:10,192 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 3.0 with 3 tasks
2020-04-08 15:16:10,193 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 9, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:16:10,193 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 3.0 (TID 9)
2020-04-08 15:16:10,194 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 156 is the same as ending offset skipping PABUSI2 2
2020-04-08 15:16:10,195 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 3.0 (TID 9). 2828 bytes result sent to driver
2020-04-08 15:16:10,196 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 3.0 (TID 10, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:16:10,196 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 3.0 (TID 10)
2020-04-08 15:16:10,196 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 9) in 3 ms on localhost (executor driver) (1/3)
2020-04-08 15:16:10,197 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 155 is the same as ending offset skipping PABUSI2 1
2020-04-08 15:16:10,198 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 3.0 (TID 10). 2828 bytes result sent to driver
2020-04-08 15:16:10,199 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 3.0 (TID 11, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:16:10,199 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 3.0 (TID 11)
2020-04-08 15:16:10,199 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 3.0 (TID 10) in 3 ms on localhost (executor driver) (2/3)
2020-04-08 15:16:10,200 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 154 is the same as ending offset skipping PABUSI2 0
2020-04-08 15:16:10,200 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 3.0 (TID 11). 2785 bytes result sent to driver
2020-04-08 15:16:10,201 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 3.0 (TID 11) in 3 ms on localhost (executor driver) (3/3)
2020-04-08 15:16:10,201 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
2020-04-08 15:16:10,201 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 3 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136) finished in 0.012 s
2020-04-08 15:16:10,202 INFO org.apache.spark.scheduler.DAGScheduler: Job 3 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:136, took 0.014951 s
2020-04-08 15:16:10,202 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586330170000 ms.0 from job set of time 1586330170000 ms
2020-04-08 15:16:10,202 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.202 s for time 1586330170000 ms (execution: 0.021 s)
2020-04-08 15:16:10,202 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 2 from persistence list
2020-04-08 15:16:10,203 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 15:16:10,203 INFO org.apache.spark.storage.BlockManager: Removing RDD 2
2020-04-08 15:16:10,203 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586330150000 ms
2020-04-08 15:16:20,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 15:16:20,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 15:16:20,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 15:16:20,179 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 155.
2020-04-08 15:16:20,180 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 156.
2020-04-08 15:16:20,180 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 154.
2020-04-08 15:16:20,181 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586330180000 ms
2020-04-08 15:16:20,181 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586330180000 ms.0 from job set of time 1586330180000 ms
2020-04-08 15:16:20,186 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:136
2020-04-08 15:16:20,187 INFO org.apache.spark.scheduler.DAGScheduler: Got job 4 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136) with 3 output partitions
2020-04-08 15:16:20,187 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 4 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136)
2020-04-08 15:16:20,187 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 15:16:20,187 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 15:16:20,187 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 4 (KafkaRDD[4] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113), which has no missing parents
2020-04-08 15:16:20,190 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 15:16:20,191 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 15:16:20,191 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.102:61516 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 15:16:20,192 INFO org.apache.spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1161
2020-04-08 15:16:20,192 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 4 (KafkaRDD[4] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 15:16:20,192 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 4.0 with 3 tasks
2020-04-08 15:16:20,193 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 12, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:16:20,193 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 4.0 (TID 12)
2020-04-08 15:16:20,194 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 156 is the same as ending offset skipping PABUSI2 2
2020-04-08 15:16:20,195 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 4.0 (TID 12). 2828 bytes result sent to driver
2020-04-08 15:16:20,195 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 4.0 (TID 13, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:16:20,195 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 4.0 (TID 13)
2020-04-08 15:16:20,195 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 12) in 2 ms on localhost (executor driver) (1/3)
2020-04-08 15:16:20,196 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 155 is the same as ending offset skipping PABUSI2 1
2020-04-08 15:16:20,197 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 4.0 (TID 13). 2785 bytes result sent to driver
2020-04-08 15:16:20,197 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 4.0 (TID 14, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:16:20,197 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 4.0 (TID 13) in 2 ms on localhost (executor driver) (2/3)
2020-04-08 15:16:20,197 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 4.0 (TID 14)
2020-04-08 15:16:20,198 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 154 is the same as ending offset skipping PABUSI2 0
2020-04-08 15:16:20,199 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 4.0 (TID 14). 2828 bytes result sent to driver
2020-04-08 15:16:20,199 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 4.0 (TID 14) in 2 ms on localhost (executor driver) (3/3)
2020-04-08 15:16:20,200 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
2020-04-08 15:16:20,200 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 4 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136) finished in 0.012 s
2020-04-08 15:16:20,200 INFO org.apache.spark.scheduler.DAGScheduler: Job 4 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:136, took 0.014108 s
2020-04-08 15:16:20,200 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586330180000 ms.0 from job set of time 1586330180000 ms
2020-04-08 15:16:20,201 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.200 s for time 1586330180000 ms (execution: 0.019 s)
2020-04-08 15:16:20,201 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 3 from persistence list
2020-04-08 15:16:20,201 INFO org.apache.spark.storage.BlockManager: Removing RDD 3
2020-04-08 15:16:20,201 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 15:16:20,202 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586330160000 ms
2020-04-08 15:16:30,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 15:16:30,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 15:16:30,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 15:16:30,177 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 156.
2020-04-08 15:16:30,177 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 155.
2020-04-08 15:16:30,179 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 154.
2020-04-08 15:16:30,180 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586330190000 ms
2020-04-08 15:16:30,180 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586330190000 ms.0 from job set of time 1586330190000 ms
2020-04-08 15:16:30,185 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:136
2020-04-08 15:16:30,186 INFO org.apache.spark.scheduler.DAGScheduler: Got job 5 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136) with 3 output partitions
2020-04-08 15:16:30,186 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 5 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136)
2020-04-08 15:16:30,186 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 15:16:30,186 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 15:16:30,187 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 5 (KafkaRDD[5] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113), which has no missing parents
2020-04-08 15:16:30,188 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 15:16:30,190 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 15:16:30,190 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.102:61516 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 15:16:30,191 INFO org.apache.spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1161
2020-04-08 15:16:30,191 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 5 (KafkaRDD[5] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 15:16:30,191 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 5.0 with 3 tasks
2020-04-08 15:16:30,192 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 15, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:16:30,192 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 5.0 (TID 15)
2020-04-08 15:16:30,194 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 156 is the same as ending offset skipping PABUSI2 2
2020-04-08 15:16:30,195 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 5.0 (TID 15). 2828 bytes result sent to driver
2020-04-08 15:16:30,196 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 5.0 (TID 16, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:16:30,196 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 5.0 (TID 16)
2020-04-08 15:16:30,196 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 15) in 4 ms on localhost (executor driver) (1/3)
2020-04-08 15:16:30,197 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 155 is the same as ending offset skipping PABUSI2 1
2020-04-08 15:16:30,197 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 5.0 (TID 16). 2785 bytes result sent to driver
2020-04-08 15:16:30,198 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 5.0 (TID 17, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:16:30,198 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 5.0 (TID 17)
2020-04-08 15:16:30,198 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 5.0 (TID 16) in 2 ms on localhost (executor driver) (2/3)
2020-04-08 15:16:30,199 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 154 is the same as ending offset skipping PABUSI2 0
2020-04-08 15:16:30,200 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 5.0 (TID 17). 2785 bytes result sent to driver
2020-04-08 15:16:30,200 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 5.0 (TID 17) in 2 ms on localhost (executor driver) (3/3)
2020-04-08 15:16:30,200 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
2020-04-08 15:16:30,201 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 5 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136) finished in 0.013 s
2020-04-08 15:16:30,201 INFO org.apache.spark.scheduler.DAGScheduler: Job 5 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:136, took 0.015722 s
2020-04-08 15:16:30,201 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586330190000 ms.0 from job set of time 1586330190000 ms
2020-04-08 15:16:30,202 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.201 s for time 1586330190000 ms (execution: 0.021 s)
2020-04-08 15:16:30,202 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 4 from persistence list
2020-04-08 15:16:30,202 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 15:16:30,202 INFO org.apache.spark.storage.BlockManager: Removing RDD 4
2020-04-08 15:16:30,202 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586330170000 ms
2020-04-08 15:16:40,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 15:16:40,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 15:16:40,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 15:16:40,182 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 156.
2020-04-08 15:16:40,182 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 155.
2020-04-08 15:16:40,183 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 154.
2020-04-08 15:16:40,184 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586330200000 ms
2020-04-08 15:16:40,184 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586330200000 ms.0 from job set of time 1586330200000 ms
2020-04-08 15:16:40,190 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:136
2020-04-08 15:16:40,191 INFO org.apache.spark.scheduler.DAGScheduler: Got job 6 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136) with 3 output partitions
2020-04-08 15:16:40,191 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 6 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136)
2020-04-08 15:16:40,191 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 15:16:40,191 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 15:16:40,191 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 6 (KafkaRDD[6] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113), which has no missing parents
2020-04-08 15:16:40,193 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 15:16:40,195 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 15:16:40,196 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.0.102:61516 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 15:16:40,197 INFO org.apache.spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1161
2020-04-08 15:16:40,197 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 6 (KafkaRDD[6] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 15:16:40,197 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 6.0 with 3 tasks
2020-04-08 15:16:40,198 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 18, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:16:40,198 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 6.0 (TID 18)
2020-04-08 15:16:40,199 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 156 is the same as ending offset skipping PABUSI2 2
2020-04-08 15:16:40,200 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 6.0 (TID 18). 2828 bytes result sent to driver
2020-04-08 15:16:40,201 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 6.0 (TID 19, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:16:40,201 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 18) in 3 ms on localhost (executor driver) (1/3)
2020-04-08 15:16:40,201 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 6.0 (TID 19)
2020-04-08 15:16:40,202 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 155 is the same as ending offset skipping PABUSI2 1
2020-04-08 15:16:40,202 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 6.0 (TID 19). 2785 bytes result sent to driver
2020-04-08 15:16:40,203 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 6.0 (TID 20, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:16:40,204 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 6.0 (TID 20)
2020-04-08 15:16:40,204 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 6.0 (TID 19) in 4 ms on localhost (executor driver) (2/3)
2020-04-08 15:16:40,205 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 154 is the same as ending offset skipping PABUSI2 0
2020-04-08 15:16:40,205 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 6.0 (TID 20). 2785 bytes result sent to driver
2020-04-08 15:16:40,206 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 6.0 (TID 20) in 3 ms on localhost (executor driver) (3/3)
2020-04-08 15:16:40,206 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
2020-04-08 15:16:40,206 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 6 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136) finished in 0.014 s
2020-04-08 15:16:40,207 INFO org.apache.spark.scheduler.DAGScheduler: Job 6 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:136, took 0.016555 s
2020-04-08 15:16:40,207 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586330200000 ms.0 from job set of time 1586330200000 ms
2020-04-08 15:16:40,207 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 5 from persistence list
2020-04-08 15:16:40,207 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.207 s for time 1586330200000 ms (execution: 0.023 s)
2020-04-08 15:16:40,208 INFO org.apache.spark.storage.BlockManager: Removing RDD 5
2020-04-08 15:16:40,208 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 15:16:40,208 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586330180000 ms
2020-04-08 15:16:50,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 15:16:50,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 15:16:50,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 15:16:50,177 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 156.
2020-04-08 15:16:50,177 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 155.
2020-04-08 15:16:50,178 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 154.
2020-04-08 15:16:50,179 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586330210000 ms
2020-04-08 15:16:50,179 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586330210000 ms.0 from job set of time 1586330210000 ms
2020-04-08 15:16:50,185 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:136
2020-04-08 15:16:50,185 INFO org.apache.spark.scheduler.DAGScheduler: Got job 7 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136) with 3 output partitions
2020-04-08 15:16:50,186 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 7 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136)
2020-04-08 15:16:50,186 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 15:16:50,186 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 15:16:50,186 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 7 (KafkaRDD[7] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113), which has no missing parents
2020-04-08 15:16:50,188 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 15:16:50,190 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 15:16:50,190 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.0.102:61516 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 15:16:50,191 INFO org.apache.spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1161
2020-04-08 15:16:50,191 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 7 (KafkaRDD[7] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 15:16:50,191 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 7.0 with 3 tasks
2020-04-08 15:16:50,192 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 7.0 (TID 21, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:16:50,192 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 7.0 (TID 21)
2020-04-08 15:16:50,194 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 156 is the same as ending offset skipping PABUSI2 2
2020-04-08 15:16:50,195 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 7.0 (TID 21). 2828 bytes result sent to driver
2020-04-08 15:16:50,195 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 7.0 (TID 22, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:16:50,196 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 7.0 (TID 22)
2020-04-08 15:16:50,196 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 7.0 (TID 21) in 4 ms on localhost (executor driver) (1/3)
2020-04-08 15:16:50,197 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 155 is the same as ending offset skipping PABUSI2 1
2020-04-08 15:16:50,197 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 7.0 (TID 22). 2785 bytes result sent to driver
2020-04-08 15:16:50,198 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 7.0 (TID 23, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:16:50,198 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 7.0 (TID 23)
2020-04-08 15:16:50,198 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 7.0 (TID 22) in 3 ms on localhost (executor driver) (2/3)
2020-04-08 15:16:50,199 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 154 is the same as ending offset skipping PABUSI2 0
2020-04-08 15:16:50,200 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 7.0 (TID 23). 2785 bytes result sent to driver
2020-04-08 15:16:50,200 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 7.0 (TID 23) in 2 ms on localhost (executor driver) (3/3)
2020-04-08 15:16:50,200 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
2020-04-08 15:16:50,201 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 7 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136) finished in 0.014 s
2020-04-08 15:16:50,201 INFO org.apache.spark.scheduler.DAGScheduler: Job 7 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:136, took 0.015985 s
2020-04-08 15:16:50,201 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586330210000 ms.0 from job set of time 1586330210000 ms
2020-04-08 15:16:50,202 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.201 s for time 1586330210000 ms (execution: 0.022 s)
2020-04-08 15:16:50,202 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 6 from persistence list
2020-04-08 15:16:50,202 INFO org.apache.spark.storage.BlockManager: Removing RDD 6
2020-04-08 15:16:50,202 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 15:16:50,202 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586330190000 ms
2020-04-08 15:17:00,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 15:17:00,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 15:17:00,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 15:17:00,180 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 156.
2020-04-08 15:17:00,180 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 155.
2020-04-08 15:17:00,180 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 154.
2020-04-08 15:17:00,181 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586330220000 ms
2020-04-08 15:17:00,181 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586330220000 ms.0 from job set of time 1586330220000 ms
2020-04-08 15:17:00,187 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:136
2020-04-08 15:17:00,187 INFO org.apache.spark.scheduler.DAGScheduler: Got job 8 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136) with 3 output partitions
2020-04-08 15:17:00,187 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 8 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136)
2020-04-08 15:17:00,187 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 15:17:00,187 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 15:17:00,188 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 8 (KafkaRDD[8] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113), which has no missing parents
2020-04-08 15:17:00,190 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 15:17:00,191 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 15:17:00,192 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.0.102:61516 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 15:17:00,192 INFO org.apache.spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1161
2020-04-08 15:17:00,193 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 8 (KafkaRDD[8] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 15:17:00,193 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 8.0 with 3 tasks
2020-04-08 15:17:00,194 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 24, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:17:00,194 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 8.0 (TID 24)
2020-04-08 15:17:00,195 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 156 is the same as ending offset skipping PABUSI2 2
2020-04-08 15:17:00,195 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 8.0 (TID 24). 2785 bytes result sent to driver
2020-04-08 15:17:00,196 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 8.0 (TID 25, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:17:00,196 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 24) in 2 ms on localhost (executor driver) (1/3)
2020-04-08 15:17:00,196 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 8.0 (TID 25)
2020-04-08 15:17:00,197 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 155 is the same as ending offset skipping PABUSI2 1
2020-04-08 15:17:00,198 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 8.0 (TID 25). 2785 bytes result sent to driver
2020-04-08 15:17:00,198 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 8.0 (TID 26, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:17:00,198 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 8.0 (TID 26)
2020-04-08 15:17:00,198 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 8.0 (TID 25) in 2 ms on localhost (executor driver) (2/3)
2020-04-08 15:17:00,199 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 154 is the same as ending offset skipping PABUSI2 0
2020-04-08 15:17:00,200 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 8.0 (TID 26). 2828 bytes result sent to driver
2020-04-08 15:17:00,200 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 8.0 (TID 26) in 2 ms on localhost (executor driver) (3/3)
2020-04-08 15:17:00,201 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
2020-04-08 15:17:00,201 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 8 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136) finished in 0.012 s
2020-04-08 15:17:00,201 INFO org.apache.spark.scheduler.DAGScheduler: Job 8 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:136, took 0.014477 s
2020-04-08 15:17:00,202 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586330220000 ms.0 from job set of time 1586330220000 ms
2020-04-08 15:17:00,202 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 7 from persistence list
2020-04-08 15:17:00,202 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.202 s for time 1586330220000 ms (execution: 0.021 s)
2020-04-08 15:17:00,202 INFO org.apache.spark.storage.BlockManager: Removing RDD 7
2020-04-08 15:17:00,202 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 15:17:00,202 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586330200000 ms
2020-04-08 15:17:10,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 15:17:10,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 15:17:10,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 15:17:10,182 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 155.
2020-04-08 15:17:10,183 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 156.
2020-04-08 15:17:10,184 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 154.
2020-04-08 15:17:10,184 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586330230000 ms
2020-04-08 15:17:10,185 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586330230000 ms.0 from job set of time 1586330230000 ms
2020-04-08 15:17:10,190 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:136
2020-04-08 15:17:10,190 INFO org.apache.spark.scheduler.DAGScheduler: Got job 9 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136) with 3 output partitions
2020-04-08 15:17:10,190 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 9 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136)
2020-04-08 15:17:10,190 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 15:17:10,190 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 15:17:10,191 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 9 (KafkaRDD[9] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113), which has no missing parents
2020-04-08 15:17:10,193 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 15:17:10,194 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 15:17:10,195 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.0.102:61516 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 15:17:10,195 INFO org.apache.spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1161
2020-04-08 15:17:10,196 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 9 (KafkaRDD[9] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:113) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 15:17:10,196 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 9.0 with 3 tasks
2020-04-08 15:17:10,196 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 27, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:17:10,197 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 9.0 (TID 27)
2020-04-08 15:17:10,198 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 156 is the same as ending offset skipping PABUSI2 2
2020-04-08 15:17:10,198 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 9.0 (TID 27). 2785 bytes result sent to driver
2020-04-08 15:17:10,199 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 9.0 (TID 28, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:17:10,199 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 9.0 (TID 28)
2020-04-08 15:17:10,199 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 27) in 3 ms on localhost (executor driver) (1/3)
2020-04-08 15:17:10,200 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 155 is the same as ending offset skipping PABUSI2 1
2020-04-08 15:17:10,200 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 9.0 (TID 28). 2785 bytes result sent to driver
2020-04-08 15:17:10,201 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 9.0 (TID 29, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:17:10,201 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 9.0 (TID 29)
2020-04-08 15:17:10,201 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 9.0 (TID 28) in 3 ms on localhost (executor driver) (2/3)
2020-04-08 15:17:10,202 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 154 is the same as ending offset skipping PABUSI2 0
2020-04-08 15:17:10,202 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 9.0 (TID 29). 2785 bytes result sent to driver
2020-04-08 15:17:10,203 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 9.0 (TID 29) in 3 ms on localhost (executor driver) (3/3)
2020-04-08 15:17:10,203 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
2020-04-08 15:17:10,203 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 9 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:136) finished in 0.011 s
2020-04-08 15:17:10,203 INFO org.apache.spark.scheduler.DAGScheduler: Job 9 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:136, took 0.013711 s
2020-04-08 15:17:10,205 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586330230000 ms.0 from job set of time 1586330230000 ms
2020-04-08 15:17:10,205 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.205 s for time 1586330230000 ms (execution: 0.020 s)
2020-04-08 15:17:10,205 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 8 from persistence list
2020-04-08 15:17:10,206 INFO org.apache.spark.storage.BlockManager: Removing RDD 8
2020-04-08 15:17:10,206 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 15:17:10,206 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586330210000 ms
2020-04-08 15:17:19,108 INFO org.apache.spark.streaming.StreamingContext: Invoking stop(stopGracefully=false) from shutdown hook
2020-04-08 15:17:19,114 INFO org.apache.spark.streaming.scheduler.ReceiverTracker: ReceiverTracker stopped
2020-04-08 15:17:19,115 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopping JobGenerator immediately
2020-04-08 15:17:19,116 INFO org.apache.spark.streaming.util.RecurringTimer: Stopped timer for JobGenerator after time 1586330230000
2020-04-08 15:17:19,294 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Member consumer-1-feec444c-b83a-4d63-954b-3e3735252694 sending LeaveGroup request to coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 15:17:19,476 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopped JobGenerator
2020-04-08 15:17:19,479 INFO org.apache.spark.streaming.scheduler.JobScheduler: Stopped JobScheduler
2020-04-08 15:17:19,483 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@499ef98e{/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 15:17:19,484 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@28501a4b{/streaming/batch,null,UNAVAILABLE,@Spark}
2020-04-08 15:17:19,485 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@2bebb74f{/static/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 15:17:19,485 INFO org.apache.spark.streaming.StreamingContext: StreamingContext stopped successfully
2020-04-08 15:17:19,486 INFO org.apache.spark.SparkContext: Invoking stop() from shutdown hook
2020-04-08 15:17:19,497 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@533b266e{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 15:17:19,498 INFO org.apache.spark.ui.SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
2020-04-08 15:17:19,499 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 188
2020-04-08 15:17:19,504 INFO org.apache.spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2020-04-08 15:17:19,518 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2020-04-08 15:17:19,518 INFO org.apache.spark.storage.BlockManager: BlockManager stopped
2020-04-08 15:17:19,518 INFO org.apache.spark.storage.BlockManagerMaster: BlockManagerMaster stopped
2020-04-08 15:17:19,520 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2020-04-08 15:17:19,527 INFO org.apache.spark.SparkContext: Successfully stopped SparkContext
2020-04-08 15:17:19,528 INFO org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2020-04-08 15:17:19,528 INFO org.apache.spark.util.ShutdownHookManager: Deleting directory /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/spark-3122f2a9-7c53-484b-aa79-84d33249007c
2020-04-08 15:52:01,475 WARN org.apache.spark.util.Utils: Your hostname, MacBook-Pro-CW.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)
2020-04-08 15:52:01,478 WARN org.apache.spark.util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
2020-04-08 15:52:01,510 INFO org.apache.spark.SparkContext: Running Spark version 2.4.0.cloudera2
2020-04-08 15:52:01,710 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-04-08 15:52:01,832 INFO org.apache.spark.SparkContext: Submitted application: Kafka2SparkStreaming2Kudu-kerberos
2020-04-08 15:52:01,871 INFO org.apache.spark.SecurityManager: Changing view acls to: godiswc
2020-04-08 15:52:01,872 INFO org.apache.spark.SecurityManager: Changing modify acls to: godiswc
2020-04-08 15:52:01,872 INFO org.apache.spark.SecurityManager: Changing view acls groups to: 
2020-04-08 15:52:01,873 INFO org.apache.spark.SecurityManager: Changing modify acls groups to: 
2020-04-08 15:52:01,873 INFO org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(godiswc); groups with view permissions: Set(); users  with modify permissions: Set(godiswc); groups with modify permissions: Set()
2020-04-08 15:52:02,007 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 15:52:02,084 INFO org.apache.spark.util.Utils: Successfully started service 'sparkDriver' on port 53414.
2020-04-08 15:52:02,099 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
2020-04-08 15:52:02,110 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
2020-04-08 15:52:02,130 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-04-08 15:52:02,131 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2020-04-08 15:52:02,143 INFO org.apache.spark.storage.DiskBlockManager: Created local directory at /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/blockmgr-5ce4f0e5-3606-49d1-a5a4-d415180ef2fa
2020-04-08 15:52:02,156 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 4.1 GB
2020-04-08 15:52:02,165 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
2020-04-08 15:52:02,215 INFO org.spark_project.jetty.util.log: Logging initialized @1510ms
2020-04-08 15:52:02,270 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2019-04-30T04:42:08+08:00, git hash: e1bc35120a6617ee3df052294e433f3a25ce7097
2020-04-08 15:52:02,286 INFO org.spark_project.jetty.server.Server: Started @1581ms
2020-04-08 15:52:02,288 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 15:52:02,303 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@89ff02e{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 15:52:02,303 INFO org.apache.spark.util.Utils: Successfully started service 'SparkUI' on port 4040.
2020-04-08 15:52:02,326 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5d5d9e5{/jobs,null,AVAILABLE,@Spark}
2020-04-08 15:52:02,330 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6fff253c{/jobs/json,null,AVAILABLE,@Spark}
2020-04-08 15:52:02,331 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c6357f9{/jobs/job,null,AVAILABLE,@Spark}
2020-04-08 15:52:02,335 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f94c4db{/jobs/job/json,null,AVAILABLE,@Spark}
2020-04-08 15:52:02,335 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@593e824f{/stages,null,AVAILABLE,@Spark}
2020-04-08 15:52:02,336 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@72ccd81a{/stages/json,null,AVAILABLE,@Spark}
2020-04-08 15:52:02,336 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6d8792db{/stages/stage,null,AVAILABLE,@Spark}
2020-04-08 15:52:02,338 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5d25e6bb{/stages/stage/json,null,AVAILABLE,@Spark}
2020-04-08 15:52:02,339 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@ce5a68e{/stages/pool,null,AVAILABLE,@Spark}
2020-04-08 15:52:02,339 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@9d157ff{/stages/pool/json,null,AVAILABLE,@Spark}
2020-04-08 15:52:02,340 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f162cc0{/storage,null,AVAILABLE,@Spark}
2020-04-08 15:52:02,340 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5df417a7{/storage/json,null,AVAILABLE,@Spark}
2020-04-08 15:52:02,341 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7c041b41{/storage/rdd,null,AVAILABLE,@Spark}
2020-04-08 15:52:02,341 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7f69d591{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-04-08 15:52:02,342 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@61078690{/environment,null,AVAILABLE,@Spark}
2020-04-08 15:52:02,342 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1cb3ec38{/environment/json,null,AVAILABLE,@Spark}
2020-04-08 15:52:02,343 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@403132fc{/executors,null,AVAILABLE,@Spark}
2020-04-08 15:52:02,343 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@71c5b236{/executors/json,null,AVAILABLE,@Spark}
2020-04-08 15:52:02,344 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2cab9998{/executors/threadDump,null,AVAILABLE,@Spark}
2020-04-08 15:52:02,344 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f7a7219{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-04-08 15:52:02,349 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@669513d8{/static,null,AVAILABLE,@Spark}
2020-04-08 15:52:02,350 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@54361a9{/,null,AVAILABLE,@Spark}
2020-04-08 15:52:02,351 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@32232e55{/api,null,AVAILABLE,@Spark}
2020-04-08 15:52:02,351 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2416a51{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-04-08 15:52:02,352 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6fa590ba{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-04-08 15:52:02,353 INFO org.apache.spark.ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
2020-04-08 15:52:02,436 INFO org.apache.spark.executor.Executor: Starting executor ID driver on host localhost
2020-04-08 15:52:02,510 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 15:52:02,511 INFO org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53416.
2020-04-08 15:52:02,512 INFO org.apache.spark.network.netty.NettyBlockTransferService: Server created on 192.168.0.102:53416
2020-04-08 15:52:02,514 INFO org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-04-08 15:52:02,540 INFO org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 53416, None)
2020-04-08 15:52:02,542 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:53416 with 4.1 GB RAM, BlockManagerId(driver, 192.168.0.102, 53416, None)
2020-04-08 15:52:02,544 INFO org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 53416, None)
2020-04-08 15:52:02,545 INFO org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 53416, None)
2020-04-08 15:52:02,703 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5488b5c5{/metrics/json,null,AVAILABLE,@Spark}
2020-04-08 15:52:02,787 WARN org.apache.spark.streaming.StreamingContext: spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
2020-04-08 15:52:02,975 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding enable.auto.commit to false for executor
2020-04-08 15:52:02,976 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding auto.offset.reset to none for executor
2020-04-08 15:52:02,976 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding executor group.id to spark-executor-testgroup1
2020-04-08 15:52:02,977 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
2020-04-08 15:52:03,556 WARN org.apache.kudu.util.NetUtil: Slow DNS lookup! Resolved IP of `cw-instances-2.vpc.cloudera.com' to cw-instances-2.vpc.cloudera.com/10.65.1.47 in 159798217ns
2020-04-08 15:52:03,816 WARN org.apache.kudu.util.NetUtil: Slow DNS lookup! Resolved IP of `cw-instances-3.vpc.cloudera.com' to cw-instances-3.vpc.cloudera.com/10.65.9.108 in 182376951ns
2020-04-08 15:52:03,976 WARN org.apache.kudu.util.NetUtil: Slow DNS lookup! Resolved IP of `cw-instances-4.vpc.cloudera.com' to cw-instances-4.vpc.cloudera.com/10.65.12.223 in 157749224ns
2020-04-08 15:52:07,090 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Slide time = 10000 ms
2020-04-08 15:52:07,091 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Storage level = Serialized 1x Replicated
2020-04-08 15:52:07,092 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Checkpoint interval = null
2020-04-08 15:52:07,092 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Remember interval = 10000 ms
2020-04-08 15:52:07,092 INFO org.apache.spark.streaming.kafka010.DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@7b306ae5
2020-04-08 15:52:07,093 INFO org.apache.spark.streaming.dstream.ForEachDStream: Slide time = 10000 ms
2020-04-08 15:52:07,093 INFO org.apache.spark.streaming.dstream.ForEachDStream: Storage level = Serialized 1x Replicated
2020-04-08 15:52:07,093 INFO org.apache.spark.streaming.dstream.ForEachDStream: Checkpoint interval = null
2020-04-08 15:52:07,093 INFO org.apache.spark.streaming.dstream.ForEachDStream: Remember interval = 10000 ms
2020-04-08 15:52:07,093 INFO org.apache.spark.streaming.dstream.ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@25b34eb8
2020-04-08 15:52:07,154 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = testgroup1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-08 15:52:07,235 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-08 15:52:07,236 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-08 15:52:07,236 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586332327234
2020-04-08 15:52:07,238 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-1, groupId=testgroup1] Subscribed to topic(s): PABUSI2
2020-04-08 15:52:07,995 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-1, groupId=testgroup1] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-08 15:52:07,996 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Discovered group coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 15:52:07,999 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Revoking previously assigned partitions []
2020-04-08 15:52:08,000 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 15:52:08,533 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] (Re-)joining group
2020-04-08 15:52:11,909 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Successfully joined group with generation 51
2020-04-08 15:52:11,915 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting newly assigned partitions: PABUSI2-2, PABUSI2-0, PABUSI2-1
2020-04-08 15:52:12,114 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-2 to the committed offset FetchPosition{offset=156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-2.vpc.cloudera.com:9092 (id: 52 rack: null), epoch=0}}
2020-04-08 15:52:12,116 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-0 to the committed offset FetchPosition{offset=154, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-4.vpc.cloudera.com:9092 (id: 53 rack: null), epoch=2}}
2020-04-08 15:52:12,116 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Setting offset for partition PABUSI2-1 to the committed offset FetchPosition{offset=155, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=cw-instances-3.vpc.cloudera.com:9092 (id: 54 rack: null), epoch=2}}
2020-04-08 15:52:12,125 INFO org.apache.spark.streaming.util.RecurringTimer: Started timer for JobGenerator at time 1586332330000
2020-04-08 15:52:12,126 INFO org.apache.spark.streaming.scheduler.JobGenerator: Started JobGenerator at 1586332330000 ms
2020-04-08 15:52:12,127 INFO org.apache.spark.streaming.scheduler.JobScheduler: Started JobScheduler
2020-04-08 15:52:12,131 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@93f432e{/streaming,null,AVAILABLE,@Spark}
2020-04-08 15:52:12,132 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@288214b1{/streaming/json,null,AVAILABLE,@Spark}
2020-04-08 15:52:12,133 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@639aba11{/streaming/batch,null,AVAILABLE,@Spark}
2020-04-08 15:52:12,133 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@341672e{/streaming/batch/json,null,AVAILABLE,@Spark}
2020-04-08 15:52:12,134 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6e33fcae{/static/streaming,null,AVAILABLE,@Spark}
2020-04-08 15:52:12,135 INFO org.apache.spark.streaming.StreamingContext: StreamingContext started
2020-04-08 15:52:12,167 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 15:52:12,167 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 15:52:12,168 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 15:52:12,868 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 156.
2020-04-08 15:52:13,242 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 156.
2020-04-08 15:52:13,243 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 154.
2020-04-08 15:52:13,293 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586332330000 ms
2020-04-08 15:52:13,298 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586332330000 ms.0 from job set of time 1586332330000 ms
2020-04-08 15:52:13,350 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:137
2020-04-08 15:52:13,373 INFO org.apache.spark.scheduler.DAGScheduler: Got job 0 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137) with 3 output partitions
2020-04-08 15:52:13,374 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 0 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137)
2020-04-08 15:52:13,374 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 15:52:13,375 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 15:52:13,380 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 0 (KafkaRDD[0] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:114), which has no missing parents
2020-04-08 15:52:13,473 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 15:52:13,891 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.4 KB, free 4.1 GB)
2020-04-08 15:52:13,900 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.102:53416 (size: 3.4 KB, free: 4.1 GB)
2020-04-08 15:52:13,905 INFO org.apache.spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2020-04-08 15:52:13,929 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 0 (KafkaRDD[0] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:114) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 15:52:13,930 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 0.0 with 3 tasks
2020-04-08 15:52:14,056 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:52:14,067 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
2020-04-08 15:52:14,101 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Computing topic PABUSI2, partition 1 offsets 155 -> 156
2020-04-08 15:52:14,105 INFO org.apache.spark.streaming.kafka010.KafkaDataConsumer: Initializing cache 16 64 0.75
2020-04-08 15:52:14,111 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-testgroup1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-08 15:52:14,121 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-08 15:52:14,122 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-08 15:52:14,122 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586332334120
2020-04-08 15:52:14,126 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Subscribed to partition(s): PABUSI2-1
2020-04-08 15:52:14,132 INFO org.apache.spark.streaming.kafka010.InternalKafkaConsumer: Initial fetch for spark-executor-testgroup1 PABUSI2-1 155
2020-04-08 15:52:14,133 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Seeking to offset 155 for partition PABUSI2-1
2020-04-08 15:52:14,674 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-2, groupId=spark-executor-testgroup1] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-08 15:52:17,996 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 2880 bytes result sent to driver
2020-04-08 15:52:17,998 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:52:17,999 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 0.0 (TID 1)
2020-04-08 15:52:18,001 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 4008 ms on localhost (executor driver) (1/3)
2020-04-08 15:52:18,002 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 156 is the same as ending offset skipping PABUSI2 2
2020-04-08 15:52:18,004 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 0.0 (TID 1). 2828 bytes result sent to driver
2020-04-08 15:52:18,006 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:52:18,006 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 0.0 (TID 2)
2020-04-08 15:52:18,006 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 9 ms on localhost (executor driver) (2/3)
2020-04-08 15:52:18,009 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 154 is the same as ending offset skipping PABUSI2 0
2020-04-08 15:52:18,010 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 0.0 (TID 2). 2828 bytes result sent to driver
2020-04-08 15:52:18,013 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 8 ms on localhost (executor driver) (3/3)
2020-04-08 15:52:18,014 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-04-08 15:52:18,015 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 0 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137) finished in 4.612 s
2020-04-08 15:52:18,020 INFO org.apache.spark.scheduler.DAGScheduler: Job 0 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:137, took 4.669756 s
2020-04-08 15:52:18,023 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586332330000 ms.0 from job set of time 1586332330000 ms
2020-04-08 15:52:18,025 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 8.023 s for time 1586332330000 ms (execution: 4.727 s)
2020-04-08 15:52:18,030 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 15:52:18,034 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 
2020-04-08 15:52:20,009 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 15:52:20,009 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 15:52:20,009 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 15:52:20,189 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 156.
2020-04-08 15:52:20,190 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 156.
2020-04-08 15:52:20,193 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 154.
2020-04-08 15:52:20,193 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586332340000 ms
2020-04-08 15:52:20,194 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586332340000 ms.0 from job set of time 1586332340000 ms
2020-04-08 15:52:20,203 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:137
2020-04-08 15:52:20,205 INFO org.apache.spark.scheduler.DAGScheduler: Got job 1 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137) with 3 output partitions
2020-04-08 15:52:20,205 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 1 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137)
2020-04-08 15:52:20,205 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 15:52:20,206 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 15:52:20,207 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 1 (KafkaRDD[1] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:114), which has no missing parents
2020-04-08 15:52:20,211 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 15:52:20,213 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 15:52:20,214 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.102:53416 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 15:52:20,215 INFO org.apache.spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2020-04-08 15:52:20,217 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 1 (KafkaRDD[1] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:114) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 15:52:20,217 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 1.0 with 3 tasks
2020-04-08 15:52:20,219 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:52:20,219 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 1.0 (TID 3)
2020-04-08 15:52:20,225 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 156 is the same as ending offset skipping PABUSI2 1
2020-04-08 15:52:20,227 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 1.0 (TID 3). 2871 bytes result sent to driver
2020-04-08 15:52:20,230 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 4, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:52:20,230 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 1.0 (TID 4)
2020-04-08 15:52:20,231 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 3) in 13 ms on localhost (executor driver) (1/3)
2020-04-08 15:52:20,234 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 156 is the same as ending offset skipping PABUSI2 2
2020-04-08 15:52:20,236 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 1.0 (TID 4). 2828 bytes result sent to driver
2020-04-08 15:52:20,237 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 1.0 (TID 5, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:52:20,238 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 1.0 (TID 5)
2020-04-08 15:52:20,238 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 4) in 9 ms on localhost (executor driver) (2/3)
2020-04-08 15:52:20,244 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 154 is the same as ending offset skipping PABUSI2 0
2020-04-08 15:52:20,245 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 1.0 (TID 5). 2828 bytes result sent to driver
2020-04-08 15:52:20,247 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 1.0 (TID 5) in 10 ms on localhost (executor driver) (3/3)
2020-04-08 15:52:20,247 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
2020-04-08 15:52:20,248 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 1 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137) finished in 0.038 s
2020-04-08 15:52:20,248 INFO org.apache.spark.scheduler.DAGScheduler: Job 1 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:137, took 0.044788 s
2020-04-08 15:52:20,248 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586332340000 ms.0 from job set of time 1586332340000 ms
2020-04-08 15:52:20,249 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.248 s for time 1586332340000 ms (execution: 0.054 s)
2020-04-08 15:52:20,250 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 0 from persistence list
2020-04-08 15:52:20,256 INFO org.apache.spark.storage.BlockManager: Removing RDD 0
2020-04-08 15:52:20,257 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 15:52:20,257 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 
2020-04-08 15:52:30,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 15:52:30,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 15:52:30,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 15:52:30,185 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 163.
2020-04-08 15:52:30,186 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 163.
2020-04-08 15:52:30,186 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 160.
2020-04-08 15:52:30,187 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586332350000 ms
2020-04-08 15:52:30,188 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586332350000 ms.0 from job set of time 1586332350000 ms
2020-04-08 15:52:30,200 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:137
2020-04-08 15:52:30,202 INFO org.apache.spark.scheduler.DAGScheduler: Got job 2 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137) with 3 output partitions
2020-04-08 15:52:30,202 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 2 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137)
2020-04-08 15:52:30,202 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 15:52:30,203 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 15:52:30,204 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 2 (KafkaRDD[2] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:114), which has no missing parents
2020-04-08 15:52:30,208 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 15:52:30,210 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 15:52:30,211 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.102:53416 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 15:52:30,211 INFO org.apache.spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2020-04-08 15:52:30,212 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 2 (KafkaRDD[2] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:114) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 15:52:30,212 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 2.0 with 3 tasks
2020-04-08 15:52:30,214 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:52:30,214 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 2.0 (TID 6)
2020-04-08 15:52:30,217 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Computing topic PABUSI2, partition 1 offsets 156 -> 163
2020-04-08 15:52:34,598 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 10
2020-04-08 15:52:34,598 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 35
2020-04-08 15:52:34,598 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 38
2020-04-08 15:52:34,598 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 23
2020-04-08 15:52:34,598 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 50
2020-04-08 15:52:34,598 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 18
2020-04-08 15:52:34,598 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 26
2020-04-08 15:52:34,598 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 12
2020-04-08 15:52:34,598 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 46
2020-04-08 15:52:34,598 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 16
2020-04-08 15:52:34,598 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 14
2020-04-08 15:52:34,599 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 40
2020-04-08 15:52:34,599 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 39
2020-04-08 15:52:34,599 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 47
2020-04-08 15:52:34,599 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 2
2020-04-08 15:52:34,599 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 32
2020-04-08 15:52:34,599 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 22
2020-04-08 15:52:34,599 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 37
2020-04-08 15:52:34,599 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 45
2020-04-08 15:52:34,599 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 7
2020-04-08 15:52:34,599 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 4
2020-04-08 15:52:34,599 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 6
2020-04-08 15:52:34,605 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.102:53416 in memory (size: 3.4 KB, free: 4.1 GB)
2020-04-08 15:52:34,607 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 1
2020-04-08 15:52:34,608 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.102:53416 in memory (size: 3.3 KB, free: 4.1 GB)
2020-04-08 15:52:34,609 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 21
2020-04-08 15:52:34,610 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 15
2020-04-08 15:52:34,610 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 34
2020-04-08 15:52:34,610 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 41
2020-04-08 15:52:34,610 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 30
2020-04-08 15:52:34,610 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 13
2020-04-08 15:52:34,610 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 5
2020-04-08 15:52:34,610 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 36
2020-04-08 15:52:34,610 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 49
2020-04-08 15:52:34,610 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 24
2020-04-08 15:52:34,610 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 28
2020-04-08 15:52:34,610 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 3
2020-04-08 15:52:34,610 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 33
2020-04-08 15:52:34,610 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 48
2020-04-08 15:52:34,610 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 44
2020-04-08 15:52:34,610 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 25
2020-04-08 15:52:34,611 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 43
2020-04-08 15:52:34,611 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 19
2020-04-08 15:52:34,611 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 29
2020-04-08 15:52:34,611 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 31
2020-04-08 15:52:34,611 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 9
2020-04-08 15:52:34,611 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 42
2020-04-08 15:52:34,611 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 27
2020-04-08 15:52:34,611 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 8
2020-04-08 15:52:34,611 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 11
2020-04-08 15:52:34,611 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 20
2020-04-08 15:52:34,611 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 17
2020-04-08 15:52:40,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 15:52:40,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 15:52:40,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 15:52:40,179 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 163.
2020-04-08 15:52:40,179 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 163.
2020-04-08 15:52:40,180 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 160.
2020-04-08 15:52:40,181 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586332360000 ms
2020-04-08 15:52:40,338 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 2.0 (TID 6). 2880 bytes result sent to driver
2020-04-08 15:52:40,340 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 2.0 (TID 7, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:52:40,340 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 2.0 (TID 7)
2020-04-08 15:52:40,340 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 6) in 10126 ms on localhost (executor driver) (1/3)
2020-04-08 15:52:40,342 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Computing topic PABUSI2, partition 2 offsets 156 -> 163
2020-04-08 15:52:40,343 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-testgroup1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-08 15:52:40,348 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-08 15:52:40,348 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-08 15:52:40,348 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586332360348
2020-04-08 15:52:40,349 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-3, groupId=spark-executor-testgroup1] Subscribed to partition(s): PABUSI2-2
2020-04-08 15:52:40,349 INFO org.apache.spark.streaming.kafka010.InternalKafkaConsumer: Initial fetch for spark-executor-testgroup1 PABUSI2-2 156
2020-04-08 15:52:40,349 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-3, groupId=spark-executor-testgroup1] Seeking to offset 156 for partition PABUSI2-2
2020-04-08 15:52:40,883 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-3, groupId=spark-executor-testgroup1] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-08 15:52:49,774 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 2.0 (TID 7). 2880 bytes result sent to driver
2020-04-08 15:52:49,775 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 2.0 (TID 8, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:52:49,776 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 2.0 (TID 8)
2020-04-08 15:52:49,776 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 2.0 (TID 7) in 9437 ms on localhost (executor driver) (2/3)
2020-04-08 15:52:49,777 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Computing topic PABUSI2, partition 0 offsets 154 -> 160
2020-04-08 15:52:49,778 INFO org.apache.kafka.clients.consumer.ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092, cw-instances-2.vpc.cloudera.com:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-testgroup1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-04-08 15:52:49,781 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka version: 2.3.0.7.0.3.0-79
2020-04-08 15:52:49,781 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka commitId: 7df36e7ddd0676a5
2020-04-08 15:52:49,781 INFO org.apache.kafka.common.utils.AppInfoParser: Kafka startTimeMs: 1586332369781
2020-04-08 15:52:49,781 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-4, groupId=spark-executor-testgroup1] Subscribed to partition(s): PABUSI2-0
2020-04-08 15:52:49,782 INFO org.apache.spark.streaming.kafka010.InternalKafkaConsumer: Initial fetch for spark-executor-testgroup1 PABUSI2-0 154
2020-04-08 15:52:49,782 INFO org.apache.kafka.clients.consumer.KafkaConsumer: [Consumer clientId=consumer-4, groupId=spark-executor-testgroup1] Seeking to offset 154 for partition PABUSI2-0
2020-04-08 15:52:50,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 15:52:50,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 15:52:50,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 15:52:50,186 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 163.
2020-04-08 15:52:50,186 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 163.
2020-04-08 15:52:50,187 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 160.
2020-04-08 15:52:50,188 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586332370000 ms
2020-04-08 15:52:50,325 INFO org.apache.kafka.clients.Metadata: [Consumer clientId=consumer-4, groupId=spark-executor-testgroup1] Cluster ID: FVbT0ReOQ4y4c-fMJi_MOg
2020-04-08 15:52:55,764 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 2.0 (TID 8). 2837 bytes result sent to driver
2020-04-08 15:52:55,765 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 2.0 (TID 8) in 5990 ms on localhost (executor driver) (3/3)
2020-04-08 15:52:55,765 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
2020-04-08 15:52:55,766 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 2 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137) finished in 25.560 s
2020-04-08 15:52:55,766 INFO org.apache.spark.scheduler.DAGScheduler: Job 2 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:137, took 25.566552 s
2020-04-08 15:52:55,767 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586332350000 ms.0 from job set of time 1586332350000 ms
2020-04-08 15:52:55,767 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 25.767 s for time 1586332350000 ms (execution: 25.580 s)
2020-04-08 15:52:55,767 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 1 from persistence list
2020-04-08 15:52:55,767 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586332360000 ms.0 from job set of time 1586332360000 ms
2020-04-08 15:52:55,768 INFO org.apache.spark.storage.BlockManager: Removing RDD 1
2020-04-08 15:52:55,768 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 15:52:55,768 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586332330000 ms
2020-04-08 15:52:55,774 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:137
2020-04-08 15:52:55,775 INFO org.apache.spark.scheduler.DAGScheduler: Got job 3 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137) with 3 output partitions
2020-04-08 15:52:55,775 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 3 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137)
2020-04-08 15:52:55,775 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 15:52:55,775 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 15:52:55,776 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 3 (KafkaRDD[3] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:114), which has no missing parents
2020-04-08 15:52:55,778 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 15:52:55,779 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 15:52:55,780 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.102:53416 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 15:52:55,780 INFO org.apache.spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2020-04-08 15:52:55,781 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 3 (KafkaRDD[3] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:114) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 15:52:55,781 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 3.0 with 3 tasks
2020-04-08 15:52:55,782 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 9, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:52:55,782 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 3.0 (TID 9)
2020-04-08 15:52:55,784 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 163 is the same as ending offset skipping PABUSI2 1
2020-04-08 15:52:55,785 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 3.0 (TID 9). 2828 bytes result sent to driver
2020-04-08 15:52:55,785 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 3.0 (TID 10, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:52:55,785 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 3.0 (TID 10)
2020-04-08 15:52:55,785 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 9) in 3 ms on localhost (executor driver) (1/3)
2020-04-08 15:52:55,787 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 163 is the same as ending offset skipping PABUSI2 2
2020-04-08 15:52:55,787 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 3.0 (TID 10). 2785 bytes result sent to driver
2020-04-08 15:52:55,788 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 3.0 (TID 11, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:52:55,788 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 3.0 (TID 11)
2020-04-08 15:52:55,788 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 3.0 (TID 10) in 3 ms on localhost (executor driver) (2/3)
2020-04-08 15:52:55,789 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 160 is the same as ending offset skipping PABUSI2 0
2020-04-08 15:52:55,790 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 3.0 (TID 11). 2785 bytes result sent to driver
2020-04-08 15:52:55,790 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 3.0 (TID 11) in 2 ms on localhost (executor driver) (3/3)
2020-04-08 15:52:55,790 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
2020-04-08 15:52:55,791 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 3 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137) finished in 0.014 s
2020-04-08 15:52:55,791 INFO org.apache.spark.scheduler.DAGScheduler: Job 3 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:137, took 0.016487 s
2020-04-08 15:52:55,791 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586332360000 ms.0 from job set of time 1586332360000 ms
2020-04-08 15:52:55,792 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 15.791 s for time 1586332360000 ms (execution: 0.024 s)
2020-04-08 15:52:55,792 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 2 from persistence list
2020-04-08 15:52:55,792 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586332370000 ms.0 from job set of time 1586332370000 ms
2020-04-08 15:52:55,792 INFO org.apache.spark.storage.BlockManager: Removing RDD 2
2020-04-08 15:52:55,792 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 15:52:55,793 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586332340000 ms
2020-04-08 15:52:55,798 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:137
2020-04-08 15:52:55,799 INFO org.apache.spark.scheduler.DAGScheduler: Got job 4 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137) with 3 output partitions
2020-04-08 15:52:55,799 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 4 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137)
2020-04-08 15:52:55,799 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 15:52:55,799 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 15:52:55,800 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 4 (KafkaRDD[4] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:114), which has no missing parents
2020-04-08 15:52:55,802 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 15:52:55,804 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 15:52:55,804 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.102:53416 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 15:52:55,805 INFO org.apache.spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1161
2020-04-08 15:52:55,805 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 4 (KafkaRDD[4] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:114) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 15:52:55,805 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 4.0 with 3 tasks
2020-04-08 15:52:55,806 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 12, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:52:55,806 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 4.0 (TID 12)
2020-04-08 15:52:55,807 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 163 is the same as ending offset skipping PABUSI2 1
2020-04-08 15:52:55,808 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 4.0 (TID 12). 2828 bytes result sent to driver
2020-04-08 15:52:55,808 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 4.0 (TID 13, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:52:55,809 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 4.0 (TID 13)
2020-04-08 15:52:55,809 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 12) in 3 ms on localhost (executor driver) (1/3)
2020-04-08 15:52:55,810 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 163 is the same as ending offset skipping PABUSI2 2
2020-04-08 15:52:55,810 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 4.0 (TID 13). 2785 bytes result sent to driver
2020-04-08 15:52:55,811 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 4.0 (TID 14, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:52:55,811 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 4.0 (TID 14)
2020-04-08 15:52:55,811 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 4.0 (TID 13) in 3 ms on localhost (executor driver) (2/3)
2020-04-08 15:52:55,812 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 160 is the same as ending offset skipping PABUSI2 0
2020-04-08 15:52:55,813 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 4.0 (TID 14). 2785 bytes result sent to driver
2020-04-08 15:52:55,814 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 4.0 (TID 14) in 3 ms on localhost (executor driver) (3/3)
2020-04-08 15:52:55,814 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
2020-04-08 15:52:55,814 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 4 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137) finished in 0.013 s
2020-04-08 15:52:55,815 INFO org.apache.spark.scheduler.DAGScheduler: Job 4 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:137, took 0.016658 s
2020-04-08 15:52:55,815 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586332370000 ms.0 from job set of time 1586332370000 ms
2020-04-08 15:52:55,815 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 5.815 s for time 1586332370000 ms (execution: 0.023 s)
2020-04-08 15:52:55,815 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 3 from persistence list
2020-04-08 15:52:55,816 INFO org.apache.spark.storage.BlockManager: Removing RDD 3
2020-04-08 15:52:55,816 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 15:52:55,816 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586332350000 ms
2020-04-08 15:53:00,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 15:53:00,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 15:53:00,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 15:53:00,177 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 163.
2020-04-08 15:53:00,177 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 163.
2020-04-08 15:53:00,177 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 160.
2020-04-08 15:53:00,178 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586332380000 ms
2020-04-08 15:53:00,178 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586332380000 ms.0 from job set of time 1586332380000 ms
2020-04-08 15:53:00,183 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:137
2020-04-08 15:53:00,184 INFO org.apache.spark.scheduler.DAGScheduler: Got job 5 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137) with 3 output partitions
2020-04-08 15:53:00,184 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 5 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137)
2020-04-08 15:53:00,184 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 15:53:00,184 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 15:53:00,184 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 5 (KafkaRDD[5] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:114), which has no missing parents
2020-04-08 15:53:00,186 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 15:53:00,187 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 15:53:00,188 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.102:53416 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 15:53:00,188 INFO org.apache.spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1161
2020-04-08 15:53:00,189 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 5 (KafkaRDD[5] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:114) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 15:53:00,189 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 5.0 with 3 tasks
2020-04-08 15:53:00,190 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 15, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:53:00,190 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 5.0 (TID 15)
2020-04-08 15:53:00,193 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 163 is the same as ending offset skipping PABUSI2 1
2020-04-08 15:53:00,194 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 5.0 (TID 15). 2828 bytes result sent to driver
2020-04-08 15:53:00,194 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 5.0 (TID 16, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:53:00,194 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 5.0 (TID 16)
2020-04-08 15:53:00,195 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 15) in 5 ms on localhost (executor driver) (1/3)
2020-04-08 15:53:00,196 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 163 is the same as ending offset skipping PABUSI2 2
2020-04-08 15:53:00,197 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 5.0 (TID 16). 2828 bytes result sent to driver
2020-04-08 15:53:00,198 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 5.0 (TID 17, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:53:00,198 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 5.0 (TID 17)
2020-04-08 15:53:00,198 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 5.0 (TID 16) in 4 ms on localhost (executor driver) (2/3)
2020-04-08 15:53:00,199 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 160 is the same as ending offset skipping PABUSI2 0
2020-04-08 15:53:00,200 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 5.0 (TID 17). 2785 bytes result sent to driver
2020-04-08 15:53:00,201 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 5.0 (TID 17) in 3 ms on localhost (executor driver) (3/3)
2020-04-08 15:53:00,201 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
2020-04-08 15:53:00,202 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 5 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137) finished in 0.017 s
2020-04-08 15:53:00,202 INFO org.apache.spark.scheduler.DAGScheduler: Job 5 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:137, took 0.018686 s
2020-04-08 15:53:00,202 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586332380000 ms.0 from job set of time 1586332380000 ms
2020-04-08 15:53:00,203 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.202 s for time 1586332380000 ms (execution: 0.024 s)
2020-04-08 15:53:00,203 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 4 from persistence list
2020-04-08 15:53:00,203 INFO org.apache.spark.storage.BlockManager: Removing RDD 4
2020-04-08 15:53:00,203 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 15:53:00,203 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586332360000 ms
2020-04-08 15:53:10,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 15:53:10,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 15:53:10,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 15:53:10,179 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 163.
2020-04-08 15:53:10,181 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 163.
2020-04-08 15:53:10,181 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 160.
2020-04-08 15:53:10,182 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586332390000 ms
2020-04-08 15:53:10,182 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586332390000 ms.0 from job set of time 1586332390000 ms
2020-04-08 15:53:10,188 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:137
2020-04-08 15:53:10,189 INFO org.apache.spark.scheduler.DAGScheduler: Got job 6 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137) with 3 output partitions
2020-04-08 15:53:10,189 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 6 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137)
2020-04-08 15:53:10,189 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 15:53:10,189 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 15:53:10,190 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 6 (KafkaRDD[6] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:114), which has no missing parents
2020-04-08 15:53:10,191 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 15:53:10,197 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 82
2020-04-08 15:53:10,197 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 104
2020-04-08 15:53:10,197 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 136
2020-04-08 15:53:10,197 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 123
2020-04-08 15:53:10,197 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 128
2020-04-08 15:53:10,197 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 118
2020-04-08 15:53:10,197 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 130
2020-04-08 15:53:10,197 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 134
2020-04-08 15:53:10,197 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 145
2020-04-08 15:53:10,197 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 110
2020-04-08 15:53:10,197 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 58
2020-04-08 15:53:10,197 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 69
2020-04-08 15:53:10,198 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 62
2020-04-08 15:53:10,198 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 126
2020-04-08 15:53:10,198 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 121
2020-04-08 15:53:10,198 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 66
2020-04-08 15:53:10,198 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 93
2020-04-08 15:53:10,198 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 15:53:10,198 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 68
2020-04-08 15:53:10,198 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 79
2020-04-08 15:53:10,198 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 108
2020-04-08 15:53:10,198 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 142
2020-04-08 15:53:10,198 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 135
2020-04-08 15:53:10,198 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 70
2020-04-08 15:53:10,198 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 139
2020-04-08 15:53:10,198 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 91
2020-04-08 15:53:10,198 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 143
2020-04-08 15:53:10,198 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 132
2020-04-08 15:53:10,198 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 102
2020-04-08 15:53:10,199 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 55
2020-04-08 15:53:10,199 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 78
2020-04-08 15:53:10,199 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.0.102:53416 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 15:53:10,199 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 96
2020-04-08 15:53:10,199 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 85
2020-04-08 15:53:10,199 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 59
2020-04-08 15:53:10,199 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 133
2020-04-08 15:53:10,199 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 75
2020-04-08 15:53:10,199 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 147
2020-04-08 15:53:10,199 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 61
2020-04-08 15:53:10,199 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 92
2020-04-08 15:53:10,199 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 149
2020-04-08 15:53:10,199 INFO org.apache.spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1161
2020-04-08 15:53:10,199 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 117
2020-04-08 15:53:10,199 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 60
2020-04-08 15:53:10,200 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 103
2020-04-08 15:53:10,200 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 115
2020-04-08 15:53:10,200 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 72
2020-04-08 15:53:10,200 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 95
2020-04-08 15:53:10,200 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 127
2020-04-08 15:53:10,200 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 140
2020-04-08 15:53:10,200 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 6 (KafkaRDD[6] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:114) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 15:53:10,200 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 51
2020-04-08 15:53:10,200 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 6.0 with 3 tasks
2020-04-08 15:53:10,200 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 120
2020-04-08 15:53:10,200 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 129
2020-04-08 15:53:10,200 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 71
2020-04-08 15:53:10,200 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 107
2020-04-08 15:53:10,201 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 124
2020-04-08 15:53:10,201 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 18, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:53:10,202 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 6.0 (TID 18)
2020-04-08 15:53:10,202 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.0.102:53416 in memory (size: 3.3 KB, free: 4.1 GB)
2020-04-08 15:53:10,203 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 163 is the same as ending offset skipping PABUSI2 1
2020-04-08 15:53:10,204 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 148
2020-04-08 15:53:10,204 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 6.0 (TID 18). 2785 bytes result sent to driver
2020-04-08 15:53:10,204 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 54
2020-04-08 15:53:10,204 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 63
2020-04-08 15:53:10,204 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 106
2020-04-08 15:53:10,204 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 150
2020-04-08 15:53:10,205 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 53
2020-04-08 15:53:10,205 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 114
2020-04-08 15:53:10,205 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 98
2020-04-08 15:53:10,205 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 113
2020-04-08 15:53:10,205 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 97
2020-04-08 15:53:10,205 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 6.0 (TID 19, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:53:10,205 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 125
2020-04-08 15:53:10,205 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 52
2020-04-08 15:53:10,205 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 73
2020-04-08 15:53:10,205 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 18) in 4 ms on localhost (executor driver) (1/3)
2020-04-08 15:53:10,205 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 6.0 (TID 19)
2020-04-08 15:53:10,205 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 77
2020-04-08 15:53:10,206 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 90
2020-04-08 15:53:10,206 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 122
2020-04-08 15:53:10,206 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 89
2020-04-08 15:53:10,207 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 163 is the same as ending offset skipping PABUSI2 2
2020-04-08 15:53:10,207 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.0.102:53416 in memory (size: 3.3 KB, free: 4.1 GB)
2020-04-08 15:53:10,208 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 6.0 (TID 19). 2785 bytes result sent to driver
2020-04-08 15:53:10,209 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 6.0 (TID 20, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:53:10,209 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 6.0 (TID 20)
2020-04-08 15:53:10,209 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 6.0 (TID 19) in 4 ms on localhost (executor driver) (2/3)
2020-04-08 15:53:10,209 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 112
2020-04-08 15:53:10,210 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 88
2020-04-08 15:53:10,210 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 131
2020-04-08 15:53:10,210 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 67
2020-04-08 15:53:10,210 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 65
2020-04-08 15:53:10,210 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 74
2020-04-08 15:53:10,210 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 144
2020-04-08 15:53:10,210 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 94
2020-04-08 15:53:10,210 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 146
2020-04-08 15:53:10,210 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 83
2020-04-08 15:53:10,210 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 111
2020-04-08 15:53:10,210 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 137
2020-04-08 15:53:10,210 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 84
2020-04-08 15:53:10,211 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 86
2020-04-08 15:53:10,211 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 100
2020-04-08 15:53:10,211 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 81
2020-04-08 15:53:10,211 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 99
2020-04-08 15:53:10,211 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 105
2020-04-08 15:53:10,211 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 119
2020-04-08 15:53:10,211 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 138
2020-04-08 15:53:10,211 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 76
2020-04-08 15:53:10,211 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 160 is the same as ending offset skipping PABUSI2 0
2020-04-08 15:53:10,212 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 6.0 (TID 20). 2785 bytes result sent to driver
2020-04-08 15:53:10,212 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.0.102:53416 in memory (size: 3.3 KB, free: 4.1 GB)
2020-04-08 15:53:10,212 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 6.0 (TID 20) in 3 ms on localhost (executor driver) (3/3)
2020-04-08 15:53:10,213 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
2020-04-08 15:53:10,213 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 6 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137) finished in 0.023 s
2020-04-08 15:53:10,214 INFO org.apache.spark.scheduler.DAGScheduler: Job 6 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:137, took 0.025153 s
2020-04-08 15:53:10,214 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 56
2020-04-08 15:53:10,214 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 116
2020-04-08 15:53:10,214 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 80
2020-04-08 15:53:10,214 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586332390000 ms.0 from job set of time 1586332390000 ms
2020-04-08 15:53:10,215 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.214 s for time 1586332390000 ms (execution: 0.032 s)
2020-04-08 15:53:10,215 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 5 from persistence list
2020-04-08 15:53:10,216 INFO org.apache.spark.storage.BlockManager: Removing RDD 5
2020-04-08 15:53:10,216 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 15:53:10,216 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586332370000 ms
2020-04-08 15:53:10,216 INFO org.apache.spark.storage.BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.0.102:53416 in memory (size: 3.3 KB, free: 4.1 GB)
2020-04-08 15:53:10,218 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 141
2020-04-08 15:53:10,218 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 87
2020-04-08 15:53:10,218 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 101
2020-04-08 15:53:10,218 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 57
2020-04-08 15:53:10,218 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 109
2020-04-08 15:53:10,218 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 64
2020-04-08 15:53:20,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 15:53:20,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 15:53:20,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 15:53:20,178 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 163.
2020-04-08 15:53:20,180 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 163.
2020-04-08 15:53:20,180 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 160.
2020-04-08 15:53:20,181 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586332400000 ms
2020-04-08 15:53:20,181 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586332400000 ms.0 from job set of time 1586332400000 ms
2020-04-08 15:53:20,186 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:137
2020-04-08 15:53:20,186 INFO org.apache.spark.scheduler.DAGScheduler: Got job 7 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137) with 3 output partitions
2020-04-08 15:53:20,187 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 7 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137)
2020-04-08 15:53:20,187 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 15:53:20,187 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 15:53:20,187 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 7 (KafkaRDD[7] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:114), which has no missing parents
2020-04-08 15:53:20,189 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 15:53:20,190 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 15:53:20,190 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.0.102:53416 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 15:53:20,190 INFO org.apache.spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1161
2020-04-08 15:53:20,191 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 7 (KafkaRDD[7] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:114) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 15:53:20,191 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 7.0 with 3 tasks
2020-04-08 15:53:20,192 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 7.0 (TID 21, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:53:20,192 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 7.0 (TID 21)
2020-04-08 15:53:20,193 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 163 is the same as ending offset skipping PABUSI2 1
2020-04-08 15:53:20,193 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 7.0 (TID 21). 2785 bytes result sent to driver
2020-04-08 15:53:20,194 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 7.0 (TID 22, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:53:20,194 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 7.0 (TID 22)
2020-04-08 15:53:20,194 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 7.0 (TID 21) in 2 ms on localhost (executor driver) (1/3)
2020-04-08 15:53:20,195 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 163 is the same as ending offset skipping PABUSI2 2
2020-04-08 15:53:20,196 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 7.0 (TID 22). 2785 bytes result sent to driver
2020-04-08 15:53:20,196 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 7.0 (TID 23, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:53:20,196 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 7.0 (TID 23)
2020-04-08 15:53:20,196 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 7.0 (TID 22) in 2 ms on localhost (executor driver) (2/3)
2020-04-08 15:53:20,197 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 160 is the same as ending offset skipping PABUSI2 0
2020-04-08 15:53:20,197 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 7.0 (TID 23). 2785 bytes result sent to driver
2020-04-08 15:53:20,198 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 7.0 (TID 23) in 2 ms on localhost (executor driver) (3/3)
2020-04-08 15:53:20,198 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
2020-04-08 15:53:20,198 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 7 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137) finished in 0.010 s
2020-04-08 15:53:20,199 INFO org.apache.spark.scheduler.DAGScheduler: Job 7 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:137, took 0.012503 s
2020-04-08 15:53:20,199 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586332400000 ms.0 from job set of time 1586332400000 ms
2020-04-08 15:53:20,199 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.199 s for time 1586332400000 ms (execution: 0.018 s)
2020-04-08 15:53:20,199 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 6 from persistence list
2020-04-08 15:53:20,200 INFO org.apache.spark.storage.BlockManager: Removing RDD 6
2020-04-08 15:53:20,200 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 15:53:20,200 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586332380000 ms
2020-04-08 15:53:30,009 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 15:53:30,010 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 15:53:30,010 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 15:53:30,184 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 163.
2020-04-08 15:53:30,186 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 163.
2020-04-08 15:53:30,186 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 160.
2020-04-08 15:53:30,187 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586332410000 ms
2020-04-08 15:53:30,187 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586332410000 ms.0 from job set of time 1586332410000 ms
2020-04-08 15:53:30,195 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:137
2020-04-08 15:53:30,196 INFO org.apache.spark.scheduler.DAGScheduler: Got job 8 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137) with 3 output partitions
2020-04-08 15:53:30,196 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 8 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137)
2020-04-08 15:53:30,196 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 15:53:30,196 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 15:53:30,198 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 8 (KafkaRDD[8] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:114), which has no missing parents
2020-04-08 15:53:30,200 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 15:53:30,202 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 15:53:30,203 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.0.102:53416 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 15:53:30,203 INFO org.apache.spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1161
2020-04-08 15:53:30,204 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 8 (KafkaRDD[8] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:114) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 15:53:30,204 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 8.0 with 3 tasks
2020-04-08 15:53:30,206 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 24, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:53:30,206 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 8.0 (TID 24)
2020-04-08 15:53:30,208 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 163 is the same as ending offset skipping PABUSI2 1
2020-04-08 15:53:30,209 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 8.0 (TID 24). 2785 bytes result sent to driver
2020-04-08 15:53:30,209 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 8.0 (TID 25, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:53:30,209 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 8.0 (TID 25)
2020-04-08 15:53:30,209 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 24) in 4 ms on localhost (executor driver) (1/3)
2020-04-08 15:53:30,210 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 163 is the same as ending offset skipping PABUSI2 2
2020-04-08 15:53:30,211 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 8.0 (TID 25). 2828 bytes result sent to driver
2020-04-08 15:53:30,212 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 8.0 (TID 26, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:53:30,212 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 8.0 (TID 26)
2020-04-08 15:53:30,212 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 8.0 (TID 25) in 3 ms on localhost (executor driver) (2/3)
2020-04-08 15:53:30,213 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 160 is the same as ending offset skipping PABUSI2 0
2020-04-08 15:53:30,213 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 8.0 (TID 26). 2785 bytes result sent to driver
2020-04-08 15:53:30,214 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 8.0 (TID 26) in 3 ms on localhost (executor driver) (3/3)
2020-04-08 15:53:30,214 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
2020-04-08 15:53:30,215 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 8 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137) finished in 0.015 s
2020-04-08 15:53:30,215 INFO org.apache.spark.scheduler.DAGScheduler: Job 8 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:137, took 0.019418 s
2020-04-08 15:53:30,216 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586332410000 ms.0 from job set of time 1586332410000 ms
2020-04-08 15:53:30,216 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 7 from persistence list
2020-04-08 15:53:30,216 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.215 s for time 1586332410000 ms (execution: 0.028 s)
2020-04-08 15:53:30,217 INFO org.apache.spark.storage.BlockManager: Removing RDD 7
2020-04-08 15:53:30,217 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 15:53:30,217 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586332390000 ms
2020-04-08 15:53:40,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 15:53:40,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 15:53:40,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 15:53:40,179 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 163.
2020-04-08 15:53:40,181 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 163.
2020-04-08 15:53:40,183 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 160.
2020-04-08 15:53:40,183 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586332420000 ms
2020-04-08 15:53:40,183 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586332420000 ms.0 from job set of time 1586332420000 ms
2020-04-08 15:53:40,187 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:137
2020-04-08 15:53:40,188 INFO org.apache.spark.scheduler.DAGScheduler: Got job 9 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137) with 3 output partitions
2020-04-08 15:53:40,188 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 9 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137)
2020-04-08 15:53:40,188 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 15:53:40,188 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 15:53:40,188 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 9 (KafkaRDD[9] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:114), which has no missing parents
2020-04-08 15:53:40,190 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 15:53:40,191 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 15:53:40,192 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.0.102:53416 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 15:53:40,192 INFO org.apache.spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1161
2020-04-08 15:53:40,193 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 9 (KafkaRDD[9] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:114) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 15:53:40,193 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 9.0 with 3 tasks
2020-04-08 15:53:40,194 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 27, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:53:40,194 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 9.0 (TID 27)
2020-04-08 15:53:40,195 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 163 is the same as ending offset skipping PABUSI2 1
2020-04-08 15:53:40,196 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 9.0 (TID 27). 2785 bytes result sent to driver
2020-04-08 15:53:40,196 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 9.0 (TID 28, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:53:40,196 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 9.0 (TID 28)
2020-04-08 15:53:40,196 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 27) in 2 ms on localhost (executor driver) (1/3)
2020-04-08 15:53:40,198 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 163 is the same as ending offset skipping PABUSI2 2
2020-04-08 15:53:40,198 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 9.0 (TID 28). 2785 bytes result sent to driver
2020-04-08 15:53:40,199 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 9.0 (TID 29, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:53:40,199 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 9.0 (TID 29)
2020-04-08 15:53:40,199 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 9.0 (TID 28) in 3 ms on localhost (executor driver) (2/3)
2020-04-08 15:53:40,200 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 160 is the same as ending offset skipping PABUSI2 0
2020-04-08 15:53:40,201 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 9.0 (TID 29). 2785 bytes result sent to driver
2020-04-08 15:53:40,201 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 9.0 (TID 29) in 3 ms on localhost (executor driver) (3/3)
2020-04-08 15:53:40,201 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
2020-04-08 15:53:40,202 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 9 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137) finished in 0.012 s
2020-04-08 15:53:40,202 INFO org.apache.spark.scheduler.DAGScheduler: Job 9 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:137, took 0.014324 s
2020-04-08 15:53:40,202 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586332420000 ms.0 from job set of time 1586332420000 ms
2020-04-08 15:53:40,202 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.202 s for time 1586332420000 ms (execution: 0.019 s)
2020-04-08 15:53:40,202 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 8 from persistence list
2020-04-08 15:53:40,203 INFO org.apache.spark.storage.BlockManager: Removing RDD 8
2020-04-08 15:53:40,203 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 15:53:40,203 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586332400000 ms
2020-04-08 15:53:50,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 15:53:50,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 15:53:50,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 15:53:50,186 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 163.
2020-04-08 15:53:50,186 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 160.
2020-04-08 15:53:50,187 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 163.
2020-04-08 15:53:50,187 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586332430000 ms
2020-04-08 15:53:50,188 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586332430000 ms.0 from job set of time 1586332430000 ms
2020-04-08 15:53:50,194 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:137
2020-04-08 15:53:50,194 INFO org.apache.spark.scheduler.DAGScheduler: Got job 10 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137) with 3 output partitions
2020-04-08 15:53:50,194 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 10 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137)
2020-04-08 15:53:50,194 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 15:53:50,194 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 15:53:50,195 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 10 (KafkaRDD[10] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:114), which has no missing parents
2020-04-08 15:53:50,197 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 15:53:50,198 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 15:53:50,199 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 192.168.0.102:53416 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 15:53:50,199 INFO org.apache.spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1161
2020-04-08 15:53:50,200 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 10 (KafkaRDD[10] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:114) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 15:53:50,200 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 10.0 with 3 tasks
2020-04-08 15:53:50,201 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 30, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:53:50,201 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 10.0 (TID 30)
2020-04-08 15:53:50,202 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 163 is the same as ending offset skipping PABUSI2 1
2020-04-08 15:53:50,203 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 10.0 (TID 30). 2785 bytes result sent to driver
2020-04-08 15:53:50,203 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 10.0 (TID 31, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:53:50,203 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 10.0 (TID 31)
2020-04-08 15:53:50,203 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 30) in 3 ms on localhost (executor driver) (1/3)
2020-04-08 15:53:50,204 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 163 is the same as ending offset skipping PABUSI2 2
2020-04-08 15:53:50,205 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 10.0 (TID 31). 2828 bytes result sent to driver
2020-04-08 15:53:50,205 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 10.0 (TID 32, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:53:50,206 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 10.0 (TID 32)
2020-04-08 15:53:50,206 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 10.0 (TID 31) in 3 ms on localhost (executor driver) (2/3)
2020-04-08 15:53:50,207 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 160 is the same as ending offset skipping PABUSI2 0
2020-04-08 15:53:50,207 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 10.0 (TID 32). 2785 bytes result sent to driver
2020-04-08 15:53:50,208 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 10.0 (TID 32) in 3 ms on localhost (executor driver) (3/3)
2020-04-08 15:53:50,208 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
2020-04-08 15:53:50,208 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 10 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137) finished in 0.012 s
2020-04-08 15:53:50,209 INFO org.apache.spark.scheduler.DAGScheduler: Job 10 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:137, took 0.014795 s
2020-04-08 15:53:50,209 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586332430000 ms.0 from job set of time 1586332430000 ms
2020-04-08 15:53:50,209 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.209 s for time 1586332430000 ms (execution: 0.021 s)
2020-04-08 15:53:50,209 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 9 from persistence list
2020-04-08 15:53:50,210 INFO org.apache.spark.storage.BlockManager: Removing RDD 9
2020-04-08 15:53:50,210 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 15:53:50,210 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586332410000 ms
2020-04-08 15:54:00,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 15:54:00,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 15:54:00,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 15:54:00,185 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 163.
2020-04-08 15:54:00,188 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 163.
2020-04-08 15:54:00,188 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 160.
2020-04-08 15:54:00,189 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586332440000 ms
2020-04-08 15:54:00,189 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586332440000 ms.0 from job set of time 1586332440000 ms
2020-04-08 15:54:00,195 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:137
2020-04-08 15:54:00,195 INFO org.apache.spark.scheduler.DAGScheduler: Got job 11 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137) with 3 output partitions
2020-04-08 15:54:00,195 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 11 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137)
2020-04-08 15:54:00,195 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 15:54:00,195 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 15:54:00,196 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 11 (KafkaRDD[11] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:114), which has no missing parents
2020-04-08 15:54:00,197 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 15:54:00,198 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 15:54:00,199 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 192.168.0.102:53416 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 15:54:00,199 INFO org.apache.spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1161
2020-04-08 15:54:00,200 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 11 (KafkaRDD[11] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:114) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 15:54:00,200 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 11.0 with 3 tasks
2020-04-08 15:54:00,200 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 33, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:54:00,201 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 11.0 (TID 33)
2020-04-08 15:54:00,201 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 163 is the same as ending offset skipping PABUSI2 1
2020-04-08 15:54:00,202 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 11.0 (TID 33). 2828 bytes result sent to driver
2020-04-08 15:54:00,202 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 11.0 (TID 34, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:54:00,202 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 33) in 2 ms on localhost (executor driver) (1/3)
2020-04-08 15:54:00,202 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 11.0 (TID 34)
2020-04-08 15:54:00,203 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 163 is the same as ending offset skipping PABUSI2 2
2020-04-08 15:54:00,204 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 11.0 (TID 34). 2785 bytes result sent to driver
2020-04-08 15:54:00,204 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 11.0 (TID 35, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:54:00,204 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 11.0 (TID 34) in 2 ms on localhost (executor driver) (2/3)
2020-04-08 15:54:00,204 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 11.0 (TID 35)
2020-04-08 15:54:00,205 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 160 is the same as ending offset skipping PABUSI2 0
2020-04-08 15:54:00,205 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 11.0 (TID 35). 2785 bytes result sent to driver
2020-04-08 15:54:00,206 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 11.0 (TID 35) in 2 ms on localhost (executor driver) (3/3)
2020-04-08 15:54:00,206 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
2020-04-08 15:54:00,206 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 11 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137) finished in 0.009 s
2020-04-08 15:54:00,206 INFO org.apache.spark.scheduler.DAGScheduler: Job 11 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:137, took 0.011379 s
2020-04-08 15:54:00,206 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586332440000 ms.0 from job set of time 1586332440000 ms
2020-04-08 15:54:00,207 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.206 s for time 1586332440000 ms (execution: 0.017 s)
2020-04-08 15:54:00,207 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 10 from persistence list
2020-04-08 15:54:00,207 INFO org.apache.spark.storage.BlockManager: Removing RDD 10
2020-04-08 15:54:00,207 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 15:54:00,207 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586332420000 ms
2020-04-08 15:54:10,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 15:54:10,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 15:54:10,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 15:54:10,184 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 163.
2020-04-08 15:54:10,184 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 163.
2020-04-08 15:54:10,184 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 160.
2020-04-08 15:54:10,185 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586332450000 ms
2020-04-08 15:54:10,186 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586332450000 ms.0 from job set of time 1586332450000 ms
2020-04-08 15:54:10,191 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:137
2020-04-08 15:54:10,191 INFO org.apache.spark.scheduler.DAGScheduler: Got job 12 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137) with 3 output partitions
2020-04-08 15:54:10,192 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 12 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137)
2020-04-08 15:54:10,192 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 15:54:10,192 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 15:54:10,192 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 12 (KafkaRDD[12] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:114), which has no missing parents
2020-04-08 15:54:10,194 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 15:54:10,196 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 15:54:10,196 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 192.168.0.102:53416 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 15:54:10,196 INFO org.apache.spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1161
2020-04-08 15:54:10,197 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 12 (KafkaRDD[12] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:114) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 15:54:10,197 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 12.0 with 3 tasks
2020-04-08 15:54:10,198 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 12.0 (TID 36, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:54:10,198 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 12.0 (TID 36)
2020-04-08 15:54:10,199 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 163 is the same as ending offset skipping PABUSI2 1
2020-04-08 15:54:10,199 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 12.0 (TID 36). 2785 bytes result sent to driver
2020-04-08 15:54:10,200 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 12.0 (TID 37, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:54:10,200 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 12.0 (TID 36) in 3 ms on localhost (executor driver) (1/3)
2020-04-08 15:54:10,200 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 12.0 (TID 37)
2020-04-08 15:54:10,201 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 163 is the same as ending offset skipping PABUSI2 2
2020-04-08 15:54:10,201 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 12.0 (TID 37). 2785 bytes result sent to driver
2020-04-08 15:54:10,201 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 12.0 (TID 38, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:54:10,202 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 12.0 (TID 37) in 3 ms on localhost (executor driver) (2/3)
2020-04-08 15:54:10,202 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 12.0 (TID 38)
2020-04-08 15:54:10,202 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 160 is the same as ending offset skipping PABUSI2 0
2020-04-08 15:54:10,203 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 12.0 (TID 38). 2785 bytes result sent to driver
2020-04-08 15:54:10,203 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 12.0 (TID 38) in 2 ms on localhost (executor driver) (3/3)
2020-04-08 15:54:10,203 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
2020-04-08 15:54:10,204 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 12 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137) finished in 0.011 s
2020-04-08 15:54:10,204 INFO org.apache.spark.scheduler.DAGScheduler: Job 12 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:137, took 0.012874 s
2020-04-08 15:54:10,204 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586332450000 ms.0 from job set of time 1586332450000 ms
2020-04-08 15:54:10,204 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.204 s for time 1586332450000 ms (execution: 0.018 s)
2020-04-08 15:54:10,204 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 11 from persistence list
2020-04-08 15:54:10,205 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 15:54:10,205 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586332430000 ms
2020-04-08 15:54:10,205 INFO org.apache.spark.storage.BlockManager: Removing RDD 11
2020-04-08 15:54:20,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 15:54:20,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 15:54:20,002 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 15:54:20,177 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 163.
2020-04-08 15:54:20,177 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 163.
2020-04-08 15:54:20,177 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 160.
2020-04-08 15:54:20,178 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586332460000 ms
2020-04-08 15:54:20,178 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586332460000 ms.0 from job set of time 1586332460000 ms
2020-04-08 15:54:20,183 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:137
2020-04-08 15:54:20,184 INFO org.apache.spark.scheduler.DAGScheduler: Got job 13 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137) with 3 output partitions
2020-04-08 15:54:20,184 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 13 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137)
2020-04-08 15:54:20,184 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 15:54:20,184 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 15:54:20,184 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 13 (KafkaRDD[13] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:114), which has no missing parents
2020-04-08 15:54:20,186 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 15:54:20,187 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 15:54:20,188 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 192.168.0.102:53416 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 15:54:20,188 INFO org.apache.spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1161
2020-04-08 15:54:20,188 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 13 (KafkaRDD[13] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:114) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 15:54:20,189 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 13.0 with 3 tasks
2020-04-08 15:54:20,189 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 13.0 (TID 39, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:54:20,190 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 13.0 (TID 39)
2020-04-08 15:54:20,191 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 163 is the same as ending offset skipping PABUSI2 1
2020-04-08 15:54:20,191 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 13.0 (TID 39). 2785 bytes result sent to driver
2020-04-08 15:54:20,192 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 13.0 (TID 40, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:54:20,192 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 13.0 (TID 39) in 3 ms on localhost (executor driver) (1/3)
2020-04-08 15:54:20,192 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 13.0 (TID 40)
2020-04-08 15:54:20,193 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 163 is the same as ending offset skipping PABUSI2 2
2020-04-08 15:54:20,193 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 13.0 (TID 40). 2785 bytes result sent to driver
2020-04-08 15:54:20,194 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 13.0 (TID 41, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:54:20,194 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 13.0 (TID 40) in 3 ms on localhost (executor driver) (2/3)
2020-04-08 15:54:20,194 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 13.0 (TID 41)
2020-04-08 15:54:20,195 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 160 is the same as ending offset skipping PABUSI2 0
2020-04-08 15:54:20,195 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 13.0 (TID 41). 2785 bytes result sent to driver
2020-04-08 15:54:20,196 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 13.0 (TID 41) in 2 ms on localhost (executor driver) (3/3)
2020-04-08 15:54:20,196 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
2020-04-08 15:54:20,196 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 13 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137) finished in 0.011 s
2020-04-08 15:54:20,197 INFO org.apache.spark.scheduler.DAGScheduler: Job 13 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:137, took 0.013679 s
2020-04-08 15:54:20,197 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586332460000 ms.0 from job set of time 1586332460000 ms
2020-04-08 15:54:20,197 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.197 s for time 1586332460000 ms (execution: 0.019 s)
2020-04-08 15:54:20,197 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 12 from persistence list
2020-04-08 15:54:20,198 INFO org.apache.spark.storage.BlockManager: Removing RDD 12
2020-04-08 15:54:20,198 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 15:54:20,198 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586332440000 ms
2020-04-08 15:54:30,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 15:54:30,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 15:54:30,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 15:54:30,180 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 163.
2020-04-08 15:54:30,181 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 163.
2020-04-08 15:54:30,181 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 160.
2020-04-08 15:54:30,182 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586332470000 ms
2020-04-08 15:54:30,182 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586332470000 ms.0 from job set of time 1586332470000 ms
2020-04-08 15:54:30,187 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:137
2020-04-08 15:54:30,187 INFO org.apache.spark.scheduler.DAGScheduler: Got job 14 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137) with 3 output partitions
2020-04-08 15:54:30,187 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 14 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137)
2020-04-08 15:54:30,187 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 15:54:30,187 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 15:54:30,188 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 14 (KafkaRDD[14] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:114), which has no missing parents
2020-04-08 15:54:30,190 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 15:54:30,191 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 15:54:30,191 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 192.168.0.102:53416 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 15:54:30,192 INFO org.apache.spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1161
2020-04-08 15:54:30,192 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 14 (KafkaRDD[14] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:114) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 15:54:30,192 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 14.0 with 3 tasks
2020-04-08 15:54:30,193 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 42, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:54:30,193 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 14.0 (TID 42)
2020-04-08 15:54:30,194 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 163 is the same as ending offset skipping PABUSI2 1
2020-04-08 15:54:30,195 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 14.0 (TID 42). 2828 bytes result sent to driver
2020-04-08 15:54:30,195 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 14.0 (TID 43, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:54:30,196 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 14.0 (TID 43)
2020-04-08 15:54:30,196 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 42) in 3 ms on localhost (executor driver) (1/3)
2020-04-08 15:54:30,196 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 163 is the same as ending offset skipping PABUSI2 2
2020-04-08 15:54:30,197 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 14.0 (TID 43). 2742 bytes result sent to driver
2020-04-08 15:54:30,197 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 14.0 (TID 44, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:54:30,197 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 14.0 (TID 44)
2020-04-08 15:54:30,197 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 14.0 (TID 43) in 2 ms on localhost (executor driver) (2/3)
2020-04-08 15:54:30,198 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 160 is the same as ending offset skipping PABUSI2 0
2020-04-08 15:54:30,198 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 14.0 (TID 44). 2785 bytes result sent to driver
2020-04-08 15:54:30,199 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 14.0 (TID 44) in 2 ms on localhost (executor driver) (3/3)
2020-04-08 15:54:30,199 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
2020-04-08 15:54:30,199 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 14 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137) finished in 0.010 s
2020-04-08 15:54:30,199 INFO org.apache.spark.scheduler.DAGScheduler: Job 14 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:137, took 0.012633 s
2020-04-08 15:54:30,200 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586332470000 ms.0 from job set of time 1586332470000 ms
2020-04-08 15:54:30,200 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.200 s for time 1586332470000 ms (execution: 0.018 s)
2020-04-08 15:54:30,200 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 13 from persistence list
2020-04-08 15:54:30,201 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 15:54:30,201 INFO org.apache.spark.storage.BlockManager: Removing RDD 13
2020-04-08 15:54:30,201 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586332450000 ms
2020-04-08 15:54:40,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 15:54:40,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 15:54:40,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 15:54:40,184 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 163.
2020-04-08 15:54:40,188 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 163.
2020-04-08 15:54:40,572 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 160.
2020-04-08 15:54:40,573 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586332480000 ms
2020-04-08 15:54:40,573 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586332480000 ms.0 from job set of time 1586332480000 ms
2020-04-08 15:54:40,578 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:137
2020-04-08 15:54:40,579 INFO org.apache.spark.scheduler.DAGScheduler: Got job 15 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137) with 3 output partitions
2020-04-08 15:54:40,579 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 15 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137)
2020-04-08 15:54:40,579 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 15:54:40,579 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 15:54:40,579 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 15 (KafkaRDD[15] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:114), which has no missing parents
2020-04-08 15:54:40,581 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 15:54:40,582 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 15:54:40,583 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 192.168.0.102:53416 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 15:54:40,583 INFO org.apache.spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1161
2020-04-08 15:54:40,584 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 15 (KafkaRDD[15] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:114) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 15:54:40,584 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 15.0 with 3 tasks
2020-04-08 15:54:40,584 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 45, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:54:40,585 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 15.0 (TID 45)
2020-04-08 15:54:40,585 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 163 is the same as ending offset skipping PABUSI2 1
2020-04-08 15:54:40,586 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 15.0 (TID 45). 2785 bytes result sent to driver
2020-04-08 15:54:40,586 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 15.0 (TID 46, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:54:40,586 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 45) in 2 ms on localhost (executor driver) (1/3)
2020-04-08 15:54:40,586 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 15.0 (TID 46)
2020-04-08 15:54:40,587 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 163 is the same as ending offset skipping PABUSI2 2
2020-04-08 15:54:40,587 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 15.0 (TID 46). 2785 bytes result sent to driver
2020-04-08 15:54:40,588 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 15.0 (TID 47, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:54:40,589 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 15.0 (TID 47)
2020-04-08 15:54:40,589 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 15.0 (TID 46) in 3 ms on localhost (executor driver) (2/3)
2020-04-08 15:54:40,589 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 160 is the same as ending offset skipping PABUSI2 0
2020-04-08 15:54:40,590 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 15.0 (TID 47). 2785 bytes result sent to driver
2020-04-08 15:54:40,590 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 15.0 (TID 47) in 2 ms on localhost (executor driver) (3/3)
2020-04-08 15:54:40,590 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
2020-04-08 15:54:40,590 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 15 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137) finished in 0.010 s
2020-04-08 15:54:40,591 INFO org.apache.spark.scheduler.DAGScheduler: Job 15 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:137, took 0.012047 s
2020-04-08 15:54:40,591 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586332480000 ms.0 from job set of time 1586332480000 ms
2020-04-08 15:54:40,591 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.591 s for time 1586332480000 ms (execution: 0.018 s)
2020-04-08 15:54:40,591 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 14 from persistence list
2020-04-08 15:54:40,592 INFO org.apache.spark.storage.BlockManager: Removing RDD 14
2020-04-08 15:54:40,592 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 15:54:40,592 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586332460000 ms
2020-04-08 15:54:50,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 15:54:50,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 15:54:50,004 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 15:54:50,179 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 163.
2020-04-08 15:54:50,179 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 163.
2020-04-08 15:54:50,180 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 160.
2020-04-08 15:54:50,180 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586332490000 ms
2020-04-08 15:54:50,181 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586332490000 ms.0 from job set of time 1586332490000 ms
2020-04-08 15:54:50,185 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:137
2020-04-08 15:54:50,186 INFO org.apache.spark.scheduler.DAGScheduler: Got job 16 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137) with 3 output partitions
2020-04-08 15:54:50,186 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 16 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137)
2020-04-08 15:54:50,186 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 15:54:50,186 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 15:54:50,186 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 16 (KafkaRDD[16] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:114), which has no missing parents
2020-04-08 15:54:50,188 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 15:54:50,189 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 15:54:50,189 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 192.168.0.102:53416 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 15:54:50,190 INFO org.apache.spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1161
2020-04-08 15:54:50,190 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 16 (KafkaRDD[16] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:114) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 15:54:50,190 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 16.0 with 3 tasks
2020-04-08 15:54:50,191 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 16.0 (TID 48, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:54:50,191 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 16.0 (TID 48)
2020-04-08 15:54:50,192 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 163 is the same as ending offset skipping PABUSI2 1
2020-04-08 15:54:50,192 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 16.0 (TID 48). 2785 bytes result sent to driver
2020-04-08 15:54:50,192 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 16.0 (TID 49, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:54:50,193 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 16.0 (TID 49)
2020-04-08 15:54:50,193 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 16.0 (TID 48) in 2 ms on localhost (executor driver) (1/3)
2020-04-08 15:54:50,193 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 163 is the same as ending offset skipping PABUSI2 2
2020-04-08 15:54:50,194 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 16.0 (TID 49). 2785 bytes result sent to driver
2020-04-08 15:54:50,194 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 16.0 (TID 50, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:54:50,194 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 16.0 (TID 50)
2020-04-08 15:54:50,194 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 16.0 (TID 49) in 2 ms on localhost (executor driver) (2/3)
2020-04-08 15:54:50,195 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 160 is the same as ending offset skipping PABUSI2 0
2020-04-08 15:54:50,195 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 16.0 (TID 50). 2785 bytes result sent to driver
2020-04-08 15:54:50,195 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 16.0 (TID 50) in 1 ms on localhost (executor driver) (3/3)
2020-04-08 15:54:50,196 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
2020-04-08 15:54:50,196 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 16 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137) finished in 0.009 s
2020-04-08 15:54:50,196 INFO org.apache.spark.scheduler.DAGScheduler: Job 16 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:137, took 0.010698 s
2020-04-08 15:54:50,196 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586332490000 ms.0 from job set of time 1586332490000 ms
2020-04-08 15:54:50,196 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.196 s for time 1586332490000 ms (execution: 0.015 s)
2020-04-08 15:54:50,196 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 15 from persistence list
2020-04-08 15:54:50,197 INFO org.apache.spark.storage.BlockManager: Removing RDD 15
2020-04-08 15:54:50,197 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 15:54:50,197 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586332470000 ms
2020-04-08 15:55:00,005 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 15:55:00,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 15:55:00,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 15:55:00,184 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 163.
2020-04-08 15:55:00,186 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 163.
2020-04-08 15:55:00,187 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 160.
2020-04-08 15:55:00,187 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586332500000 ms
2020-04-08 15:55:00,188 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586332500000 ms.0 from job set of time 1586332500000 ms
2020-04-08 15:55:00,192 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:137
2020-04-08 15:55:00,192 INFO org.apache.spark.scheduler.DAGScheduler: Got job 17 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137) with 3 output partitions
2020-04-08 15:55:00,193 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 17 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137)
2020-04-08 15:55:00,193 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 15:55:00,193 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 15:55:00,193 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 17 (KafkaRDD[17] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:114), which has no missing parents
2020-04-08 15:55:00,195 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 15:55:00,195 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 15:55:00,196 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 192.168.0.102:53416 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 15:55:00,196 INFO org.apache.spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1161
2020-04-08 15:55:00,197 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 17 (KafkaRDD[17] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:114) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 15:55:00,197 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 17.0 with 3 tasks
2020-04-08 15:55:00,198 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 17.0 (TID 51, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:55:00,198 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 17.0 (TID 51)
2020-04-08 15:55:00,199 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 163 is the same as ending offset skipping PABUSI2 1
2020-04-08 15:55:00,200 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 17.0 (TID 51). 2785 bytes result sent to driver
2020-04-08 15:55:00,200 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 17.0 (TID 52, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:55:00,200 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 17.0 (TID 52)
2020-04-08 15:55:00,200 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 17.0 (TID 51) in 2 ms on localhost (executor driver) (1/3)
2020-04-08 15:55:00,201 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 163 is the same as ending offset skipping PABUSI2 2
2020-04-08 15:55:00,202 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 17.0 (TID 52). 2785 bytes result sent to driver
2020-04-08 15:55:00,202 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 17.0 (TID 53, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:55:00,202 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 17.0 (TID 53)
2020-04-08 15:55:00,203 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 17.0 (TID 52) in 2 ms on localhost (executor driver) (2/3)
2020-04-08 15:55:00,204 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 160 is the same as ending offset skipping PABUSI2 0
2020-04-08 15:55:00,204 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 17.0 (TID 53). 2828 bytes result sent to driver
2020-04-08 15:55:00,204 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 17.0 (TID 53) in 2 ms on localhost (executor driver) (3/3)
2020-04-08 15:55:00,204 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool 
2020-04-08 15:55:00,205 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 17 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137) finished in 0.011 s
2020-04-08 15:55:00,205 INFO org.apache.spark.scheduler.DAGScheduler: Job 17 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:137, took 0.012891 s
2020-04-08 15:55:00,205 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586332500000 ms.0 from job set of time 1586332500000 ms
2020-04-08 15:55:00,205 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.205 s for time 1586332500000 ms (execution: 0.018 s)
2020-04-08 15:55:00,206 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 16 from persistence list
2020-04-08 15:55:00,206 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 15:55:00,206 INFO org.apache.spark.storage.BlockManager: Removing RDD 16
2020-04-08 15:55:00,206 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586332480000 ms
2020-04-08 15:55:10,006 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 15:55:10,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 15:55:10,007 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 15:55:10,187 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 163.
2020-04-08 15:55:10,188 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 163.
2020-04-08 15:55:10,188 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 160.
2020-04-08 15:55:10,188 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586332510000 ms
2020-04-08 15:55:10,189 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586332510000 ms.0 from job set of time 1586332510000 ms
2020-04-08 15:55:10,194 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:137
2020-04-08 15:55:10,194 INFO org.apache.spark.scheduler.DAGScheduler: Got job 18 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137) with 3 output partitions
2020-04-08 15:55:10,194 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 18 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137)
2020-04-08 15:55:10,194 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 15:55:10,194 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 15:55:10,195 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 18 (KafkaRDD[18] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:114), which has no missing parents
2020-04-08 15:55:10,196 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 15:55:10,197 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 15:55:10,198 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 192.168.0.102:53416 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 15:55:10,198 INFO org.apache.spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1161
2020-04-08 15:55:10,199 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 18 (KafkaRDD[18] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:114) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 15:55:10,199 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 18.0 with 3 tasks
2020-04-08 15:55:10,199 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 54, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:55:10,199 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 18.0 (TID 54)
2020-04-08 15:55:10,200 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 163 is the same as ending offset skipping PABUSI2 1
2020-04-08 15:55:10,201 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 18.0 (TID 54). 2828 bytes result sent to driver
2020-04-08 15:55:10,201 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 18.0 (TID 55, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:55:10,201 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 18.0 (TID 55)
2020-04-08 15:55:10,201 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 54) in 2 ms on localhost (executor driver) (1/3)
2020-04-08 15:55:10,202 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 163 is the same as ending offset skipping PABUSI2 2
2020-04-08 15:55:10,202 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 18.0 (TID 55). 2785 bytes result sent to driver
2020-04-08 15:55:10,202 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 18.0 (TID 56, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:55:10,203 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 18.0 (TID 56)
2020-04-08 15:55:10,203 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 18.0 (TID 55) in 2 ms on localhost (executor driver) (2/3)
2020-04-08 15:55:10,203 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 160 is the same as ending offset skipping PABUSI2 0
2020-04-08 15:55:10,204 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 18.0 (TID 56). 2742 bytes result sent to driver
2020-04-08 15:55:10,204 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 18.0 (TID 56) in 2 ms on localhost (executor driver) (3/3)
2020-04-08 15:55:10,204 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
2020-04-08 15:55:10,206 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 18 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137) finished in 0.008 s
2020-04-08 15:55:10,206 INFO org.apache.spark.scheduler.DAGScheduler: Job 18 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:137, took 0.011957 s
2020-04-08 15:55:10,206 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586332510000 ms.0 from job set of time 1586332510000 ms
2020-04-08 15:55:10,206 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.206 s for time 1586332510000 ms (execution: 0.017 s)
2020-04-08 15:55:10,206 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 17 from persistence list
2020-04-08 15:55:10,207 INFO org.apache.spark.storage.BlockManager: Removing RDD 17
2020-04-08 15:55:10,207 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 15:55:10,207 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586332490000 ms
2020-04-08 15:55:20,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-1
2020-04-08 15:55:20,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-2
2020-04-08 15:55:20,003 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Seeking to LATEST offset of partition PABUSI2-0
2020-04-08 15:55:20,184 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-2 to offset 163.
2020-04-08 15:55:20,184 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-1 to offset 163.
2020-04-08 15:55:20,185 INFO org.apache.kafka.clients.consumer.internals.SubscriptionState: [Consumer clientId=consumer-1, groupId=testgroup1] Resetting offset for partition PABUSI2-0 to offset 160.
2020-04-08 15:55:20,185 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1586332520000 ms
2020-04-08 15:55:20,186 INFO org.apache.spark.streaming.scheduler.JobScheduler: Starting job streaming job 1586332520000 ms.0 from job set of time 1586332520000 ms
2020-04-08 15:55:20,191 INFO org.apache.spark.SparkContext: Starting job: foreachPartition at Kafka2SparkStreaming2Kudu.scala:137
2020-04-08 15:55:20,192 INFO org.apache.spark.scheduler.DAGScheduler: Got job 19 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137) with 3 output partitions
2020-04-08 15:55:20,192 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 19 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137)
2020-04-08 15:55:20,192 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2020-04-08 15:55:20,192 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2020-04-08 15:55:20,192 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 19 (KafkaRDD[19] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:114), which has no missing parents
2020-04-08 15:55:20,195 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 4.8 KB, free 4.1 GB)
2020-04-08 15:55:20,196 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 3.3 KB, free 4.1 GB)
2020-04-08 15:55:20,196 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 192.168.0.102:53416 (size: 3.3 KB, free: 4.1 GB)
2020-04-08 15:55:20,197 INFO org.apache.spark.SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1161
2020-04-08 15:55:20,197 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 19 (KafkaRDD[19] at createDirectStream at Kafka2SparkStreaming2Kudu.scala:114) (first 15 tasks are for partitions Vector(0, 1, 2))
2020-04-08 15:55:20,197 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 19.0 with 3 tasks
2020-04-08 15:55:20,198 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 19.0 (TID 57, localhost, executor driver, partition 0, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:55:20,198 INFO org.apache.spark.executor.Executor: Running task 0.0 in stage 19.0 (TID 57)
2020-04-08 15:55:20,199 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 163 is the same as ending offset skipping PABUSI2 1
2020-04-08 15:55:20,199 INFO org.apache.spark.executor.Executor: Finished task 0.0 in stage 19.0 (TID 57). 2785 bytes result sent to driver
2020-04-08 15:55:20,200 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 19.0 (TID 58, localhost, executor driver, partition 1, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:55:20,200 INFO org.apache.spark.executor.Executor: Running task 1.0 in stage 19.0 (TID 58)
2020-04-08 15:55:20,200 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 19.0 (TID 57) in 2 ms on localhost (executor driver) (1/3)
2020-04-08 15:55:20,200 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 163 is the same as ending offset skipping PABUSI2 2
2020-04-08 15:55:20,201 INFO org.apache.spark.executor.Executor: Finished task 1.0 in stage 19.0 (TID 58). 2785 bytes result sent to driver
2020-04-08 15:55:20,201 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 2.0 in stage 19.0 (TID 59, localhost, executor driver, partition 2, PROCESS_LOCAL, 7746 bytes)
2020-04-08 15:55:20,201 INFO org.apache.spark.executor.Executor: Running task 2.0 in stage 19.0 (TID 59)
2020-04-08 15:55:20,201 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 1.0 in stage 19.0 (TID 58) in 2 ms on localhost (executor driver) (2/3)
2020-04-08 15:55:20,202 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Beginning offset 160 is the same as ending offset skipping PABUSI2 0
2020-04-08 15:55:20,202 INFO org.apache.spark.executor.Executor: Finished task 2.0 in stage 19.0 (TID 59). 2785 bytes result sent to driver
2020-04-08 15:55:20,203 INFO org.apache.spark.scheduler.TaskSetManager: Finished task 2.0 in stage 19.0 (TID 59) in 2 ms on localhost (executor driver) (3/3)
2020-04-08 15:55:20,203 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool 
2020-04-08 15:55:20,203 INFO org.apache.spark.scheduler.DAGScheduler: ResultStage 19 (foreachPartition at Kafka2SparkStreaming2Kudu.scala:137) finished in 0.010 s
2020-04-08 15:55:20,203 INFO org.apache.spark.scheduler.DAGScheduler: Job 19 finished: foreachPartition at Kafka2SparkStreaming2Kudu.scala:137, took 0.012429 s
2020-04-08 15:55:20,204 INFO org.apache.spark.streaming.scheduler.JobScheduler: Finished job streaming job 1586332520000 ms.0 from job set of time 1586332520000 ms
2020-04-08 15:55:20,204 INFO org.apache.spark.streaming.scheduler.JobScheduler: Total delay: 0.204 s for time 1586332520000 ms (execution: 0.018 s)
2020-04-08 15:55:20,204 INFO org.apache.spark.streaming.kafka010.KafkaRDD: Removing RDD 18 from persistence list
2020-04-08 15:55:20,204 INFO org.apache.spark.storage.BlockManager: Removing RDD 18
2020-04-08 15:55:20,204 INFO org.apache.spark.streaming.scheduler.ReceivedBlockTracker: Deleting batches: 
2020-04-08 15:55:20,204 INFO org.apache.spark.streaming.scheduler.InputInfoTracker: remove old batch metadata: 1586332500000 ms
2020-04-08 15:55:21,353 INFO org.apache.spark.streaming.StreamingContext: Invoking stop(stopGracefully=false) from shutdown hook
2020-04-08 15:55:21,356 INFO org.apache.spark.streaming.scheduler.ReceiverTracker: ReceiverTracker stopped
2020-04-08 15:55:21,356 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopping JobGenerator immediately
2020-04-08 15:55:21,358 INFO org.apache.spark.streaming.util.RecurringTimer: Stopped timer for JobGenerator after time 1586332520000
2020-04-08 15:55:21,536 INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator: [Consumer clientId=consumer-1, groupId=testgroup1] Member consumer-1-681d2f4d-2096-4cb5-b11d-18ee4d29a62d sending LeaveGroup request to coordinator cw-instances-2.vpc.cloudera.com:9092 (id: 2147483595 rack: null)
2020-04-08 15:55:21,718 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopped JobGenerator
2020-04-08 15:55:21,721 INFO org.apache.spark.streaming.scheduler.JobScheduler: Stopped JobScheduler
2020-04-08 15:55:21,728 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@93f432e{/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 15:55:21,728 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@639aba11{/streaming/batch,null,UNAVAILABLE,@Spark}
2020-04-08 15:55:21,729 INFO org.spark_project.jetty.server.handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@6e33fcae{/static/streaming,null,UNAVAILABLE,@Spark}
2020-04-08 15:55:21,730 INFO org.apache.spark.streaming.StreamingContext: StreamingContext stopped successfully
2020-04-08 15:55:21,731 INFO org.apache.spark.SparkContext: Invoking stop() from shutdown hook
2020-04-08 15:55:21,737 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@89ff02e{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 15:55:21,739 INFO org.apache.spark.ui.SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
2020-04-08 15:55:21,748 INFO org.apache.spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2020-04-08 15:55:21,765 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2020-04-08 15:55:21,765 INFO org.apache.spark.storage.BlockManager: BlockManager stopped
2020-04-08 15:55:21,766 INFO org.apache.spark.storage.BlockManagerMaster: BlockManagerMaster stopped
2020-04-08 15:55:21,768 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2020-04-08 15:55:21,775 INFO org.apache.spark.SparkContext: Successfully stopped SparkContext
2020-04-08 15:55:21,775 INFO org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2020-04-08 15:55:21,776 INFO org.apache.spark.util.ShutdownHookManager: Deleting directory /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/spark-b56df697-cb11-42ad-83aa-a160dad21c9d
2020-04-08 16:56:06,485 WARN org.apache.spark.util.Utils: Your hostname, MacBook-Pro-CW.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)
2020-04-08 16:56:06,488 WARN org.apache.spark.util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
2020-04-08 16:56:06,678 INFO org.apache.spark.SparkContext: Running Spark version 2.4.0.cloudera2
2020-04-08 16:56:06,907 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-04-08 16:56:07,014 INFO org.apache.spark.SparkContext: Submitted application: Kafka2SparkStreaming2Kudu-kerberos
2020-04-08 16:56:07,052 INFO org.apache.spark.SecurityManager: Changing view acls to: godiswc
2020-04-08 16:56:07,052 INFO org.apache.spark.SecurityManager: Changing modify acls to: godiswc
2020-04-08 16:56:07,052 INFO org.apache.spark.SecurityManager: Changing view acls groups to: 
2020-04-08 16:56:07,053 INFO org.apache.spark.SecurityManager: Changing modify acls groups to: 
2020-04-08 16:56:07,053 INFO org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(godiswc); groups with view permissions: Set(); users  with modify permissions: Set(godiswc); groups with modify permissions: Set()
2020-04-08 16:56:07,184 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 16:56:07,273 INFO org.apache.spark.util.Utils: Successfully started service 'sparkDriver' on port 51791.
2020-04-08 16:56:07,289 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
2020-04-08 16:56:07,300 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
2020-04-08 16:56:07,302 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-04-08 16:56:07,302 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2020-04-08 16:56:07,331 INFO org.apache.spark.storage.DiskBlockManager: Created local directory at /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/blockmgr-d721bbbb-523d-4b52-a6e1-8817e3007e60
2020-04-08 16:56:07,345 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 4.1 GB
2020-04-08 16:56:07,354 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
2020-04-08 16:56:07,404 INFO org.spark_project.jetty.util.log: Logging initialized @1710ms
2020-04-08 16:56:07,463 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2019-04-30T04:42:08+08:00, git hash: e1bc35120a6617ee3df052294e433f3a25ce7097
2020-04-08 16:56:07,479 INFO org.spark_project.jetty.server.Server: Started @1787ms
2020-04-08 16:56:07,482 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 16:56:07,497 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@1a1da881{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 16:56:07,497 INFO org.apache.spark.util.Utils: Successfully started service 'SparkUI' on port 4040.
2020-04-08 16:56:07,521 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@63c5efee{/jobs,null,AVAILABLE,@Spark}
2020-04-08 16:56:07,522 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6d1d4d7{/jobs/json,null,AVAILABLE,@Spark}
2020-04-08 16:56:07,524 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@89ff02e{/jobs/job,null,AVAILABLE,@Spark}
2020-04-08 16:56:07,527 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a988392{/jobs/job/json,null,AVAILABLE,@Spark}
2020-04-08 16:56:07,527 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1d71006f{/stages,null,AVAILABLE,@Spark}
2020-04-08 16:56:07,528 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5b6813df{/stages/json,null,AVAILABLE,@Spark}
2020-04-08 16:56:07,528 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5f2606b{/stages/stage,null,AVAILABLE,@Spark}
2020-04-08 16:56:07,529 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2552f2cb{/stages/stage/json,null,AVAILABLE,@Spark}
2020-04-08 16:56:07,530 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@33352f32{/stages/pool,null,AVAILABLE,@Spark}
2020-04-08 16:56:07,530 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5f3b9c57{/stages/pool/json,null,AVAILABLE,@Spark}
2020-04-08 16:56:07,531 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1e044120{/storage,null,AVAILABLE,@Spark}
2020-04-08 16:56:07,531 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2cf23c81{/storage/json,null,AVAILABLE,@Spark}
2020-04-08 16:56:07,532 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3624da92{/storage/rdd,null,AVAILABLE,@Spark}
2020-04-08 16:56:07,532 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@35fe2125{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-04-08 16:56:07,533 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@94f6bfb{/environment,null,AVAILABLE,@Spark}
2020-04-08 16:56:07,533 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@34645867{/environment/json,null,AVAILABLE,@Spark}
2020-04-08 16:56:07,534 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2484f433{/executors,null,AVAILABLE,@Spark}
2020-04-08 16:56:07,534 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60b71e8f{/executors/json,null,AVAILABLE,@Spark}
2020-04-08 16:56:07,535 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1255b1d1{/executors/threadDump,null,AVAILABLE,@Spark}
2020-04-08 16:56:07,535 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@464649c{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-04-08 16:56:07,541 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7c22d4f{/static,null,AVAILABLE,@Spark}
2020-04-08 16:56:07,541 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6f80fafe{/,null,AVAILABLE,@Spark}
2020-04-08 16:56:07,542 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3af17be2{/api,null,AVAILABLE,@Spark}
2020-04-08 16:56:07,543 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6e521c1e{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-04-08 16:56:07,543 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@224b4d61{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-04-08 16:56:07,544 INFO org.apache.spark.ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.102:4040
2020-04-08 16:56:07,620 INFO org.apache.spark.executor.Executor: Starting executor ID driver on host localhost
2020-04-08 16:56:07,692 INFO org.apache.spark.util.Utils: max retries is 16
2020-04-08 16:56:07,693 INFO org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51792.
2020-04-08 16:56:07,694 INFO org.apache.spark.network.netty.NettyBlockTransferService: Server created on 192.168.0.102:51792
2020-04-08 16:56:07,695 INFO org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-04-08 16:56:07,727 INFO org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.102, 51792, None)
2020-04-08 16:56:07,730 INFO org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager 192.168.0.102:51792 with 4.1 GB RAM, BlockManagerId(driver, 192.168.0.102, 51792, None)
2020-04-08 16:56:07,733 INFO org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.102, 51792, None)
2020-04-08 16:56:07,733 INFO org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.102, 51792, None)
2020-04-08 16:56:07,893 INFO org.spark_project.jetty.server.handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4bf324f9{/metrics/json,null,AVAILABLE,@Spark}
2020-04-08 16:56:07,971 WARN org.apache.spark.streaming.StreamingContext: spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
2020-04-08 16:56:08,152 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding enable.auto.commit to false for executor
2020-04-08 16:56:08,153 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding auto.offset.reset to none for executor
2020-04-08 16:56:08,153 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding executor group.id to spark-executor-testgroup1
2020-04-08 16:56:08,154 WARN org.apache.spark.streaming.kafka010.KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
2020-04-08 16:56:08,740 WARN org.apache.kudu.util.NetUtil: Slow DNS lookup! Resolved IP of `cw-instances-2.vpc.cloudera.com' to cw-instances-2.vpc.cloudera.com/10.65.1.47 in 210984022ns
2020-04-08 16:56:09,048 WARN org.apache.kudu.util.NetUtil: Slow DNS lookup! Resolved IP of `cw-instances-3.vpc.cloudera.com' to cw-instances-3.vpc.cloudera.com/10.65.9.108 in 228442608ns
2020-04-08 16:56:09,205 WARN org.apache.kudu.util.NetUtil: Slow DNS lookup! Resolved IP of `cw-instances-4.vpc.cloudera.com' to cw-instances-4.vpc.cloudera.com/10.65.12.223 in 155531769ns
2020-04-08 16:56:09,285 INFO org.apache.kudu.client.ConnectToCluster: Unable to connect to master cw-instances-2.vpc.cloudera.com:7051: server requires authentication, but client does not have Kerberos credentials (tgt). Authentication tokens were not used because no token is available
2020-04-08 16:56:09,400 INFO org.apache.kudu.client.ConnectToCluster: Unable to connect to master cw-instances-3.vpc.cloudera.com:7051: server requires authentication, but client does not have Kerberos credentials (tgt). Authentication tokens were not used because no token is available
2020-04-08 16:56:09,558 INFO org.apache.kudu.client.ConnectToCluster: Unable to connect to master cw-instances-4.vpc.cloudera.com:7051: server requires authentication, but client does not have Kerberos credentials (tgt). Authentication tokens were not used because no token is available
2020-04-08 16:56:09,570 INFO org.apache.spark.SparkContext: Invoking stop() from shutdown hook
2020-04-08 16:56:09,576 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@1a1da881{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-04-08 16:56:09,578 INFO org.apache.spark.ui.SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
2020-04-08 16:56:09,584 INFO org.apache.spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2020-04-08 16:56:09,597 INFO org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2020-04-08 16:56:09,597 INFO org.apache.spark.storage.BlockManager: BlockManager stopped
2020-04-08 16:56:09,601 INFO org.apache.spark.storage.BlockManagerMaster: BlockManagerMaster stopped
2020-04-08 16:56:09,603 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2020-04-08 16:56:09,611 INFO org.apache.spark.SparkContext: Successfully stopped SparkContext
2020-04-08 16:56:09,611 INFO org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2020-04-08 16:56:09,612 INFO org.apache.spark.util.ShutdownHookManager: Deleting directory /private/var/folders/_0/ty0cz_5170g7yqd6gtwmsm2r0000gn/T/spark-096e568f-bbcd-4e0a-89da-cf4807e910b5
